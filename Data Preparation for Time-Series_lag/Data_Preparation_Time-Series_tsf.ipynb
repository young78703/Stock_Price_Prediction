{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "def convert_tsf_to_dataframe(full_file_path_and_name, replace_missing_vals_with=\"NaN\", value_column_name=\"series_value\"):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if len(line_content) != 3:\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if len(line_content) != 2:\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(strtobool(line_content[1]))\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\"Missing attribute section. Attribute section must come before data.\")\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0 or not found_data_tag:\n",
    "                        raise Exception(\"Missing attribute section or @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[-1].split(\",\")\n",
    "                        if not series:\n",
    "                            raise Exception(\"A given series should contain at least one numeric value. Missing values should be indicated with ? symbol.\")\n",
    "\n",
    "                        series_data = [replace_missing_vals_with if val == \"?\" else float(val) for val in series]\n",
    "                        if series_data.count(replace_missing_vals_with) == len(series_data):\n",
    "                            raise Exception(\"All series values are missing.\")\n",
    "\n",
    "                        all_data.setdefault(value_column_name, []).append(series_data)\n",
    "\n",
    "                        for i, val in enumerate(full_info[:-1]):\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                all_data[col_names[i]].append(int(val))\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                all_data[col_names[i]].append(val)\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                try:\n",
    "                                    all_data[col_names[i]].append(datetime.strptime(val, \"%Y-%m-%d %H-%M-%S\"))\n",
    "                                except ValueError as e:\n",
    "                                    print(f\"Error parsing date: {val}. Expected format: '%Y-%m-%d %H-%M-%S'. Error: {e}\")\n",
    "                                    raise\n",
    "                            else:\n",
    "                                raise Exception(\"Unsupported attribute type.\")\n",
    "    if not all_data:\n",
    "        raise Exception(\"No data loaded. Check file content and format.\")\n",
    "\n",
    "    # Create DataFrame from dictionary\n",
    "    loaded_data = pd.DataFrame(all_data)\n",
    "    return loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Example of usage\n",
    "full_file_path_and_name = ''\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(full_file_path_and_name)\n",
    "output_file_path = \" \"\n",
    "# output_file_path = \" \"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "# Saving the embedded series DataFrame to a CSV file\n",
    "loaded_data.to_csv(f'{output_file_path}_loaded_dat.csv', index=False)\n",
    "\n",
    "print(loaded_data)\n",
    "print(frequency)\n",
    "print(forecast_horizon)\n",
    "print(contain_missing_values)\n",
    "print(contain_equal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def create_input_matrix(dataset, lag):\n",
    "    embedded_series = []\n",
    "    final_lags = []\n",
    "    series_means = []\n",
    "    \n",
    "    for i, series in enumerate(dataset['series_value']):\n",
    "        print(f\"Processing series {i + 1}/{len(dataset)}\")\n",
    "        time_series = np.array(series)\n",
    "\n",
    "        if len(time_series) < lag + 1:\n",
    "            print(f\"Skipping series {i + 1} due to insufficient length.\")\n",
    "            continue\n",
    "        \n",
    "        mean_val = np.mean(time_series)\n",
    "        \n",
    "        # Mean normalization\n",
    "        if mean_val == 0:\n",
    "            mean_val = 1  # Avoid division by zero\n",
    "        \n",
    "        time_series_normalized = time_series / mean_val\n",
    "        series_means.append(mean_val)\n",
    "        \n",
    "        # Prepare lagged data only if enough data points are available\n",
    "        try:\n",
    "            slices = []\n",
    "            for j in range(lag + 1):\n",
    "                if len(time_series_normalized[j:]) > lag:  # Ensure slice contains data\n",
    "                    slices.append(time_series_normalized[j:-lag+j if -lag+j != 0 else None])\n",
    "            if slices:\n",
    "                embedded = np.column_stack(slices)\n",
    "                embedded_series.append(embedded)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error in processing series {i + 1}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Creating the test set\n",
    "        final_lags.append(time_series_normalized[-lag:][::-1])\n",
    "    \n",
    "    if embedded_series:\n",
    "        embedded_series_df = pd.DataFrame(np.vstack(embedded_series), columns=[\"y\"] + [f\"Lag{j}\" for j in range(1, lag+1)])\n",
    "    else:\n",
    "        embedded_series_df = pd.DataFrame(columns=[\"y\"] + [f\"Lag{j}\" for j in range(1, lag+1)])\n",
    "    \n",
    "    if final_lags:\n",
    "        final_lags_df = pd.DataFrame(final_lags, columns=[f\"Lag{j}\" for j in range(1, lag+1)])\n",
    "    else:\n",
    "        final_lags_df = pd.DataFrame(columns=[f\"Lag{j}\" for j in range(1, lag+1)])\n",
    "    \n",
    "    return embedded_series_df, final_lags_df, series_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "embedded_series_df, final_lags_df, series_means = create_input_matrix(loaded_data, lag=10)\n",
    "\n",
    "output_file_path = \"/home/predict_price/stock_price/data_save/Direction/ver4/ver4_Volatility_Signal_seq_5_split_0.7_US_list_2001\"\n",
    "# output_file_path = \"/home/predict_price/stock_price/data_save/daily/ver5.5/test/test\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "if not os.path.exists(output_file_path):\n",
    "    os.makedirs(output_file_path)\n",
    "\n",
    "# Saving the embedded series DataFrame to a CSV file\n",
    "embedded_series_df.to_csv(f'{output_file_path}_embedded_series_df.csv', index=False)\n",
    "# Saving the final lags DataFrame to a CSV file\n",
    "final_lags_df.to_csv(f'{output_file_path}_final_lags_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_symbol_data(df, num_lags):\n",
    "#     all_features = []\n",
    "#     all_labels = []\n",
    "#     scale_params = {}\n",
    "\n",
    "#     for symbol in df['Symbol']. unique():\n",
    "#         symbol_data = df[df['Symbol'] == symbol]\n",
    "#         symbol_data = symbol_data.sort_values('Date')\n",
    "#         symbol_data.set_index('Date', inplace=True)\n",
    "        \n",
    "#         normalized_data, mean, std = normalize_data(symbol_data.drop(columns=['Symbol']))\n",
    "#         scale_params[symbol] = (mean, std)  # Store scaling parameters\n",
    "\n",
    "#         lagged_data = create_lagged_rows(normalized_data, num_lags)\n",
    "        \n",
    "#         features = lagged_data.iloc[:, :-1]  # All but the last column as features\n",
    "#         labels = lagged_data.iloc[:, -1]  # Last column as labels\n",
    "        \n",
    "#         all_features.append(features)\n",
    "#         all_labels.append(labels)\n",
    "\n",
    "#     all_features = pd.concat(all_features)\n",
    "#     all_labels = pd.concat(all_labels)\n",
    "\n",
    "#     return all_features, all_labels, scale_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Generate example data\n",
    "# np.random.seed(0)\n",
    "# dates = pd.date_range('20200101', periods=1000)  # Two months of data\n",
    "# data = pd.DataFrame({\n",
    "#     'Symbol': ['A']*500 + ['B']*500,\n",
    "#     'Date': dates[:500].tolist() + dates[:500].tolist(),  # Repeating the same date range for simplicity\n",
    "#     'Open': np.random.rand(1000) * 100,\n",
    "#     'High': np.random.rand(1000) * 100,\n",
    "#     'Low': np.random.rand(1000) * 100,\n",
    "#     'Close': np.random.rand(1000) * 100,\n",
    "#     'Volume': np.random.randint(1000, 10000, size=1000)\n",
    "# })\n",
    "\n",
    "# # Function to create lagged features for multiple columns\n",
    "# def create_lagged_features(data, num_lags, columns):\n",
    "#     df = data.copy()\n",
    "#     for col in columns:\n",
    "#         for lag in range(1, num_lags + 1):\n",
    "#             df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n",
    "#     return df.dropna()  # Dropping rows to remove NA values from lagging\n",
    "\n",
    "# # Columns to lag\n",
    "# columns_to_lag = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "\n",
    "# # Prepare data by symbol\n",
    "# grouped = data.groupby('Symbol')\n",
    "# prepared_data = [create_lagged_features(group, num_lags=20, columns=columns_to_lag) for _, group in grouped]\n",
    "\n",
    "# # Combine all symbols back into a single DataFrame\n",
    "# final_data = pd.concat(prepared_data)\n",
    "\n",
    "# # Print the head of the final dataset to see some of the transformed data\n",
    "# print(final_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
