{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from distutils.util import strtobool\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Converts the contents in a .tsf file into a dataframe and returns it along with other meta-data of the dataset: frequency, horizon, whether the dataset contains missing values and whether the series have equal lengths\n",
    "#\n",
    "# Parameters\n",
    "# full_file_path_and_name - complete .tsf file path\n",
    "# replace_missing_vals_with - a term to indicate the missing values in series in the returning dataframe\n",
    "# value_column_name - Any name that is preferred to have as the name of the column containing series values in the returning dataframe\n",
    "\n",
    "def convert_tsf_to_dataframe(\n",
    "    full_file_path_and_name,\n",
    "    replace_missing_vals_with=\"NaN\",\n",
    "    value_column_name=\"series_value\",\n",
    "):\n",
    "    col_names = []\n",
    "    col_types = []\n",
    "    all_data = {}\n",
    "    line_count = 0\n",
    "    frequency = None\n",
    "    forecast_horizon = None\n",
    "    contain_missing_values = None\n",
    "    contain_equal_length = None\n",
    "    found_data_tag = False\n",
    "    found_data_section = False\n",
    "    started_reading_data_section = False\n",
    "\n",
    "    with open(full_file_path_and_name, \"r\", encoding=\"cp1252\") as file:\n",
    "        for line in file:\n",
    "            # Strip white space from start/end of line\n",
    "            line = line.strip()\n",
    "\n",
    "            if line:\n",
    "                if line.startswith(\"@\"):  # Read meta-data\n",
    "                    if not line.startswith(\"@data\"):\n",
    "                        line_content = line.split(\" \")\n",
    "                        if line.startswith(\"@attribute\"):\n",
    "                            if (\n",
    "                                len(line_content) != 3\n",
    "                            ):  # Attributes have both name and type\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            col_names.append(line_content[1])\n",
    "                            col_types.append(line_content[2])\n",
    "                        else:\n",
    "                            if (\n",
    "                                len(line_content) != 2\n",
    "                            ):  # Other meta-data have only values\n",
    "                                raise Exception(\"Invalid meta-data specification.\")\n",
    "\n",
    "                            if line.startswith(\"@frequency\"):\n",
    "                                frequency = line_content[1]\n",
    "                            elif line.startswith(\"@horizon\"):\n",
    "                                forecast_horizon = int(line_content[1])\n",
    "                            elif line.startswith(\"@missing\"):\n",
    "                                contain_missing_values = bool(\n",
    "                                    strtobool(line_content[1])\n",
    "                                )\n",
    "                            elif line.startswith(\"@equallength\"):\n",
    "                                contain_equal_length = bool(strtobool(line_content[1]))\n",
    "\n",
    "                    else:\n",
    "                        if len(col_names) == 0:\n",
    "                            raise Exception(\n",
    "                                \"Missing attribute section. Attribute section must come before data.\"\n",
    "                            )\n",
    "\n",
    "                        found_data_tag = True\n",
    "                elif not line.startswith(\"#\"):\n",
    "                    if len(col_names) == 0:\n",
    "                        raise Exception(\n",
    "                            \"Missing attribute section. Attribute section must come before data.\"\n",
    "                        )\n",
    "                    elif not found_data_tag:\n",
    "                        raise Exception(\"Missing @data tag.\")\n",
    "                    else:\n",
    "                        if not started_reading_data_section:\n",
    "                            started_reading_data_section = True\n",
    "                            found_data_section = True\n",
    "                            all_series = []\n",
    "\n",
    "                            for col in col_names:\n",
    "                                all_data[col] = []\n",
    "\n",
    "                        full_info = line.split(\":\")\n",
    "\n",
    "                        if len(full_info) != (len(col_names) + 1):\n",
    "                            raise Exception(\"Missing attributes/values in series.\")\n",
    "\n",
    "                        series = full_info[len(full_info) - 1]\n",
    "                        series = series.split(\",\")\n",
    "\n",
    "                        if len(series) == 0:\n",
    "                            raise Exception(\n",
    "                                \"A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series. Missing values should be indicated with ? symbol\"\n",
    "                            )\n",
    "\n",
    "                        numeric_series = []\n",
    "\n",
    "                        for val in series:\n",
    "                            if val == \"?\":\n",
    "                                numeric_series.append(replace_missing_vals_with)\n",
    "                            else:\n",
    "                                numeric_series.append(float(val))\n",
    "\n",
    "                        if numeric_series.count(replace_missing_vals_with) == len(\n",
    "                            numeric_series\n",
    "                        ):\n",
    "                            raise Exception(\n",
    "                                \"All series values are missing. A given series should contains a set of comma separated numeric values. At least one numeric value should be there in a series.\"\n",
    "                            )\n",
    "\n",
    "                        all_series.append(pd.Series(numeric_series).array)\n",
    "\n",
    "                        for i in range(len(col_names)):\n",
    "                            att_val = None\n",
    "                            if col_types[i] == \"numeric\":\n",
    "                                att_val = int(full_info[i])\n",
    "                            elif col_types[i] == \"string\":\n",
    "                                att_val = str(full_info[i])\n",
    "                            elif col_types[i] == \"date\":\n",
    "                                att_val = datetime.strptime(\n",
    "                                    full_info[i], \"%Y-%m-%d %H-%M-%S\"\n",
    "                                )\n",
    "                            else:\n",
    "                                raise Exception(\n",
    "                                    \"Invalid attribute type.\"\n",
    "                                )  # Currently, the code supports only numeric, string and date types. Extend this as required.\n",
    "\n",
    "                            if att_val is None:\n",
    "                                raise Exception(\"Invalid attribute value.\")\n",
    "                            else:\n",
    "                                all_data[col_names[i]].append(att_val)\n",
    "\n",
    "                line_count = line_count + 1\n",
    "\n",
    "        if line_count == 0:\n",
    "            raise Exception(\"Empty file.\")\n",
    "        if len(col_names) == 0:\n",
    "            raise Exception(\"Missing attribute section.\")\n",
    "        if not found_data_section:\n",
    "            raise Exception(\"Missing series information under data section.\")\n",
    "\n",
    "        all_data[value_column_name] = all_series\n",
    "        loaded_data = pd.DataFrame(all_data)\n",
    "\n",
    "        return (\n",
    "            loaded_data,\n",
    "            frequency,\n",
    "            forecast_horizon,\n",
    "            contain_missing_values,\n",
    "            contain_equal_length,\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    series_name     start_timestamp  \\\n",
      "0            T1 2015-07-01 12:00:00   \n",
      "1            T2 2015-07-01 12:00:00   \n",
      "2            T3 2015-07-01 12:00:00   \n",
      "3            T4 2015-07-01 12:00:00   \n",
      "4            T5 2015-07-01 12:00:00   \n",
      "..          ...                 ...   \n",
      "409        T410 2017-01-01 12:00:00   \n",
      "410        T411 2017-01-01 12:00:00   \n",
      "411        T412 2017-01-01 12:00:00   \n",
      "412        T413 2017-06-12 12:00:00   \n",
      "413        T414 2017-04-27 12:00:00   \n",
      "\n",
      "                                          series_value  \n",
      "0    [605.0, 586.0, 586.0, 559.0, 511.0, 443.0, 422...  \n",
      "1    [3124.0, 2990.0, 2862.0, 2809.0, 2544.0, 2201....  \n",
      "2    [1828.0, 1806.0, 1897.0, 1750.0, 1679.0, 1620....  \n",
      "3    [6454.0, 6324.0, 6075.0, 5949.0, 5858.0, 5579....  \n",
      "4    [4263.0, 4297.0, 4236.0, 4080.0, 3883.0, 3672....  \n",
      "..                                                 ...  \n",
      "409  [153.0, 196.0, 163.0, 131.0, 82.0, 53.0, 57.0,...  \n",
      "410  [24.0, 30.0, 22.0, 14.0, 19.0, 20.0, 24.0, 45....  \n",
      "411  [19.0, 30.0, 12.0, 16.0, 13.0, 15.0, 15.0, 21....  \n",
      "412  [21.0, 16.0, 17.0, 19.0, 16.0, 22.0, 21.0, 15....  \n",
      "413  [15.0, 13.0, 13.0, 16.0, 26.0, 51.0, 91.0, 103...  \n",
      "\n",
      "[414 rows x 3 columns]\n",
      "hourly\n",
      "48\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Example of usage\n",
    "loaded_data, frequency, forecast_horizon, contain_missing_values, contain_equal_length = convert_tsf_to_dataframe(r\"C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Time_Series_Data\\m4_hourly_dataset.tsf\")\n",
    "\n",
    "print(loaded_data)\n",
    "print(frequency)\n",
    "print(forecast_horizon)\n",
    "print(contain_missing_values)\n",
    "print(contain_equal_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def normalize_data(df):\n",
    "    \"\"\" Normalize the dataframe using mean and standard deviation. \"\"\"\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\" Create sequences from the dataset. \"\"\"\n",
    "    sequences = [data[i:i + sequence_length] for i in range(len(data) - sequence_length + 1)]\n",
    "    return np.array(sequences)\n",
    "\n",
    "def prepare_symbol_data(df, sequence_length):\n",
    "    \"\"\" Prepare data for each symbol. \"\"\"\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "\n",
    "    for symbol in df['Symbol'].unique():\n",
    "        symbol_data = df[df['Symbol'] == symbol]\n",
    "        symbol_data = symbol_data.sort_values('Date')\n",
    "        symbol_data.set_index('Date', inplace=True)\n",
    "        \n",
    "        normalized_data = normalize_data(symbol_data.drop(columns=['Symbol']))\n",
    "        \n",
    "        sequences = create_sequences(normalized_data.drop(columns=['Close']), sequence_length)\n",
    "        labels = create_sequences(normalized_data['Close'], sequence_length)\n",
    "        \n",
    "        all_sequences.append(sequences)\n",
    "        all_labels.append(labels[:, -1])  # Assuming prediction of the closing price\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    all_sequences = np.concatenate(all_sequences)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return all_sequences, all_labels\n",
    "\n",
    "def split_data(sequences, labels, train_frac=0.7, valid_frac=0.15):\n",
    "    \"\"\" Split the data into train, validation, and test sets. \"\"\"\n",
    "    total_count = sequences.shape[0]\n",
    "    train_size = int(total_count * train_frac)\n",
    "    valid_size = int(total_count * valid_frac)\n",
    "    test_size = total_count - train_size - valid_size\n",
    "\n",
    "    indices = np.random.permutation(total_count)\n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size + valid_size]\n",
    "    test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "    X_train, y_train = sequences[train_indices], labels[train_indices]\n",
    "    X_valid, y_valid = sequences[valid_indices], labels[valid_indices]\n",
    "    X_test, y_test = sequences[test_indices], labels[test_indices]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def convert_to_tensors(*arrays):\n",
    "    \"\"\" Convert arrays to PyTorch tensors. \"\"\"\n",
    "    return tuple(torch.tensor(array, dtype=torch.float32) for array in arrays)\n",
    "\n",
    "def create_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=32):\n",
    "    \"\"\" Create DataLoader for train, validation, and test sets. \"\"\"\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "# Example usage assuming 'df' is your DataFrame containing the stock data\n",
    "sequence_length = 10\n",
    "all_sequences, all_labels = prepare_symbol_data(df, sequence_length)\n",
    "\n",
    "# Split and convert to tensors\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(all_sequences, all_labels)\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = convert_to_tensors(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader, valid_loader, test_loader = create_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def normalize_data(df):\n",
    "    \"\"\" Normalize the dataframe using mean and standard deviation, returns normalized data and scale parameters. \"\"\"\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - mean) / std, mean, std\n",
    "\n",
    "def inverse_normalize_data(normalized_data, mean, std):\n",
    "    \"\"\" Revert data back to its original scale using stored mean and std. \"\"\"\n",
    "    return normalized_data * std + mean\n",
    "\n",
    "def create_sequences(data, sequence_length, padding_value=0):\n",
    "    \"\"\" Optionally pad sequences to handle variable lengths. \"\"\"\n",
    "    sequences = [data[i:i + sequence_length] if len(data) >= sequence_length else np.pad(data[i:], (0, sequence_length-len(data[i:])), 'constant', constant_values=(padding_value,)) for i in range(len(data) - sequence_length + 1)]\n",
    "    return np.array(sequences)\n",
    "\n",
    "def prepare_symbol_data(df, sequence_length):\n",
    "    all_sequences = []\n",
    "    all_labels = []\n",
    "    scale_params = {}\n",
    "\n",
    "    for symbol in df['Symbol'].unique():\n",
    "        symbol_data = df[df['Symbol'] == symbol]\n",
    "        symbol_data = symbol_data.sort_values('Date')\n",
    "        symbol_data.set_index('Date', inplace=True)\n",
    "        \n",
    "        normalized_data, mean, std = normalize_data(symbol_data.drop(columns=['Symbol']))\n",
    "        scale_params[symbol] = (mean, std)  # Store scaling parameters\n",
    "        \n",
    "        sequences = create_sequences(normalized_data.drop(columns=['Close']), sequence_length)\n",
    "        labels = create_sequences(normalized_data['Close'], sequence_length)\n",
    "        \n",
    "        all_sequences.append(sequences)\n",
    "        all_labels.append(labels[:, -1])\n",
    "\n",
    "    all_sequences = np.concatenate(all_sequences)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    return all_sequences, all_labels, scale_params\n",
    "\n",
    "def split_data(sequences, labels, train_frac=0.7, valid_frac=0.15):\n",
    "    \"\"\" Split the data into train, validation, and test sets. \"\"\"\n",
    "    total_count = sequences.shape[0]\n",
    "    train_size = int(total_count * train_frac)\n",
    "    valid_size = int(total_count * valid_frac)\n",
    "    test_size = total_count - train_size - valid_size\n",
    "\n",
    "    indices = np.random.permutation(total_count)\n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size + valid_size]\n",
    "    test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "    X_train, y_train = sequences[train_indices], labels[train_indices]\n",
    "    X_valid, y_valid = sequences[valid_indices], labels[valid_indices]\n",
    "    X_test, y_test = sequences[test_indices], labels[test_indices]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def convert_to_tensors(*arrays):\n",
    "    \"\"\" Convert arrays to PyTorch tensors. \"\"\"\n",
    "    return tuple(torch.tensor(array, dtype=torch.float32) for array in arrays)\n",
    "\n",
    "def create_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test, batch_size=32):\n",
    "    \"\"\" Create DataLoader for train, validation, and test sets. \"\"\"\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(TensorDataset(X_valid, y_valid), batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "# Example usage assuming 'df' is your DataFrame containing the stock data\n",
    "sequence_length = 10\n",
    "all_sequences, all_labels = prepare_symbol_data(df, sequence_length)\n",
    "\n",
    "# Split and convert to tensors\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(all_sequences, all_labels)\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = convert_to_tensors(X_train, y_train, X_valid, y_valid, X_test, y_test)\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader, valid_loader, test_loader = create_dataloaders(X_train, y_train, X_valid, y_valid, X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
