{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_seq_len + merged_future_prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import datetime\n",
    "import calendar\n",
    "import warnings\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.modules.transformer import TransformerDecoder, TransformerDecoderLayer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def create_sequences(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(input_data) - sequence_length):\n",
    "        seq = input_data[i:i+sequence_length]\n",
    "        sequences.append(seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "# class DecoderOnlyTransformerModel(nn.Module):\n",
    "#     def __init__(self, n_features, nhead=None, nhid=None, nlayers=None, dropout=None,\n",
    "#                  l1_regularization=0, l2_regularization=0,\n",
    "#                  activation_function=None):\n",
    "#         super(DecoderOnlyTransformerModel, self).__init__()\n",
    "\n",
    "#         self.pos_encoder = nn.Sequential(\n",
    "#             nn.Linear(n_features, nhid),\n",
    "#             activation_function,\n",
    "#             nn.Linear(nhid, nhid),\n",
    "#             activation_function\n",
    "#         )\n",
    "\n",
    "#         decoder_layers = TransformerDecoderLayer(nhid, nhead)\n",
    "#         self.transformer_decoder = TransformerDecoder(decoder_layers,nlayers)\n",
    "\n",
    "#         self.decoder = nn.Linear(nhid,n_features)\n",
    "\n",
    "#         self.l1_regularization = l1_regularization\n",
    "#         self.l2_regularization = l2_regularization\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # initrange = 0.1\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[0].weight)\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[2].weight)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "#     def regularization_loss(self):\n",
    "#         l1_loss = 0\n",
    "#         l2_loss = 0\n",
    "#         for param in self.parameters():\n",
    "#             l1_loss += torch.norm(param, 1)\n",
    "#             l2_loss += torch.norm(param, 2) ** 2\n",
    "#         return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pos_encoder(x)\n",
    "\n",
    "#         # Generating target mask\n",
    "#         tgt_mask = self.generate_square_subsequent_mask(x.size(0)).to(x.device)\n",
    "\n",
    "#         # Processing the entire input at once\n",
    "#         output = self.transformer_decoder(x, x, tgt_mask=tgt_mask)\n",
    "\n",
    "#         # Apply the final linear layer\n",
    "#         output = self.decoder(output)\n",
    "#         # Selecting the last element from each sequence\n",
    "#         output = output[:, -1, :].unsqueeze(1)\n",
    "#         return output\n",
    "    \n",
    "# class TransformerModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_features, nhead=None, nhid=None, nlayers=None, dropout=None, l1_regularization=0, l2_regularization=0, activation_function = None):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "\n",
    "#         self.activation_function = activation_function\n",
    "\n",
    "#         # Model architecture\n",
    "#         self.pos_encoder = nn.Sequential(\n",
    "#             nn.Linear(n_features, nhid),\n",
    "#             self.activation_function,\n",
    "#             nn.LayerNorm(nhid),\n",
    "#             nn.Dropout(dropout),\n",
    "#             nn.Linear(nhid, nhid),\n",
    "#             self.activation_function,\n",
    "#             nn.LayerNorm(nhid)\n",
    "#         )\n",
    "#         encoder_layers = TransformerEncoderLayer(nhid, nhead, nhid, dropout,)\n",
    "#         # encoder_layers.self_attn.batch_first = True\n",
    "#         self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "#         self.decoder = nn.Linear(nhid, n_features)\n",
    "#         self.l1_regularization = l1_regularization\n",
    "#         self.l2_regularization = l2_regularization\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         for layer in self.pos_encoder:\n",
    "#             if isinstance(layer, nn.Linear):\n",
    "#                 nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "#     def regularization_loss(self):\n",
    "#         l1_loss = 0\n",
    "#         l2_loss = 0\n",
    "#         for param in self.parameters():\n",
    "#             l1_loss += torch.norm(param, 1)\n",
    "#             l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "#         return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "#     def forward(self, src):\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_encoder(src)\n",
    "#         output = self.decoder(output)\n",
    "#         output = output[:, -1:, :]\n",
    "#         return output    \n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, n_features, n_filters=64, filter_size=3, dropout=0.5, activation_function=None, l1_regularization=0, l2_regularization=0, sequence_length=5):\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.activation = activation_function\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=n_features, out_channels=n_filters, kernel_size=min(filter_size, sequence_length), padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(n_filters, n_filters * 2, kernel_size=min(filter_size, sequence_length // 2), padding=1)  # Adjusted kernel size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Dynamically calculate the flattened size after convolutions and pooling\n",
    "        test_input = torch.rand(1, n_features, sequence_length)  # Simulate a random input to the model\n",
    "        test_output = self.pool(self.conv1(test_input))\n",
    "        test_output = self.pool(self.conv2(test_output))\n",
    "        self.flattened_size = test_output.numel() // test_output.shape[0]\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattened_size, 100)\n",
    "        self.fc2 = nn.Linear(100, n_features)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        l1_loss, l2_loss = 0, 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, 1)\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # Adjusting input shape for Conv1d\n",
    "        x = self.activation(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.activation(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output for the dense layer\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        return x\n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam, criterion=torch.nn.MSELoss, \n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, max_norm=1.0, nan_patience=1, num_workers=4, \n",
    "                pin_memory=False, validation_frequency=1, save_directory=None, trial=1, model_ver=None):\n",
    "    # Setup the device (GPU or CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)  # Move the model to the specified device\n",
    "\n",
    "    # Initialize the optimizer and loss function\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion()\n",
    "\n",
    "    # Prepare DataLoader for training data\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    # Prepare DataLoader for validation data\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    # Initialize variables for early stopping and best validation loss tracking\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    nan_counter = 0  # Counter for epochs with NaN loss\n",
    "    stopped_early = False  # Flag to indicate if training stopped early\n",
    "\n",
    "    # Initialize gradient scaler for mixed precision training\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # Variables to store training and validation losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        epoch_train_losses = []  # List to store losses for each batch\n",
    "\n",
    "        # Loop over batches of data\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train, batch_y_train = batch_X_train.to(device), batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "\n",
    "            # Forward pass with autocast for mixed precision\n",
    "            with autocast():\n",
    "                output = model(batch_X_train)\n",
    "                # Recalculate the loss with the correctly adjusted target tensor\n",
    "                loss = criterion(output, batch_y_train)\n",
    "                # Assert to ensure shapes match\n",
    "                assert output.shape == batch_y_train.shape, \"Shape mismatch between model output and targets\"\n",
    "                reg_loss = model.regularization_loss() if hasattr(model, 'regularization_loss') else 0\n",
    "                total_loss = loss + reg_loss  # Combine loss with regularization loss\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "                if nan_counter >= nan_patience:\n",
    "                    print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                    stopped_early = True\n",
    "                    break\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            scaler.scale(total_loss).backward()  # Scale loss for mixed precision\n",
    "            scaler.unscale_(optimizer)  # Unscale gradients for gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)  # Clip gradients\n",
    "            scaler.step(optimizer)  # Optimizer step\n",
    "            scaler.update()  # Update the scaler\n",
    "\n",
    "            epoch_train_losses.append(total_loss.item())  # Store loss\n",
    "            \n",
    "        if stopped_early:\n",
    "            break\n",
    "\n",
    "        # Store average training loss for the epoch\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        # Validation phase\n",
    "        if epoch % validation_frequency == 0:\n",
    "            model.eval()  # Set model to evaluation mode\n",
    "            epoch_val_losses = []  # List to store validation losses\n",
    "\n",
    "            # Loop over batches of validation data\n",
    "            with torch.no_grad():\n",
    "                for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                    batch_X_valid, batch_y_valid = batch_X_valid.to(device), batch_y_valid.to(device)\n",
    "                    valid_output = model(batch_X_valid)\n",
    "                    val_loss = criterion(valid_output, batch_y_valid)\n",
    "                    epoch_val_losses.append(val_loss.item())  # Store validation loss\n",
    "\n",
    "            # Calculate average validation loss and update early stopping\n",
    "            val_losses.append(np.mean(epoch_val_losses))\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.5f}, Val Loss = {val_losses[-1]:.5f}\")\n",
    "\n",
    "            # Check for validation loss improvement\n",
    "            if val_losses[-1] < best_val_loss - min_delta:\n",
    "                best_val_loss = val_losses[-1]  # Update best validation loss\n",
    "                early_stopping_counter = 0  # Reset early stopping counter\n",
    "                # Save model checkpoint if directory is provided\n",
    "                # Save the model\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"model_ver{model_ver}_trial{trial}_epoch{epoch}.pt\")\n",
    "                else:\n",
    "                    save_path = f\"model_trial_{trial}.pt\"\n",
    "\n",
    "                torch.save(model, save_path)\n",
    "                print(f\"Validation loss improved to {best_val_loss:.5f} at epoch {epoch}, saving model...\")\n",
    "            else:\n",
    "                early_stopping_counter += 1  # Increment early stopping counter\n",
    "\n",
    "            # Trigger early stopping if no improvement for 'patience' epochs\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "                break  # Exit training loop\n",
    "\n",
    "    return train_losses, val_losses, stopped_early  # Return loss history and early stopping flag\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# def model_save(n_trials=1, model_save=True, save_directory=None, plot_loss=True, output_file_path=None, model_ver=None):\n",
    "\n",
    "#     if save_directory and not os.path.exists(save_directory):\n",
    "#         os.makedirs(save_directory)\n",
    "\n",
    "#     all_results_params = []\n",
    "\n",
    "#     for trial in range(n_trials):\n",
    "#         print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "#         start = timeit.default_timer()\n",
    "\n",
    "#         # seq_len = random.choice(range(3, 6))\n",
    "#         # # nhead = random.choice(range(20, 21))\n",
    "#         # nhid = random.choice(range(5000, 5001))\n",
    "#         # nlayers = random.choice(range(1, 2))\n",
    "#         # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "#         # # activation_function = random.choice([torch.nn.GELU(), torch.nn.SiLU()])\n",
    "#         # # activation_function = random.choice([torch.nn.Tanh()])\n",
    "#         # # activation_function = random.choice([torch.nn.Mish()])\n",
    "#         # dropout = 0 * random.choice(range(10, 11))\n",
    "#         # optimizer = random.choice([torch.optim.AdamW])\n",
    "#         # criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "#         # # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.KLDivLoss])\n",
    "#         # n_epochs = random.choice(range(300, 501))\n",
    "#         # batch_size = random.choice(range(256, 257))\n",
    "#         # learning_rate = 0.00001 * random.choice(range(10, 11))\n",
    "#         # patience = random.choice(range(30, 31))\n",
    "#         # min_delta = 0.00001 * random.choice(range(1, 2))\n",
    "#         # l1_regularization = 0.0000001 * random.choice(range(4, 5))\n",
    "#         # l2_regularization = l1_regularization\n",
    "\n",
    "#         seq_len = random.choice(range(5, 6))\n",
    "#         nhead = random.choice(range(21, 31))\n",
    "#         nhid = nhead * random.choice(range(30, 31))\n",
    "#         nlayers = random.choice(range(1, 2))\n",
    "#         # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "#         # activation_function = random.choice([torch.nn.GELU(), torch.nn.SiLU()])\n",
    "#         activation_function = random.choice([torch.nn.Tanh(), torch.nn.ELU(), torch.nn.Mish()])\n",
    "#         # activation_function = random.choice([torch.nn.Mish()])\n",
    "#         dropout = 0.01 * random.choice(range(5, 21))\n",
    "#         optimizer = random.choice([torch.optim.Adam])\n",
    "#         criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "#         # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.KLDivLoss, torch.nn.SmoothL1Loss])\n",
    "#         n_epochs = random.choice(range(200, 301))\n",
    "#         batch_size = random.choice(range(256, 257))\n",
    "#         learning_rate = 0.00001 * random.choice(range(10, 11))\n",
    "#         patience = random.choice(range(10, 11))\n",
    "#         min_delta = 0.00001 * random.choice(range(1, 2))\n",
    "#         l1_regularization = 0.0000001 * random.choice(range(1, 2))\n",
    "#         l2_regularization = l1_regularization\n",
    "\n",
    "#         # Load tensors\n",
    "#         X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "#         y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "#         X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "#         y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "\n",
    "#         # Initialize the model\n",
    "#         if model_ver == 'Transformer':\n",
    "#             model = TransformerModel(n_features=X_train.shape[2], nhead=nhead, nhid=nhid, nlayers=nlayers, activation_function=activation_function, dropout=dropout,\n",
    "#                 l1_regularization=l1_regularization, l2_regularization=l2_regularization)\n",
    "#         elif model_ver == 'Decoder':\n",
    "#             model = DecoderOnlyTransformerModel(n_features=X_train.shape[2], nhead=nhead, nhid=nhid, nlayers=nlayers, activation_function=activation_function, dropout=dropout,\n",
    "#             l1_regularization=l1_regularization, l2_regularization=l2_regularization)\n",
    "\n",
    "#         # Train the model\n",
    "#         train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, criterion=criterion,\n",
    "#                                                               batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, trial=trial, \n",
    "#                                                               model_ver=model_ver, save_directory=save_directory)\n",
    "#         # Check if training stopped early due to NaNs or not\n",
    "#         if stopped_early:\n",
    "#             print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "#             # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "#             continue\n",
    "\n",
    "#         if plot_loss:\n",
    "#             plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "#         # # Save the model\n",
    "#         # if model_save:\n",
    "#         #     if save_directory:\n",
    "#         #         save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "#         #     else:\n",
    "#         #         save_path = f\"model_trial_{trial}.pt\"\n",
    "#         #     torch.save(model, save_path)\n",
    "\n",
    "#         # Add the results to the results dataframe\n",
    "#         params = {\"seq_len\": seq_len, \"nhead\": nhead, \"nhid\": nhid, \"nlayers\": nlayers, \"activation_function\": activation_function,\n",
    "#                 \"optimizer\": optimizer, \"criterion\": criterion, \"dropout\": dropout, \"n_epochs\": n_epochs,\n",
    "#                 \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"patience\": patience, \"min_delta\": min_delta,\n",
    "#                 \"l1_regularization\": l1_regularization, \"l2_regularization\": l2_regularization,\n",
    "#                 }\n",
    "\n",
    "#         results_params = {**params, \"trial\": trial+1, \"train loss\": train_losses[-1], \"valid loss\": val_losses[-1]}\n",
    "\n",
    "#         all_results_params.append(results_params)\n",
    "#         all_results_params_df = pd.DataFrame(all_results_params)\n",
    "\n",
    "#         if save_directory:\n",
    "#             all_results_params_df.to_csv(os.path.join(save_directory, f\"all_results_params_{trial}.csv\"))\n",
    "\n",
    "#         end = timeit.default_timer()\n",
    "#         # Calculate and print duration\n",
    "#         duration = end - start\n",
    "#         print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")\n",
    "\n",
    "#     return all_results_params_df\n",
    "\n",
    "def model_save(n_trials=1, model_save=True, save_directory=None, plot_loss=True, output_file_path=None):\n",
    "\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    all_results_params = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # # Adjust these parameters for your CNN\n",
    "        # n_filters = random.choice([16, 32, 64, 128, 256])\n",
    "        # filter_size = random.choice([3, 5, 7, 9])\n",
    "        # dropout = 0.01 * random.choice(range(5, 21))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.ReLU(), torch.nn.ELU(), torch.nn.Mish()])\n",
    "        # optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW])\n",
    "        # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.SmoothL1Loss])\n",
    "        # n_epochs = random.choice(range(200, 301))\n",
    "        # batch_size = random.choice(range(256, 257))\n",
    "        # learning_rate = 0.00001 * random.choice(range(5, 51))\n",
    "        # patience = random.choice(range(1, 2))\n",
    "        # min_delta = 0.0001 * random.choice(range(1, 2))\n",
    "        # l1_regularization = 0.0000001 * random.choice(range(5, 21))\n",
    "        # l2_regularization = l1_regularization\n",
    "        # sequence_length = random.choice(range(5, 6)) \n",
    "         \n",
    "        # Adjust these parameters for your CNN\n",
    "        n_filters = random.choice(range(256, 512))\n",
    "        filter_size = random.choice([3, 5])\n",
    "        dropout = 0.01 * random.choice(range(5, 21))\n",
    "        activation_function = random.choice([torch.nn.Tanh(), torch.nn.ReLU(), torch.nn.ELU(), torch.nn.Mish()])\n",
    "        optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW])\n",
    "        criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        n_epochs = random.choice(range(200, 301))\n",
    "        batch_size = random.choice(range(256, 257))\n",
    "        learning_rate = 0.00001 * random.choice(range(10, 201))\n",
    "        patience = random.choice(range(3, 4))\n",
    "        min_delta = 0.0001 * random.choice(range(10, 21))\n",
    "        l1_regularization = 0.0000001 * random.choice(range(5, 21))\n",
    "        l2_regularization = l1_regularization\n",
    "        sequence_length = random.choice(range(5, 6))\n",
    "\n",
    "        # # Construct the full file paths\n",
    "        # train_data_file_path = os.path.join(output_file_path, 'train_data.csv')\n",
    "        # valid_data_file_path = os.path.join(output_file_path, 'valid_data.csv')\n",
    "\n",
    "        # # Read the CSV files into Pandas DataFrames\n",
    "        # train_data = pd.read_csv(train_data_file_path)\n",
    "        # valid_data = pd.read_csv(valid_data_file_path)\n",
    "\n",
    "        # X_train_tensor = torch.tensor(train_data.values, dtype=torch.float32)\n",
    "        # X_valid_tensor = torch.tensor(valid_data.values, dtype=torch.float32)\n",
    "\n",
    "        # X_train = create_sequences(X_train_tensor, sequence_length)\n",
    "        # X_valid = create_sequences(X_valid_tensor, sequence_length)\n",
    "\n",
    "        # y_train = X_train_tensor[sequence_length:, :]\n",
    "        # y_valid = X_valid_tensor[sequence_length:, :]\n",
    "        \n",
    "        # Load tensors\n",
    "        X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "        y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "        X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "\n",
    "        # Initialize the CNN model\n",
    "        model = CNNModel(n_features=X_train.shape[2], n_filters=n_filters, filter_size=filter_size, dropout=dropout, activation_function=activation_function, \n",
    "                        l1_regularization=l1_regularization, l2_regularization=l2_regularization, sequence_length=sequence_length)\n",
    "        \n",
    "        # Train the model with the adjusted function for CNN\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, criterion=criterion,\n",
    "                                                                   batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, trial=trial, save_directory=save_directory)\n",
    "\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # # Save the model\n",
    "        # if model_save:\n",
    "        #     if save_directory:\n",
    "        #         save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "        #     else:\n",
    "        #         save_path = f\"model_trial_{trial}.pt\"\n",
    "        #     torch.save(model, save_path)\n",
    "\n",
    "        # For example, adjust parameters to log for CNN\n",
    "        params = {\n",
    "            \"n_filters\": n_filters, \"filter_size\": filter_size, \"activation_function\": activation_function,\n",
    "                \"optimizer\": optimizer, \"criterion\": criterion, \"dropout\": dropout, \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"patience\": patience, \"min_delta\": min_delta,\n",
    "                \"l1_regularization\": l1_regularization, \"l2_regularization\": l2_regularization,\n",
    "        }\n",
    "\n",
    "        results_params = {**params, \"trial\": trial+1, \"train loss\": train_losses[-1], \"valid loss\": val_losses[-1]}\n",
    "\n",
    "        all_results_params.append(results_params)\n",
    "        all_results_params_df = pd.DataFrame(all_results_params)\n",
    "\n",
    "        if save_directory:\n",
    "            all_results_params_df.to_csv(os.path.join(save_directory, f\"all_results_params_{trial}.csv\"))\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")\n",
    "\n",
    "    return all_results_params_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 3\n",
    "# validation_frequency = 2\n",
    "# batch_size_data = 100000\n",
    "\n",
    "save_directory = \"/home/predict_price/stock_price/save_model/CNN/ver4/ver4_no_target/ver4_seq_5_split_0.7_World_list_2001\"\n",
    "# output_file_path_params = \"/home/predict_price/stock_price/save_model/ver5.8/ver5.8_decoder_no_split/ver5.8_decoder_no_split\"\n",
    "output_file_path = \"/home/predict_price/stock_price/data_save/CNN/ver4/ver4_no_target/ver4_seq_5_split_0.7_World_list_2001\"\n",
    "all_results_params_df = model_save(n_trials=n_trials, model_save=True, save_directory=save_directory, plot_loss=True, output_file_path=output_file_path)\n",
    "# Save results_df\n",
    "all_results_params_df.to_csv(f'{save_directory}_all_results_params_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
