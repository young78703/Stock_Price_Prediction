{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_seq_len + merged_future_prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import datetime\n",
    "import calendar\n",
    "import warnings\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.modules.transformer import TransformerDecoder, TransformerDecoderLayer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "class DecoderOnlyTransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, nhead=8, nhid=64, nlayers=6, dropout=0.1,\n",
    "                 l1_regularization=0, l2_regularization=0,\n",
    "                 activation_function=torch.nn.ReLU()):\n",
    "        super(DecoderOnlyTransformerModel, self).__init__()\n",
    "\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, nhid),\n",
    "            activation_function,\n",
    "            nn.Linear(nhid, nhid),\n",
    "            activation_function\n",
    "        )\n",
    "\n",
    "        decoder_layers = TransformerDecoderLayer(nhid, nhead)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers,nlayers)\n",
    "\n",
    "        self.decoder = nn.Linear(nhid,n_features)\n",
    "\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "\n",
    "    def init_weights(self):\n",
    "        # initrange = 0.1\n",
    "        nn.init.xavier_uniform_(self.pos_encoder[0].weight)\n",
    "        nn.init.xavier_uniform_(self.pos_encoder[2].weight)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, 1)\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "\n",
    "        # Generating target mask\n",
    "        tgt_mask = self.generate_square_subsequent_mask(x.size(0)).to(x.device)\n",
    "\n",
    "        # Processing the entire input at once\n",
    "        output = self.transformer_decoder(x, x, tgt_mask=tgt_mask)\n",
    "\n",
    "        # Apply the final linear layer\n",
    "        output = self.decoder(output)\n",
    "        # Selecting the last element from each sequence\n",
    "        output = output[:, -1, :].unsqueeze(1)\n",
    "        return output\n",
    "    \n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, nhead=8, nhid=64, nlayers=1, dropout=0.1, l1_regularization=0, l2_regularization=0, activation_function = torch.nn.LeakyReLU()):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Model architecture\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(n_features, nhid),\n",
    "            self.activation_function,\n",
    "            nn.LayerNorm(nhid),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(nhid, nhid),\n",
    "            self.activation_function,\n",
    "            nn.LayerNorm(nhid)\n",
    "        )\n",
    "\n",
    "        encoder_layers = TransformerEncoderLayer(nhid, nhead, nhid, dropout,)\n",
    "        # encoder_layers.self_attn.batch_first = True\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.decoder = nn.Linear(nhid, n_features)\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.pos_encoder:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "        self.decoder.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, 1)\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        output = output[:, -1:, :]\n",
    "        return output\n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam, criterion=nn.MSELoss,\n",
    "                l1_regularization=0, l2_regularization=0, activation_function=None,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "            reg_loss = model.regularization_loss()\n",
    "            total_loss = loss + reg_loss\n",
    "            #print(loss.device)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            total_loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(total_loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.5f}, Val Loss = {val_losses[-1]:.5f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "#  For mixed precision training\n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, \n",
    "                optimizer=torch.optim.Adam, criterion=torch.nn.MSELoss, \n",
    "                l1_regularization=0, l2_regularization=0, activation_function=None,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, \n",
    "                max_norm=1.0, nan_patience=1, num_workers=8, pin_memory=True, validation_frequency=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # Create DataLoaders with optimizations\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, \n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, \n",
    "                              num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    # Early stopping and NaN stopping parameters\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    " \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train, batch_y_train = batch_X_train.to(device), batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Mixed precision context\n",
    "            with autocast():\n",
    "                output = model(batch_X_train)\n",
    "                loss = criterion(output, batch_y_train)\n",
    "                reg_loss = model.regularization_loss()\n",
    "                total_loss = loss + reg_loss\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "                if nan_counter >= nan_patience:\n",
    "                    print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                    stopped_early = True\n",
    "                    break\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            # Gradient scaling for mixed precision\n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.unscale_(optimizer)  # For gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_train_losses.append(total_loss.item())\n",
    "\n",
    "        if stopped_early:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        # Validation (can be done less frequently to save time)\n",
    "        if epoch % validation_frequency == 0:  \n",
    "            model.eval()\n",
    "            epoch_val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                    batch_X_valid, batch_y_valid = batch_X_valid.to(device), batch_y_valid.to(device)\n",
    "                    valid_output = model(batch_X_valid)\n",
    "                    val_loss = criterion(valid_output, batch_y_valid)\n",
    "                    epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "            val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.5f}, Val Loss = {val_losses[-1]:.5f}\")\n",
    "\n",
    "            # Early stopping check\n",
    "            if val_losses[-1] < best_val_loss - min_delta:\n",
    "                best_val_loss = val_losses[-1]\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "                break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def model_save(n_trials=1, model_save=True, save_directory=None, plot_loss=True, output_file_path=None):\n",
    "\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    all_results_params = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # seq_len = random.choice(range(3, 6))\n",
    "        # nhead = random.choice(range(15, 21))\n",
    "        # nhid = nhead * random.choice(range(21, 31))\n",
    "        # nlayers = random.choice(range(1, 2))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # # activation_function = random.choice([torch.nn.GELU(), torch.nn.SiLU(), torch.nn.Mish()])\n",
    "        # activation_function = random.choice([torch.nn.Mish()])\n",
    "        # dropout = 0.01 * random.choice(range(9, 10))\n",
    "        # optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW])\n",
    "        # criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        # # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.SmoothL1Loss, torch.nn.KLDivLoss])\n",
    "        # n_epochs = random.choice(range(300, 301))\n",
    "        # batch_size = random.choice(range(256, 257))\n",
    "        # learning_rate = 0.00001 * random.choice(range(10, 11))\n",
    "        # patience = random.choice(range(30, 31))\n",
    "        # min_delta = random.choice([0.0001])\n",
    "        # l1_regularization = 0.0000001 * random.choice(range(1, 2))\n",
    "        # l2_regularization = l1_regularization\n",
    "\n",
    "        seq_len = random.choice(range(3, 6))\n",
    "        nhead = random.choice(range(30, 31))\n",
    "        nhid = nhead * random.choice(range(50, 51))\n",
    "        nlayers = random.choice(range(1, 2))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # activation_function = random.choice([torch.nn.GELU(), torch.nn.SiLU()])\n",
    "        activation_function = random.choice([torch.nn.Tanh()])\n",
    "        # activation_function = random.choice([torch.nn.Mish()])\n",
    "        dropout = 0.01 * random.choice(range(9, 10))\n",
    "        optimizer = random.choice([torch.optim.AdamW])\n",
    "        criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.KLDivLoss, torch.nn.SmoothL1Loss])\n",
    "        n_epochs = random.choice(range(300, 501))\n",
    "        batch_size = random.choice(range(256, 257))\n",
    "        learning_rate = 0.00001 * random.choice(range(3, 4))\n",
    "        patience = random.choice(range(20, 21))\n",
    "        min_delta = 0.00001 * random.choice(range(1, 2))\n",
    "        l1_regularization = 0.00000001 * random.choice(range(4, 5))\n",
    "        l2_regularization = l1_regularization\n",
    "\n",
    "        # Load tensors\n",
    "        X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "        y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "        X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "\n",
    "        # Initialize the model\n",
    "        model = TransformerModel(n_features=X_train.shape[2], nhead=nhead, nhid=nhid, nlayers=nlayers, activation_function=activation_function, dropout=dropout,\n",
    "            l1_regularization=l1_regularization, l2_regularization=l2_regularization)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, criterion=criterion,\n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "\n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nhead\": nhead, \"nhid\": nhid, \"nlayers\": nlayers, \"activation_function\": activation_function,\n",
    "                \"optimizer\": optimizer, \"criterion\": criterion, \"dropout\": dropout, \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"patience\": patience, \"min_delta\": min_delta,\n",
    "                \"l1_regularization\": l1_regularization, \"l2_regularization\": l2_regularization,\n",
    "                }\n",
    "\n",
    "        results_params = {**params, \"trial\": trial+1, \"train loss\": train_losses[-1], \"valid loss\": val_losses[-1]}\n",
    "\n",
    "        all_results_params.append(results_params)\n",
    "        all_results_params_df = pd.DataFrame(all_results_params)\n",
    "\n",
    "        if save_directory:\n",
    "            all_results_params_df.to_csv(os.path.join(save_directory, f\"all_results_params_{trial}.csv\"))\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")\n",
    "\n",
    "    return all_results_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 1\n",
    "# validation_frequency = 2\n",
    "# batch_size_data = 100000\n",
    "\n",
    "# save_directory = \"/home/predict_price/stock_price/save_model/daily/ver5.8/ver5.8_decoder_split_separate+US+ETF_start_2001\"\n",
    "save_directory = \"/home/predict_price/stock_price/save_model/daily/ver5.8/ver5.8_decoder_no_split\"\n",
    "# output_file_path_params = \"/home/predict_price/stock_price/save_model/ver5.8/ver5.8_decoder_no_split/ver5.8_decoder_no_split\"\n",
    "output_file_path = \"/home/predict_price/stock_price/data_save/daily/ver5.8/ver5.8_decoder_no_split\"\n",
    "all_results_params_df = model_save(n_trials=n_trials, model_save=True, save_directory=save_directory, plot_loss=True, output_file_path=output_file_path)\n",
    "# Save results_df\n",
    "all_results_params_df.to_csv(f'{save_directory}_all_results_params_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
