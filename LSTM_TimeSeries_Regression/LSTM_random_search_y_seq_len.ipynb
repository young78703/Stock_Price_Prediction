{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_seq_len + merged_future_prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.modules.transformer import TransformerDecoder, TransformerDecoderLayer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, X_seq_len, y_seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data) - X_seq_len - y_seq_len + 1):\n",
    "        X.append(data[i : i + X_seq_len])\n",
    "        y.append(data[i + X_seq_len : i + X_seq_len + y_seq_len])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def calculate_indicators(data, period=10, acceleration_factor=2, rsi_period=14, short_ema_period=12,\n",
    "                         long_ema_period=26, signal_period=9, vol_period=20, bollinger_band_window_size=20):\n",
    "    # Check if 'Close' column exists in the data\n",
    "    if 'Close' in data.columns:\n",
    "        # Calculate daily returns\n",
    "        data['Return'] = data['Close'].pct_change()\n",
    "\n",
    "        # Price Range\n",
    "        data['Price Range'] = data['High'] - data['Low']\n",
    "\n",
    "        # Candle Body Size\n",
    "        data['Body Size'] = data['Open'] - data['Close']\n",
    "\n",
    "        # Upper Shadow Size\n",
    "        data['Upper Shadow'] = data.apply(lambda row: row.High - max(row.Open, row.Close), axis=1)\n",
    "\n",
    "        # Lower Shadow Size\n",
    "        data['Lower Shadow'] = data.apply(lambda row: min(row.Open, row.Close) - row.Low, axis=1)\n",
    "\n",
    "        # # Directional Movement\n",
    "        # data['Directional Movement'] = data.apply(lambda row: 1 if row.Close > row.Open else (-1 if row.Close < 0 else 0), axis=1)\n",
    "\n",
    "        # Calculate Moving Averages\n",
    "        # data['MA_25'] = data['Close'].rolling(window=25).mean()\n",
    "        # data['MA_90'] = data['Close'].rolling(window=90).mean()\n",
    "        # data['MA_200'] = data['Close'].rolling(window=200).mean()\n",
    "\n",
    "        zlema = np.zeros(shape=(len(data),))\n",
    "\n",
    "        # Compute the initial ZLEMA value for each row.\n",
    "        for i in range(period):\n",
    "          zlema[i] = data['Close'].iloc[:i + 1].mean()\n",
    "\n",
    "        # Compute the remaining ZLEMA values for each row.\n",
    "        for i in range(period, len(data)):\n",
    "          zlema[i] = zlema[i - 1] + acceleration_factor * (data['Close'].iloc[i] - zlema[i - 1])\n",
    "\n",
    "        # Convert the ZLEMA to a Pandas Series.\n",
    "        data['ZLEMA'] = zlema\n",
    "\n",
    "        # zlema_signal = np.zeros(shape=(len(zlema),))\n",
    "\n",
    "        # for i in range(len(zlema)):\n",
    "        #   if data['Close'].iloc[i] > data['ZLEMA'].iloc[i]:\n",
    "        #     zlema_signal[i] = 1\n",
    "        #   elif data['Close'].iloc[i] < data['ZLEMA'].iloc[i]:\n",
    "        #     zlema_signal[i] = -1\n",
    "        #   else:\n",
    "        #     zlema_signal[i] = 0\n",
    "\n",
    "        # data['ZLEMA_Signal'] = zlema_signal\n",
    "\n",
    "        # Define tolerance as a percentage of the close price\n",
    "        tolerance = 0.02 # for example, 1%\n",
    "\n",
    "        # # Initialize column with 0s\n",
    "        # data['ZLEMA_Signal'] = 0\n",
    "\n",
    "        # # Buy signal when Close is above (1 + tolerance) * ZLEMA\n",
    "        # data.loc[data['Close'] > data['ZLEMA'] * (1 + tolerance), 'ZLEMA_Signal'] = 1\n",
    "\n",
    "        # # Sell signal when Close is below (1 - tolerance) * ZLEMA\n",
    "        # data.loc[data['Close'] < data['ZLEMA'] * (1 - tolerance), 'ZLEMA_Signal'] = -1\n",
    "\n",
    "        # Calculate the first exponential moving average (EMA1).\n",
    "        ema1 = data['Close'].ewm(span=period, min_periods=period).mean()\n",
    "\n",
    "        # Calculate the second exponential moving average (EMA2).\n",
    "        ema2 = ema1.ewm(span=period, min_periods=period).mean()\n",
    "\n",
    "        # Calculate the third exponential moving average (EMA3).\n",
    "        ema3 = ema2.ewm(span=period, min_periods=period).mean()\n",
    "\n",
    "        # Calculate the TEMA.\n",
    "        tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "\n",
    "        # Convert the TEMA to a Pandas Series.\n",
    "        data['TEMA'] = tema\n",
    "\n",
    "        # tema_signal = np.zeros(shape=(len(tema),))\n",
    "\n",
    "        # for i in range(len(tema)):\n",
    "        #   if data['Close'].iloc[i] > data['TEMA'].iloc[i]:\n",
    "        #     tema_signal[i] = 1\n",
    "        #   elif data['Close'].iloc[i] < data['TEMA'].iloc[i]:\n",
    "        #     tema_signal[i] = -1\n",
    "        #   else:\n",
    "        #     tema_signal[i] = 0\n",
    "\n",
    "        # data['TEMA_Signal'] = tema_signal\n",
    "\n",
    "        # # Initialize column with 0s\n",
    "        # data['TEMA_Signal'] = 0\n",
    "\n",
    "\n",
    "        # # Buy signal when Close is above (1 + tolerance) * TEMA\n",
    "        # data.loc[data ['Close']> data ['TEMA']* (1 + tolerance), 'TEMA_Signal']= 1\n",
    "\n",
    "        # # Sell signal when Close is below (1 - tolerance) * TEMA\n",
    "        # data.loc[data ['Close']< data ['TEMA']*(1-tolerance), 'TEMA_Signal']= -1\n",
    "\n",
    "        # Calculate RSI and its signal\n",
    "        delta = data['Close'].diff()\n",
    "        up, down = delta.copy(), delta.copy()\n",
    "\n",
    "        up[up < 0] = 0\n",
    "        down[down > 0] = 0\n",
    "\n",
    "        average_gain = up.rolling(window=rsi_period).mean()\n",
    "        average_loss = abs(down.rolling(window=rsi_period).mean())\n",
    "\n",
    "        rs = average_gain / average_loss\n",
    "\n",
    "        data['RSI'] =100 - (100 / (1 + rs))\n",
    "        data[\"RSI_Signal\"] =0\n",
    "        data.loc[data.RSI < 30,\"RSI_Signal\"] =1\n",
    "        data.loc[data.RSI > 70,\"RSI_Signal\"] =-1\n",
    "\n",
    "        # Calculate MACD Line: (12-day EMA - 26-day EMA)\n",
    "        EMA_short = data['Close'].ewm(span=short_ema_period).mean()\n",
    "        EMA_long = data['Close'].ewm(span=long_ema_period).mean()\n",
    "        data['MACD_Line'] = EMA_short - EMA_long\n",
    "\n",
    "        # Calculate Signal Line: a n-day MA of MACD Line\n",
    "        data['Signal_Line'] = data[\"MACD_Line\"].ewm(span=signal_period).mean()\n",
    "\n",
    "        # Generate MACD signals based on crossovers\n",
    "        data[\"MACD_Signal\"] = 0 # Neutral\n",
    "        data.loc[(data.MACD_Line > data.Signal_Line) & (data.MACD_Line.shift() < data.Signal_Line.shift()),\"MACD_Signal\"] = 1 # Buy\n",
    "        data.loc[(data.MACD_Line < data.Signal_Line) & (data.MACD_Line.shift() > data.Signal_Line.shift()),\"MACD_Signal\"] = -1 # Sell\n",
    "\n",
    "        #Calculate Exponential Moving Average(EMA) and generate signals based on crossovers\n",
    "        short_EMA =(data.Close.ewm(span=short_ema_period, adjust=False).mean())\n",
    "        long_EMA =(data.Close.ewm(span=long_ema_period, adjust=False).mean())\n",
    "\n",
    "        # Generate EMA signals based on crossovers\n",
    "        data[\"EMA_Short\"]=short_EMA;\n",
    "        data[\"EMA_Long\"]=long_EMA;\n",
    "        # Default to Neutral, then if short EMA is greater than long EMA, signal to Buy. If short EMA is less than long EMA, signal to Sell.\n",
    "\n",
    "        # Define a small percentage as a threshold\n",
    "        ema_diff_threshold = 0.01 # 1% difference\n",
    "\n",
    "        # Calculate absolute percent difference between short and long EMA\n",
    "        data['EMA_Diff'] = abs((data['EMA_Short'] - data['EMA_Long']) / data['EMA_Long'])\n",
    "\n",
    "        # Default all to neutral signal (0)\n",
    "        data[\"EMA_Signal\"] = 0\n",
    "\n",
    "        # If absolute percent difference is greater than threshold, assign buy or sell signal\n",
    "        data.loc[(data['EMA_Short'] > data['EMA_Long']) & (data['EMA_Diff'] > ema_diff_threshold), \"EMA_Signal\"] = 1   # \"Buy\"\n",
    "        data.loc[(data['EMA_Short'] < data['EMA_Long']) & (data['EMA_Diff'] > ema_diff_threshold), \"EMA_Signal\"] = -1  # \"Sell\"\n",
    "\n",
    "\n",
    "        # Calculate Volatility as rolling standard deviation of log returns\n",
    "        data[\"Log_Return\"] =(np.log(data.Close).diff())\n",
    "        data[\"Volatility\"] =(data.Log_Return.rolling(window=vol_period, center=False).std())\n",
    "\n",
    "        # # Generate Volatility signals based on high or low volatility\n",
    "        # high_vol_threshold = 0.025\n",
    "        # low_vol_threshold = 0.015\n",
    "        # data['Volatility_Signal'] = 0\n",
    "        # data.loc[data.Volatility > high_vol_threshold, 'Volatility_Signal'] = 1\n",
    "        # data.loc[data.Volatility < low_vol_threshold, 'Volatility_Signal'] = -1\n",
    "\n",
    "        #Calculate Bollinger Bands\n",
    "        middle_band =data.Close.rolling(window=bollinger_band_window_size, center=False).mean()\n",
    "        std_dev =data.Close.rolling(window=bollinger_band_window_size, center=False).std()\n",
    "        data[\"Upper_Band\"] =(middle_band + std_dev*2)\n",
    "        data[\"Lower_Band\"] =(middle_band - std_dev*2)\n",
    "\n",
    "        # Generate Bollinger Band signals based on crossovers\n",
    "        data['BBand_Signal'] = 0\n",
    "        data.loc[(data.Close > data.Upper_Band), \"BBand_Signal\"]=-1\n",
    "        data.loc[(data.Close < data.Lower_Band), \"BBand_Signal\"]=1\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=['Open', 'Low', 'High'])\n",
    "\n",
    "    else:\n",
    "        print(\"'Close' column is not present in the input dataframe.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "\n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "\n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    # return data.drop(['Adj Close', 'Volume'], axis=1)\n",
    "    return data.drop('Adj Close', axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, X_seq_len, y_seq_len, target_col, symbol, start_date, end_date,\n",
    "                          start_date_short=None, end_date_short=None, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        # train_data = train_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        # train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # train_data[train_data.columns] = train_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # train_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "        train_data = train_data.copy()\n",
    "        train_data = calculate_indicators(train_data)\n",
    "\n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        train_data.dropna(inplace=True)\n",
    "        train_data = train_data.drop(target_col_name, axis=1)\n",
    "\n",
    "        train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "\n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "\n",
    "        # valid_data = valid_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        # valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # valid_data[valid_data.columns] = valid_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # valid_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "\n",
    "        valid_data = valid_data.copy()\n",
    "        valid_data = calculate_indicators(valid_data)\n",
    "\n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        valid_data.dropna(inplace=True)\n",
    "        valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "        valid_data[valid_data.columns] = scaler.fit_transform(valid_data)\n",
    "\n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "\n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized = calculate_indicators(test_data_unnormalized)\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "\n",
    "    # test_data_unshifted = test_data.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_unshifted_reshaped = test_data_unshifted.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_unshifted_transformed = scaler.fit_transform(test_data_unshifted_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = test_data_unshifted_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data_unshifted.drop(target_col_name, axis=1, inplace=True)\n",
    "\n",
    "    # test_data = test_data.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data[test_data.columns] = test_data_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "\n",
    "    # # Fetch a fresh copy of the test data\n",
    "    # test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized = calculate_indicators(test_data_unnormalized)\n",
    "\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "\n",
    "    # test_data_unshifted = test_data.copy()\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "    # test_data_unshifted = test_data_unshifted.drop(target_col_name, axis=1)\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = scaler.fit_transform(test_data_unshifted)\n",
    "\n",
    "    # Scale test data\n",
    "    test_data = test_data.copy()\n",
    "    test_data = calculate_indicators(test_data)\n",
    "\n",
    "    test_data[test_data.columns] = scaler.fit_transform(test_data)\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "    test_data.dropna(inplace=True)\n",
    "    test_data = test_data.drop(target_col_name, axis=1)\n",
    "\n",
    "    # Fetch a fresh copy of a short test data\n",
    "    test_data_short = fetch_data(symbol=symbol,start_date=start_date_short,end_date=end_date_short)\n",
    "\n",
    "    # Scale test data\n",
    "    test_data_short_unnormalized = test_data_short.copy()\n",
    "    test_data_short_unnormalized = calculate_indicators(test_data_short_unnormalized)\n",
    "\n",
    "    test_data_short_unnormalized['Target'] = test_data_short_unnormalized[target_col_name]\n",
    "    test_data_short_unnormalized.dropna(inplace=True)\n",
    "    test_data_short_unnormalized = test_data_short_unnormalized.drop(target_col_name, axis=1)\n",
    "\n",
    "    # test_data_short = test_data_short.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_short_reshaped = test_data_short.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_short_transformed = scaler.fit_transform(test_data_short_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data_short[test_data_short.columns] = test_data_short_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data_short.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data_short.drop(target_col_name, axis=1, inplace=True)\n",
    "\n",
    "    # Scale test data\n",
    "    test_data_short = test_data_short.copy()\n",
    "    test_data_short = calculate_indicators(test_data_short)\n",
    "    test_data_short[test_data_short.columns] = scaler.fit_transform(test_data_short)\n",
    "    test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "    test_data_short.dropna(inplace=True)\n",
    "    test_data_short = test_data_short.drop(target_col_name, axis=1)\n",
    "\n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, X_seq_len, y_seq_len\n",
    "\n",
    "def prepare_data_common(train_data, valid_data, test_data, test_data_short, X_seq_len, y_seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, X_seq_len, y_seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, X_seq_len, y_seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, X_seq_len, y_seq_len)\n",
    "    X_test_short, y_test_short = create_sequences(test_data_short, X_seq_len, y_seq_len)\n",
    "\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    X_test_short = torch.Tensor(X_test_short)\n",
    "    y_test_short = torch.Tensor(y_test_short)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short\n",
    "\n",
    "# class DecoderOnlyTransformerModel(nn.Module):\n",
    "#     def __init__(self, n_features, nhead=8, nhid=64, nlayers=6, dropout=0.1,\n",
    "#                  l1_regularization=0, l2_regularization=0, activation_function=torch.nn.ReLU(), y_seq_len=5):\n",
    "#         super(DecoderOnlyTransformerModel, self).__init__()\n",
    "\n",
    "#         self.pos_encoder = nn.Sequential(\n",
    "#             nn.Linear(n_features, nhid),\n",
    "#             activation_function,\n",
    "#             nn.Linear(nhid, nhid),\n",
    "#             activation_function\n",
    "#         )\n",
    "\n",
    "#         decoder_layers = TransformerDecoderLayer(nhid, nhead)\n",
    "#         self.transformer_decoder = TransformerDecoder(decoder_layers,nlayers)\n",
    "\n",
    "#         self.decoder = nn.Linear(nhid,n_features)\n",
    "\n",
    "#         self.l1_regularization = l1_regularization\n",
    "#         self.l2_regularization = l2_regularization\n",
    "#         self.y_seq_len = y_seq_len\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # initrange = 0.1\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[0].weight)\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[2].weight)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "#     def regularization_loss(self):\n",
    "#         l1_loss = 0\n",
    "#         l2_loss = 0\n",
    "#         for param in self.parameters():\n",
    "#             l1_loss += torch.norm(param, 1)\n",
    "#             l2_loss += torch.norm(param, 2) ** 2\n",
    "#         return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pos_encoder(x)\n",
    "#         output = torch.zeros_like(x)\n",
    "\n",
    "#         # Process in chunks\n",
    "#         chunk_size = 100  # Set based on your available memory\n",
    "#         for i in range(0, x.size(0), chunk_size):\n",
    "#             x_chunk = x[i:i+chunk_size]\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(x_chunk.size(0)).to(x.device)\n",
    "#             output_chunk = self.transformer_decoder(x_chunk, x_chunk, tgt_mask=tgt_mask)\n",
    "#             output[i:i+chunk_size] = output_chunk\n",
    "\n",
    "#         output = self.decoder(output)\n",
    "\n",
    "#         return output[:,-self.y_seq_len:,:]\n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2, y_seq_len=5):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.y_seq_len = y_seq_len\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-self.y_seq_len:,:])\n",
    "        # output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "            reg_loss = model.regularization_loss()\n",
    "            total_loss = loss + reg_loss\n",
    "            #print(loss.device)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(total_loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "# def evaluate_model(model, X, y, use_target_col=True):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = model.to(device)\n",
    "\n",
    "#     X = X.to(device)\n",
    "#     y = y.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         y_pred = model(X)\n",
    "\n",
    "#         # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "#         y = y.view(-1, y.shape[-1]).cpu()\n",
    "#         y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "\n",
    "#         if use_target_col:\n",
    "#             y = y[:,-1] # Pick the last column (target column)\n",
    "#             y_pred = y_pred[:,-1]\n",
    "\n",
    "#         mse = mean_squared_error(y, y_pred)\n",
    "#         mae = mean_absolute_error(y, y_pred)\n",
    "#         r2 = r2_score(y, y_pred)\n",
    "\n",
    "#     return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def evaluate_and_plot_predictions(model, X_test, y_test, trial, y_seq_len=1, use_target_col=True, use_target_col_plot=False, metrics_use=True,\n",
    "                                  plots_use=True,save_directory=None, future_predictions=None, scaler=None, col_names=None, test_length=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "\n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "\n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "\n",
    "    # Reshape tensors for metrics computation\n",
    "    y_pred_for_metrics = output.view(-1, output.shape[-1])\n",
    "    y_for_metrics = y_test.view(-1, y_test.shape[-1])\n",
    "\n",
    "    if metrics_use:\n",
    "        \n",
    "        evaluation_results = {}\n",
    "\n",
    "        # Iterate over each column in y_true\n",
    "        for i, col_name in enumerate(col_names):\n",
    "            # Calculate metrics for this column\n",
    "            mse = mean_squared_error(y_for_metrics[:,i], y_pred_for_metrics[:,i])\n",
    "            mae = mean_absolute_error(y_for_metrics[:,i], y_pred_for_metrics[:,i])\n",
    "            r2 = r2_score(y_for_metrics[:,i], y_pred_for_metrics[:,i])\n",
    "\n",
    "            # Store results in dictionary\n",
    "            evaluation_results[col_name] = {\n",
    "                'mse': mse,\n",
    "                'mae': mae,\n",
    "                'r2': r2,\n",
    "            }\n",
    "\n",
    "    if plots_use:\n",
    "        y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "        output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "\n",
    "        # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "        if use_target_col_plot:\n",
    "            last_future_prediction = future_predictions[-y_seq_len:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "\n",
    "            last_future_prediction_expanded = np.expand_dims(last_future_prediction[:, -1], axis=1)\n",
    "            # Transpose last_future_prediction_expanded\n",
    "            last_future_prediction_transposed = np.transpose(last_future_prediction_expanded)\n",
    "            # Now concatenate\n",
    "            combined_predictions = np.concatenate([output_org[:, -y_seq_len:, -1], last_future_prediction_transposed])\n",
    "\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(y_test_org[:, -y_seq_len:, -1])), y_test_org[:, -y_seq_len:, -1], label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)), combined_predictions, label='Predicted + Future Predicted')\n",
    "\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_names[-1]} (Trial {trial+1}_{test_length})')\n",
    "            plt.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial+1}_{test_length}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "        else:\n",
    "            for j in range(n_features):\n",
    "                last_future_prediction = future_predictions[-y_seq_len:]\n",
    "                print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "                # Add an extra dimension to last_future_prediction\n",
    "                last_future_prediction_expanded = np.expand_dims(last_future_prediction[:, j], axis=1)\n",
    "                # Transpose last_future_prediction_expanded\n",
    "                last_future_prediction_transposed = np.transpose(last_future_prediction_expanded)\n",
    "\n",
    "                # Now concatenate\n",
    "                combined_predictions = np.concatenate([output_org[:, -y_seq_len:, j], last_future_prediction_transposed])\n",
    "\n",
    "                fig, ax = plt.subplots(figsize=(15, 8))\n",
    "                ax.plot(np.arange(len(y_test_org[:, -y_seq_len:, j])), y_test_org[:, -y_seq_len:, j], label='Actual')\n",
    "                ax.plot(np.arange(len(combined_predictions)), combined_predictions, label='Predicted + Future Predicted')\n",
    "\n",
    "                ax.set_xlabel('Time Step')\n",
    "                ax.set_ylabel('Value')\n",
    "                # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "                ax.set_title(f'Actual and Predicted Values for {col_names[j]} (Trial {trial+1}_{test_length})')\n",
    "                ax.legend()\n",
    "\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}-{test_length}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "def calculate_metrics_all(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, col_names=None):\n",
    "    # Ensure y_true and y_pred are numpy arrays\n",
    "    assert isinstance(y_true, np.ndarray), \"y_true must be a numpy array\"\n",
    "    assert isinstance(y_pred, np.ndarray), \"y_pred must be a numpy array\"\n",
    "\n",
    "    col_names = col_names.tolist()\n",
    "\n",
    "    # Ensure col_names is a list and has correct length\n",
    "    assert isinstance(col_names, list), \"col_names must be a list\"\n",
    "    assert len(col_names) == y_true.shape[1], \"col_names must have same length as number of columns in y_true\"\n",
    "\n",
    "    # Initialize an empty dictionary to store results\n",
    "    evaluation_results = {}\n",
    "\n",
    "    # Iterate over each column in y_true\n",
    "    for i, col_name in enumerate(col_names):\n",
    "        # Calculate metrics for this column\n",
    "        mse = mean_squared_error(y_true[:, i], y_pred[:, i])\n",
    "        mae = mean_absolute_error(y_true[:, i], y_pred[:, i])\n",
    "        r2 = r2_score(y_true[:, i], y_pred[:, i])\n",
    "\n",
    "        # Store results in dictionary\n",
    "        evaluation_results[col_name] = {\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "        }\n",
    "\n",
    "    return evaluation_results\n",
    "\n",
    "def predict_future(model, X_test, y_test, n_last_sequence=1, scaler=None, y_seq_len=5):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "    #     return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "\n",
    "    def new_sequence(last_sequences, y_test_sequences, sequence_length):\n",
    "        return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test_sequences[:, :, :]], axis=1)\n",
    "\n",
    "    # Prepare the most recent input sequence\n",
    "    x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    y_test_sequences = y_test[-(n_last_sequence):, -1, :]\n",
    "    y_test_sequences = y_test_sequences.reshape(y_test_sequences.shape[0], 1, y_test_sequences.shape[1])\n",
    "\n",
    "    last_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "\n",
    "    merge_future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "      # Generate a prediction\n",
    "        recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features)\n",
    "        with torch.no_grad():\n",
    "          input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "          output = model(input_seq).cpu().numpy()\n",
    "\n",
    "          future_predictions = output[0, :, :]\n",
    "\n",
    "        future_predictions_array = np.array(future_predictions)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if merge_future_predictions is None:\n",
    "            merge_future_predictions = future_predictions_inverse\n",
    "            merge_future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "    return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True,\n",
    "                  overall_future_plot=True, future_predictions=None,\n",
    "                  use_target_col=True, use_target_col_plot=False, metrics_use=True, plots_use=True,\n",
    "                  train_data_list=None, valid_data_list=None, symbol=None, start_date=None, end_date=None,\n",
    "                  start_date_short=None, end_date_short=None, valid_size=0.5, X_seq_len=10, y_seq_len=5, n_last_sequence=1, forward=-1):\n",
    "\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    short_all_future_predictions = []\n",
    "    all_future_predictions = []\n",
    "    all_results_metrics = []\n",
    "    all_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # Generate random hyperparameters and parameters\n",
    "        X_seq_len = random.choice(range(10, 21))\n",
    "        y_seq_len = random.choice(range(3, 4))\n",
    "        nlayers = random.choice(range(1, 4))\n",
    "        nneurons = random.choice(range(200, 301))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(500, 1001))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001])\n",
    "        patience = random.choice(range(5, 6))\n",
    "        min_delta = random.choice([0.0001])\n",
    "        l2_regularization = random.choice([0])\n",
    "        # n_predict = random.choice(range(5, 6))\n",
    "        # n_last_sequence = random.choice(range(100, 101))\n",
    "        # forward = -n_predict\n",
    "\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, X_seq_len=X_seq_len, y_seq_len=y_seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, X_seq_len, y_seq_len = prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date, start_date_short=start_date_short,\n",
    "                                                                                        end_date_short=end_date_short, X_seq_len=X_seq_len, y_seq_len=y_seq_len, target_col=target_col)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short = prepare_data_common(train_data=train_data, valid_data=valid_data, test_data=test_data,\n",
    "                                                                                                                test_data_short=test_data_short, X_seq_len=X_seq_len, y_seq_len=y_seq_len)\n",
    "\n",
    "        input_shape = (X_train.shape[0], X_seq_len, X_train.shape[2])        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout, y_seq_len=y_seq_len)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer,\n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "\n",
    "\n",
    "        # # Inverse transform the y_test to the original scale\n",
    "        # test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)\n",
    "        # test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "\n",
    "        # test_data_short_unnormalized_reshaped = test_data_short_unnormalized.values.reshape(-1, 1)\n",
    "        # test_scaler_short = StandardScaler().fit(test_data_short_unnormalized_reshaped)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized)\n",
    "\n",
    "        test_scaler_short = StandardScaler().fit(test_data_short_unnormalized)\n",
    "\n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_last_sequence > 0:\n",
    "            future_predictions, future_predictions_org = predict_future(loaded_model, X_test, y_test, n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        future_predictions = np.squeeze(future_predictions)\n",
    "        future_predictions_org = np.squeeze(future_predictions_org)\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(y_seq_len):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_last_sequence > 0:\n",
    "            short_future_predictions, short_future_predictions_org = predict_future(loaded_model, X_test_short, y_test_short, n_last_sequence=n_last_sequence, scaler=test_scaler_short)\n",
    "        short_future_predictions = np.squeeze(future_predictions)\n",
    "        short_future_predictions_org = np.squeeze(future_predictions_org)\n",
    "        short_future_predictions_df = pd.DataFrame(short_future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test_short.shape[2])])\n",
    "        short_future_predictions_all_features = short_future_predictions_df.iloc[-(y_seq_len):]\n",
    "        short_future_predictions_target = short_future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        short_future_predictions_target_df = short_future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        short_future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        short_all_future_predictions.append(short_future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        short_all_future_predictions_df = pd.concat(short_all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions_Short (Trial {trial+1}): {short_future_predictions_target_df}\")\n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "\n",
    "\n",
    "            train_metrics = evaluate_and_plot_predictions(loaded_model, X_train, y_train, trial, y_seq_len=y_seq_len, use_target_col=use_target_col,\n",
    "                            use_target_col_plot=use_target_col_plot, metrics_use=True, plots_use=False, save_directory=save_directory,\n",
    "                            scaler=test_scaler, col_names=col_label, test_length=\"long period\",\n",
    "                            future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "            test_metrics = evaluate_and_plot_predictions(loaded_model, X_test, y_test, trial, y_seq_len=y_seq_len, use_target_col=use_target_col,\n",
    "                            use_target_col_plot=use_target_col_plot, metrics_use=True, plots_use=True, save_directory=save_directory,\n",
    "                            scaler=test_scaler, col_names=col_label, test_length=\"long period\",\n",
    "                            future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "            test_metrics_short = evaluate_and_plot_predictions(loaded_model, X_test_short, y_test_short, trial, y_seq_len=y_seq_len, use_target_col=use_target_col,\n",
    "                            use_target_col_plot=use_target_col_plot, metrics_use=True, plots_use=True, save_directory=save_directory,\n",
    "                            scaler=test_scaler_short, col_names=col_label, test_length=\"short period\",\n",
    "                            future_predictions=short_future_predictions if len(short_future_predictions) > 0 else None)\n",
    "\n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"X_seq_len\": X_seq_len, \"y_seq_len\": y_seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_last_sequence\": n_last_sequence, \"forward\": forward}\n",
    "        \n",
    "        results_metrics = {**params, \"trial\": trial+1}\n",
    "\n",
    "        # Add metrics for each feature\n",
    "\n",
    "        for col_name, metrics in train_metrics.items():\n",
    "            results_metrics[f\"{col_name} Train MSE\"] = np.round(metrics['mse'], 5)\n",
    "            results_metrics[f\"{col_name} Train MAE\"] = np.round(metrics['mae'], 5)\n",
    "            results_metrics[f\"{col_name} Train R2\"] = np.round(metrics['r2'], 5)\n",
    "\n",
    "        for col_name, metrics in test_metrics.items():\n",
    "            results_metrics[f\"{col_name} Test MSE\"] = np.round(metrics['mse'], 5)\n",
    "            results_metrics[f\"{col_name} Test MAE\"] = np.round(metrics['mae'], 5)\n",
    "            results_metrics[f\"{col_name} Test R2\"] = np.round(metrics['r2'], 5)\n",
    "\n",
    "        for col_name, metrics in test_metrics_short.items():\n",
    "            results_metrics[f\"{col_name} Test Short MSE\"] = np.round(metrics['mse'], 5)\n",
    "            results_metrics[f\"{col_name} Test Short MAE\"] = np.round(metrics['mae'], 5)\n",
    "            results_metrics[f\"{col_name} Test Short R2\"] = np.round(metrics['r2'], 5)\n",
    "\n",
    "        all_results_metrics.append(results_metrics)\n",
    "\n",
    "        all_results_metrics_df = pd.concat([pd.DataFrame(data=d, index=[0]) for d in all_results_metrics], axis=0)\n",
    "\n",
    "        print(all_results_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_results_metrics_df.to_csv(os.path.join(save_directory, f\"all_results_{trial}.csv\"))\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_org = []\n",
    "        accumulated_y_pred_org = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred = []\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > y_seq_len:\n",
    "\n",
    "                    y_true_org = y_test_org[-(n_last_sequence-i):-((n_last_sequence-i)-y_seq_len), -1]\n",
    "                    y_pred_org = np.array(future_predictions[-(y_seq_len*(n_last_sequence-i)):-(y_seq_len*(n_last_sequence-(i+1)))])\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_true = y_test[-(n_last_sequence-i):-((n_last_sequence-i)-y_seq_len), -1]\n",
    "                    y_pred = np.array(future_predictions_org[-(y_seq_len*(n_last_sequence-i)):-(y_seq_len*(n_last_sequence-(i+1)))])\n",
    "\n",
    "                else:\n",
    "                    y_true_org = y_test_org[-(n_last_sequence-i):, -1]\n",
    "                    y_pred_org = np.array(future_predictions[-(y_seq_len*(n_last_sequence-i)):-((y_seq_len*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(y_seq_len)))])\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_true = y_test[-(n_last_sequence-i):, -1]\n",
    "                    y_pred = np.array(future_predictions_org[-(y_seq_len*(n_last_sequence-i)):-((y_seq_len*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(y_seq_len)))])\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_org.append(y_true_org)\n",
    "                accumulated_y_pred_org.append(y_pred_org)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_org = np.concatenate(accumulated_y_true_org)\n",
    "        accumulated_y_pred_org = np.concatenate(accumulated_y_pred_org)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred = np.concatenate(accumulated_y_pred)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        predict_metrics_org = calculate_metrics(accumulated_y_true_org, accumulated_y_pred_org, col_names=col_label)\n",
    "        predict_metrics = calculate_metrics(accumulated_y_true, accumulated_y_pred, col_names=col_label)\n",
    "\n",
    "        mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics_all(accumulated_y_true_org, accumulated_y_pred_org)\n",
    "        mse_all_features, mae_all_features, r2_all_features = calculate_metrics_all(accumulated_y_true ,accumulated_y_pred)\n",
    "\n",
    "        # error_percentage = (mae/accumulated_y_true.mean())*100\n",
    "        error_percentage_all_features = (mae_all_features/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        future_metrics  ={\n",
    "            \"Trial\": [trial],\n",
    "            # \"Future MSE (org)\": [np.round(mse_org, 5)],\n",
    "            # \"Future MAE (org)\": [np.round(mae_org, 5)],\n",
    "            # \"Future R2 (org)\": [np.round(r2_org, 5)],\n",
    "            \"Future MSE (org all features)\": [np.round(mse_org_all_features , 5)],\n",
    "            \"Future MAE (org all features)\": [np.round(mae_org_all_features, 5)],\n",
    "            \"Future R2 (org all features)\": [np.round(r2_org_all_features , 5)],\n",
    "            # \"Future MSE\": [np.round(mse, 5)],\n",
    "            # \"Future MAE\": [np.round(mae, 5)],\n",
    "            # \"Future R2\": [np.round(r2, 5)],\n",
    "            \"Future MSE (all features)\": [np.round(mse_all_features, 5)],\n",
    "            \"Future MAE (all features)\": [np.round(mae_all_features, 5)],\n",
    "            \"Future R2 (all features)\": [np.round(r2_all_features, 5)],\n",
    "            # \"Future Error Percentage\": [np.round(error_percentage, 3)],\n",
    "            \"Future Error Percentage (all features)\": [np.round(error_percentage_all_features, 3)]\n",
    "        }\n",
    "        # Add metrics for each feature\n",
    "        for col_name, metrics in predict_metrics_org.items():\n",
    "            future_metrics[f\"{col_name} MSE (org)\"] = np.round(metrics['mse'], 5)\n",
    "            future_metrics[f\"{col_name} MAE (org)\"] = np.round(metrics['mae'], 5)\n",
    "            future_metrics[f\"{col_name} R2 (org)\"] = np.round(metrics['r2'], 5)\n",
    "\n",
    "        for col_name, metrics in predict_metrics.items():\n",
    "            future_metrics[f\"{col_name} MSE\"] = np.round(metrics['mse'], 5)\n",
    "            future_metrics[f\"{col_name} MAE\"] = np.round(metrics['mae'], 5)\n",
    "            future_metrics[f\"{col_name} R2\"] = np.round(metrics['r2'], 5)\n",
    "\n",
    "        all_future_metrics.append(future_metrics)\n",
    "\n",
    "        all_future_metrics_df = pd.concat([pd.DataFrame(data=d, index=[0]) for d in all_future_metrics], axis=0)\n",
    "\n",
    "        print(all_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "            if use_target_col:\n",
    "                combined_predictions = np.concatenate((accumulated_y_pred_org[:,-1], future_predictions_target))\n",
    "                plt.figure(figsize=(15,8))\n",
    "                plt.plot(np.arange(len(accumulated_y_true_org[:,-1])),\n",
    "                        accumulated_y_true_org[:,-1], label='Actual')\n",
    "                plt.plot(np.arange(len(combined_predictions)),\n",
    "                        combined_predictions, label='Predicted')\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                plt.title(f'Overall Actual and Predicted Values for {col_label[-1]} (Trial {trial+1})')\n",
    "                plt.legend()\n",
    "\n",
    "                # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                if save_directory:\n",
    "                    save_path=os.path.join(save_directory,\n",
    "                                        f\"overall_predictions_plot_Trial{trial+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "            else:\n",
    "                for j in range(X_test.shape[2]):\n",
    "                    combined_predictions = np.concatenate((accumulated_y_pred_org[:, j], future_predictions_all_features.iloc[:, j]))\n",
    "                    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "                    ax.plot(np.arange(len(accumulated_y_true_org[:, j])),\n",
    "                        accumulated_y_true_org[:, j], label='Actual')\n",
    "                    ax.plot(np.arange(len(combined_predictions)),\n",
    "                        combined_predictions, label='Predicted + Future Predicted')\n",
    "\n",
    "                    ax.set_xlabel('Time Step')\n",
    "                    ax.set_ylabel('Value')\n",
    "                    # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "                    ax.set_title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1})')\n",
    "                    ax.legend()\n",
    "\n",
    "                    if save_directory:\n",
    "                        save_path = os.path.join(save_directory, f\"predictions_plot_var_{j+1}_Trial{trial+1}.png\")\n",
    "                        plt.savefig(save_path)\n",
    "                    plt.show()\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")\n",
    "\n",
    "    return all_results_metrics_df, all_future_predictions_df, all_future_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chip Maker\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-09-26'\n",
    "symbol = 'TSM'\n",
    "TSM = yf.download(symbol, start=start_date, end=end_date)\n",
    "TSM = TSM.drop('Adj Close', axis=1)\n",
    "symbol = 'INTC'\n",
    "INTC = yf.download(symbol, start=start_date, end=end_date)\n",
    "INTC = INTC.drop('Adj Close', axis=1)\n",
    "symbol = 'ASML'\n",
    "ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASML = ASML.drop('Adj Close', axis=1)\n",
    "symbol = 'MU'\n",
    "MU = yf.download(symbol, start=start_date, end=end_date)\n",
    "MU = MU.drop('Adj Close', axis=1)\n",
    "symbol = 'NVDA'\n",
    "NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "NVDA = NVDA.drop('Adj Close', axis=1)\n",
    "symbol = 'AMD'\n",
    "AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMD = AMD.drop('Adj Close', axis=1)\n",
    "symbol = 'QCOM'\n",
    "QCOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "QCOM = QCOM.drop('Adj Close', axis=1)\n",
    "symbol = 'SNPS'\n",
    "SNPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "SNPS = SNPS.drop('Adj Close', axis=1)\n",
    "symbol = 'MRVL'\n",
    "MRVL = yf.download(symbol, start=start_date, end=end_date)\n",
    "MRVL = MRVL.drop('Adj Close', axis=1)\n",
    "symbol = '^IXIC'\n",
    "IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "IXIC = IXIC.drop('Adj Close', axis=1)\n",
    "\n",
    "# information technology\n",
    "\n",
    "symbol = 'AAPL'\n",
    "AAPL = yf.download(symbol, start=start_date, end=end_date)\n",
    "AAPL = AAPL.drop('Adj Close', axis=1)\n",
    "symbol = 'MSFT'\n",
    "MSFT = yf.download(symbol, start=start_date, end=end_date)\n",
    "MSFT = MSFT.drop('Adj Close', axis=1)\n",
    "symbol = 'TSLA'\n",
    "TSLA = yf.download(symbol, start=start_date, end=end_date)\n",
    "TSLA = TSLA.drop('Adj Close', axis=1)\n",
    "symbol = 'GOOGL'\n",
    "GOOGL = yf.download(symbol, start=start_date, end=end_date)\n",
    "GOOGL = GOOGL.drop('Adj Close', axis=1)\n",
    "symbol = 'GOOG'\n",
    "GOOG = yf.download(symbol, start=start_date, end=end_date)\n",
    "GOOG = GOOG.drop('Adj Close', axis=1)\n",
    "symbol = 'AMZN'\n",
    "AMZN = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMZN = AMZN.drop('Adj Close', axis=1)\n",
    "symbol = 'META'\n",
    "META = yf.download(symbol, start=start_date, end=end_date)\n",
    "META = META.drop('Adj Close', axis=1)\n",
    "symbol = 'AMD'\n",
    "AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMD = AMD.drop('Adj Close', axis=1)\n",
    "symbol = 'ASML'\n",
    "ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASML = ASML.drop('Adj Close', axis=1)\n",
    "symbol = 'NVDA'\n",
    "NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "NVDA = NVDA.drop('Adj Close', axis=1)\n",
    "symbol = 'IBM'\n",
    "IBM = yf.download(symbol, start=start_date, end=end_date)\n",
    "IBM = IBM.drop('Adj Close', axis=1)\n",
    "symbol = 'NFLX'\n",
    "NFLX = yf.download(symbol, start=start_date, end=end_date)\n",
    "NFLX = NFLX.drop('Adj Close', axis=1)\n",
    "\n",
    "# Consumer\n",
    "\n",
    "\n",
    "symbol = 'WMT'\n",
    "WMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "WMT = WMT.drop('Adj Close', axis=1)\n",
    "symbol = 'TGT'\n",
    "TGT = yf.download(symbol, start=start_date, end=end_date)\n",
    "TGT = TGT.drop('Adj Close', axis=1)\n",
    "symbol = 'COST'\n",
    "COST = yf.download(symbol, start=start_date, end=end_date)\n",
    "COST = COST.drop('Adj Close', axis=1)\n",
    "symbol = 'HD'\n",
    "HD = yf.download(symbol, start=start_date, end=end_date)\n",
    "HD = HD.drop('Adj Close', axis=1)\n",
    "symbol = 'LOW'\n",
    "LOW = yf.download(symbol, start=start_date, end=end_date)\n",
    "LOW = LOW.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'PG'\n",
    "PG = yf.download(symbol, start=start_date, end=end_date)\n",
    "PG = PG.drop('Adj Close', axis=1)\n",
    "symbol = 'JNJ'\n",
    "JNJ = yf.download(symbol, start=start_date, end=end_date)\n",
    "JNJ = JNJ.drop('Adj Close', axis=1)\n",
    "symbol = 'PFE'\n",
    "PFE = yf.download(symbol, start=start_date, end=end_date)\n",
    "PFE = PFE.drop('Adj Close', axis=1)\n",
    "symbol = 'CVS'\n",
    "CVS = yf.download(symbol, start=start_date, end=end_date)\n",
    "CVS = CVS.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'KO'\n",
    "KO = yf.download(symbol, start=start_date, end=end_date)\n",
    "KO = KO.drop('Adj Close', axis=1)\n",
    "symbol = 'PEP'\n",
    "PEP = yf.download(symbol, start=start_date, end=end_date)\n",
    "PEP = PEP.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'NKE'\n",
    "NKE = yf.download(symbol, start=start_date, end=end_date)\n",
    "NKE = NKE.drop('Adj Close', axis=1)\n",
    "symbol = 'MCD'\n",
    "MCD = yf.download(symbol, start=start_date, end=end_date)\n",
    "MCD = MCD.drop('Adj Close', axis=1)\n",
    "symbol = 'SBUX'\n",
    "SBUX = yf.download(symbol, start=start_date, end=end_date)\n",
    "SBUX = SBUX.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'VZ'\n",
    "VZ = yf.download(symbol, start=start_date, end=end_date)\n",
    "VZ = VZ.drop('Adj Close', axis=1)\n",
    "symbol = 'T'\n",
    "T = yf.download(symbol, start=start_date, end=end_date)\n",
    "T = T.drop('Adj Close', axis=1)\n",
    "symbol = 'FOX'\n",
    "FOX = yf.download(symbol, start=start_date, end=end_date)\n",
    "FOX = FOX.drop('Adj Close', axis=1)\n",
    "symbol = 'WBD'\n",
    "WBD = yf.download(symbol, start=start_date, end=end_date)\n",
    "WBD = WBD.drop('Adj Close', axis=1)\n",
    "symbol = 'DIS'\n",
    "DIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "DIS = DIS.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'UPS'\n",
    "UPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "UPS = UPS.drop('Adj Close', axis=1)\n",
    "symbol = 'FDX'\n",
    "FDX = yf.download(symbol, start=start_date, end=end_date)\n",
    "FDX = FDX.drop('Adj Close', axis=1)\n",
    "symbol = 'DAL'\n",
    "DAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "DAL = DAL.drop('Adj Close', axis=1)\n",
    "symbol = 'AAL'\n",
    "AAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "AAL = AAL.drop('Adj Close', axis=1)\n",
    "symbol = 'XOM'\n",
    "XOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "XOM = XOM.drop('Adj Close', axis=1)\n",
    "symbol = 'CVX'\n",
    "CVX = yf.download(symbol, start=start_date, end=end_date)\n",
    "CVX = CVX.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'BAC'\n",
    "BAC = yf.download(symbol, start=start_date, end=end_date)\n",
    "BAC = BAC.drop('Adj Close', axis=1)\n",
    "symbol = 'JPM'\n",
    "JPM = yf.download(symbol, start=start_date, end=end_date)\n",
    "JPM = JPM.drop('Adj Close', axis=1)\n",
    "symbol = 'MA'\n",
    "MA = yf.download(symbol, start=start_date, end=end_date)\n",
    "MA = MA.drop('Adj Close', axis=1)\n",
    "symbol = 'V'\n",
    "V = yf.download(symbol, start=start_date, end=end_date)\n",
    "V = V.drop('Adj Close', axis=1)\n",
    "symbol = 'SPG'\n",
    "SPG = yf.download(symbol, start=start_date, end=end_date)\n",
    "SPG = SPG.drop('Adj Close', axis=1)\n",
    "symbol = 'VNO'\n",
    "VNO = yf.download(symbol, start=start_date, end=end_date)\n",
    "VNO = VNO.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'MMM'\n",
    "MMM = yf.download(symbol, start=start_date, end=end_date)\n",
    "MMM = MMM.drop('Adj Close', axis=1)\n",
    "symbol = 'GE'\n",
    "GE = yf.download(symbol, start=start_date, end=end_date)\n",
    "GE = GE.drop('Adj Close', axis=1)\n",
    "symbol = 'F'\n",
    "F = yf.download(symbol, start=start_date, end=end_date)\n",
    "F = F.drop('Adj Close', axis=1)\n",
    "symbol = 'GM'\n",
    "GM = yf.download(symbol, start=start_date, end=end_date)\n",
    "GM = GM.drop('Adj Close', axis=1)\n",
    "symbol = 'HON'\n",
    "HON = yf.download(symbol, start=start_date, end=end_date)\n",
    "HON = HON.drop('Adj Close', axis=1)\n",
    "symbol = 'LMT'\n",
    "LMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "LMT = LMT.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# Futures\n",
    "\n",
    "\n",
    "\n",
    "symbol = 'ES=F'\n",
    "ESF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ESF = ESF.drop('Adj Close', axis=1)\n",
    "symbol = 'YM=F'\n",
    "YMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "YMF = YMF.drop('Adj Close', axis=1)\n",
    "symbol = 'NQ=F'\n",
    "NQF = yf.download(symbol, start=start_date, end=end_date)\n",
    "NQF = NQF.drop('Adj Close', axis=1)\n",
    "symbol = 'RTY=F'\n",
    "RTYF = yf.download(symbol, start=start_date, end=end_date)\n",
    "RTYF = RTYF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZB=F'\n",
    "ZBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZBF = ZBF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZN=F'\n",
    "ZNF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZNF = ZNF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZF=F'\n",
    "ZFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZFF = ZFF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZT=F'\n",
    "ZTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZTF = ZTF.drop('Adj Close', axis=1)\n",
    "symbol = 'GC=F'\n",
    "GCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "GCF = GCF.drop('Adj Close', axis=1)\n",
    "symbol = 'HG=F'\n",
    "HGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "HGF = HGF.drop('Adj Close', axis=1)\n",
    "symbol = 'SI=F'\n",
    "SIF = yf.download(symbol, start=start_date, end=end_date)\n",
    "SIF = SIF.drop('Adj Close', axis=1)\n",
    "symbol = 'PL=F'\n",
    "PLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "PLF = PLF.drop('Adj Close', axis=1)\n",
    "CLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "CLF = CLF.drop('Adj Close', axis=1)\n",
    "symbol = 'NG=F'\n",
    "NGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "NGF = NGF.drop('Adj Close', axis=1)\n",
    "symbol = 'BZ=F'\n",
    "BZF = yf.download(symbol, start=start_date, end=end_date)\n",
    "BZF = BZF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZC=F'\n",
    "ZCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZCF = ZCF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZO=F'\n",
    "ZOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZOF = ZOF.drop('Adj Close', axis=1)\n",
    "symbol = 'KE=F'\n",
    "KEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "KEF = KEF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZR=F'\n",
    "ZRF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZRF = ZRF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZM=F'\n",
    "ZMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZMF = ZMF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZL=F'\n",
    "ZLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZLF = ZLF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZS=F'\n",
    "ZSF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZSF = ZSF.drop('Adj Close', axis=1)\n",
    "symbol = 'GF=F'\n",
    "GFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "GFF = GFF.drop('Adj Close', axis=1)\n",
    "symbol = 'HE=F'\n",
    "HEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "HEF = HEF.drop('Adj Close', axis=1)\n",
    "symbol = 'HO=F'\n",
    "HOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "HOF = HOF.drop('Adj Close', axis=1)\n",
    "symbol = 'LE=F'\n",
    "LFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "LFF = LFF.drop('Adj Close', axis=1)\n",
    "symbol = 'CC=F'\n",
    "CCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "CCF = CCF.drop('Adj Close', axis=1)\n",
    "symbol = 'KC=F'\n",
    "KCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "KCF = KCF.drop('Adj Close', axis=1)\n",
    "symbol = 'CT=F'\n",
    "CTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "CTF = CTF.drop('Adj Close', axis=1)\n",
    "symbol = 'OJ=F'\n",
    "OJF = yf.download(symbol, start=start_date, end=end_date)\n",
    "OJF = OJF.drop('Adj Close', axis=1)\n",
    "symbol = 'SB=F'\n",
    "SBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "SBF = SBF.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# EFTs\n",
    "\n",
    "symbol = 'KBA'\n",
    "KBA = yf.download(symbol, start=start_date, end=end_date)\n",
    "KBA = KBA.drop('Adj Close', axis=1)\n",
    "symbol = 'CHIQ'\n",
    "CHIQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIQ = CHIQ.drop('Adj Close', axis=1)\n",
    "symbol = 'CNTX'\n",
    "CNTX = yf.download(symbol, start=start_date, end=end_date)\n",
    "CNTX = CNTX.drop('Adj Close', axis=1)\n",
    "symbol = 'CHIS'\n",
    "CHIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIS = CHIS.drop('Adj Close', axis=1)\n",
    "symbol = 'CNYA'\n",
    "CNYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "CNYA = CNYA.drop('Adj Close', axis=1)\n",
    "symbol = 'ASHX'\n",
    "ASHX = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASHX = ASHX.drop('Adj Close', axis=1)\n",
    "symbol = 'KFYP'\n",
    "KFYP = yf.download(symbol, start=start_date, end=end_date)\n",
    "KFYP = KFYP.drop('Adj Close', axis=1)\n",
    "symbol = 'KGRN'\n",
    "KGRN = yf.download(symbol, start=start_date, end=end_date)\n",
    "KGRN = KGRN.drop('Adj Close', axis=1)\n",
    "symbol = 'THD'\n",
    "THD = yf.download(symbol, start=start_date, end=end_date)\n",
    "THD = THD.drop('Adj Close', axis=1)\n",
    "symbol = 'BBAX'\n",
    "BBAX = yf.download(symbol, start=start_date, end=end_date)\n",
    "BBAX = BBAX.drop('Adj Close', axis=1)\n",
    "symbol = 'FEMS'\n",
    "FEMS = yf.download(symbol, start=start_date, end=end_date)\n",
    "FEMS = FEMS.drop('Adj Close', axis=1)\n",
    "symbol = 'EZA'\n",
    "EZA = yf.download(symbol, start=start_date, end=end_date)\n",
    "EZA = EZA.drop('Adj Close', axis=1)\n",
    "symbol = 'XSD'\n",
    "XSD = yf.download(symbol, start=start_date, end=end_date)\n",
    "XSD = XSD.drop('Adj Close', axis=1)\n",
    "symbol = 'EYLD'\n",
    "EYLD = yf.download(symbol, start=start_date, end=end_date)\n",
    "EYLD = EYLD.drop('Adj Close', axis=1)\n",
    "symbol = 'FNDE'\n",
    "FNDE = yf.download(symbol, start=start_date, end=end_date)\n",
    "FNDE = FNDE.drop('Adj Close', axis=1)\n",
    "symbol = 'SPEM'\n",
    "SPEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "SPEM = SPEM.drop('Adj Close', axis=1)\n",
    "symbol = 'DXJS'\n",
    "DXJS = yf.download(symbol, start=start_date, end=end_date)\n",
    "DXJS = DXJS.drop('Adj Close', axis=1)\n",
    "symbol = 'KURE'\n",
    "KURE = yf.download(symbol, start=start_date, end=end_date)\n",
    "KURE = KURE.drop('Adj Close', axis=1)\n",
    "symbol = 'EWX'\n",
    "EWX = yf.download(symbol, start=start_date, end=end_date)\n",
    "EWX = EWX.drop('Adj Close', axis=1)\n",
    "symbol = 'FLJH'\n",
    "FLJH = yf.download(symbol, start=start_date, end=end_date)\n",
    "FLJH = FLJH.drop('Adj Close', axis=1)\n",
    "symbol = 'CQQQ'\n",
    "CQQQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "CQQQ = CQQQ.drop('Adj Close', axis=1)\n",
    "symbol = 'CHIE'\n",
    "CHIE = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIE = CHIE.drop('Adj Close', axis=1)\n",
    "symbol = 'MFEM'\n",
    "MFEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "MFEM = MFEM.drop('Adj Close', axis=1)\n",
    "symbol = 'DGS'\n",
    "DGS = yf.download(symbol, start=start_date, end=end_date)\n",
    "DGS = DGS.drop('Adj Close', axis=1)\n",
    "symbol = 'HEEM'\n",
    "HEEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "HEEM = HEEM.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# World Composite Index\n",
    "\n",
    "\n",
    "symbol = '^HSI'\n",
    "HSI = yf.download(symbol, start=start_date, end=end_date)\n",
    "HSI = HSI.drop('Adj Close', axis=1)\n",
    "symbol = '000001.SS'\n",
    "SSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "SSE = SSE.drop('Adj Close', axis=1)\n",
    "symbol = '^N225'\n",
    "N225 = yf.download(symbol, start=start_date, end=end_date)\n",
    "N225 = N225.drop('Adj Close', axis=1)\n",
    "symbol = '^KS11'\n",
    "KS11 = yf.download(symbol, start=start_date, end=end_date)\n",
    "KS11 = KS11.drop('Adj Close', axis=1)\n",
    "symbol = '^BSESN'\n",
    "BSESN = yf.download(symbol, start=start_date, end=end_date)\n",
    "BSESN = BSESN.drop('Adj Close', axis=1)\n",
    "symbol = '^MXX'\n",
    "MXX = yf.download(symbol, start=start_date, end=end_date)\n",
    "MXX = MXX.drop('Adj Close', axis=1)\n",
    "symbol = '^TNX'\n",
    "TNX = yf.download(symbol, start=start_date, end=end_date)\n",
    "TNX = TNX.drop('Adj Close', axis=1)\n",
    "symbol = '^VIX'\n",
    "VIX = yf.download(symbol, start=start_date, end=end_date)\n",
    "VIX = VIX.drop('Adj Close', axis=1)\n",
    "symbol = '^BVSP'\n",
    "BVSP = yf.download(symbol, start=start_date, end=end_date)\n",
    "BVSP = BVSP.drop('Adj Close', axis=1)\n",
    "symbol = '^IXIC'\n",
    "IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "IXIC = IXIC.drop('Adj Close', axis=1)\n",
    "symbol = '^GSPTSE'\n",
    "GSPTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "GSPTSE = GSPTSE.drop('Adj Close', axis=1)\n",
    "symbol = '^DJI'\n",
    "DJI = yf.download(symbol, start=start_date, end=end_date)\n",
    "DJI = DJI.drop('Adj Close', axis=1)\n",
    "symbol = '^FCHI'\n",
    "FCHI = yf.download(symbol, start=start_date, end=end_date)\n",
    "FCHI = FCHI.drop('Adj Close', axis=1)\n",
    "symbol = '^GDAXI'\n",
    "GDAXI = yf.download(symbol, start=start_date, end=end_date)\n",
    "GDAXI = GDAXI.drop('Adj Close', axis=1)\n",
    "symbol = '^FTSE'\n",
    "FTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "FTSE = FTSE.drop('Adj Close', axis=1)\n",
    "symbol = '^IBEX'\n",
    "IBEX = yf.download(symbol, start=start_date, end=end_date)\n",
    "IBEX = IBEX.drop('Adj Close', axis=1)\n",
    "symbol = '^N100'\n",
    "N100 = yf.download(symbol, start=start_date, end=end_date)\n",
    "N100 = N100.drop('Adj Close', axis=1)\n",
    "symbol = '^GSPC'\n",
    "GSPC = yf.download(symbol, start=start_date, end=end_date)\n",
    "GSPC = GSPC.drop('Adj Close', axis=1)\n",
    "symbol = '^RUT'\n",
    "RUT = yf.download(symbol, start=start_date, end=end_date)\n",
    "RUT = RUT.drop('Adj Close', axis=1)\n",
    "symbol = '^NYA'\n",
    "NYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "NYA = NYA.drop('Adj Close', axis=1)\n",
    "symbol = '^STI'\n",
    "STI = yf.download(symbol, start=start_date, end=end_date)\n",
    "STI = STI.drop('Adj Close', axis=1)\n",
    "symbol = '^AXJO'\n",
    "AXJO = yf.download(symbol, start=start_date, end=end_date)\n",
    "AXJO = AXJO.drop('Adj Close', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_inverse transformation\n",
    "import csv\n",
    "# train_data_list = [data3, data4, data5, data6, data7, data8, data9, data10\n",
    "#                   , data11, data12, data13, data14, data15, data16, data17, data18, data19, data20]\n",
    "# data21, data22, data23, data24, data25, data26, data27, data28, data29, data30, data31, data32, data33, data34, data35, data36, data37, data38, data39, data40\n",
    "data = None\n",
    "# train_data_list=[HSI,SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                  ESF,YMF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,HGF,SIF,NGF,BZF,ZCF,ZOF,KEF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF,\n",
    "#                  SSE,N225,KS11,BSESN,FCHI,GDAXI,IBEX,DJI,GSPC,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO\n",
    "#                 ]\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,\n",
    "#                  KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                  ESF,YMF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,HGF,SIF,NGF,BZF,ZCF,ZOF,KEF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF,\n",
    "#                  SSE,N225,KS11,BSESN,FCHI,GDAXI,IBEX,DJI,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO\n",
    "#                 ]\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,\n",
    "#                 MSFT,AAPL,GOOGL,GOOG,AMZN,META,AMD,ASML,NVDA,TSM,TSLA,\n",
    "#                 SSE,N225,KS11,BSESN,FCHI,GDAXI,IBEX,DJI,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO\n",
    "#                 ]\n",
    "# train_data_list=[ESF,YMF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,HGF,SIF,NGF,BZF,ZCF,ZOF,KEF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF,\n",
    "#                 ]\n",
    "# train_data_list=[ESF,YMF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,HGF,SIF,NGF,BZF,ZCF,ZOF,KEF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF,\n",
    "#                 ]\n",
    "# train_data_list=[ESF,YMF,NQF,RTYF,ZBF,ZNF,ZFF\n",
    "#                 ]\n",
    "train_data_list = [MSFT,AAPL,GOOGL]\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,\n",
    "#                  FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,GSPC,DJI,RUT,\n",
    "#                  HSI,SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,ESF,YMF,NQF,\n",
    "#                  RTYF,ZBF,ZNF,CLF,GCF,HGF,SIF,CLF,NGF,ZCF,KEF,MSFT,AAPL,GOOGL,GOOG,NVDA,AMZN,META,TSLA,AMD,ASML]\n",
    "\n",
    "# valid_data_list=[SSE,N225,KS11,BSESN,FCHI,GDAXI,IBEX,DJI,GSPC,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO]\n",
    "# valid_data_list=[WMT,TGT,COST,HD,LOW,PG,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,FDX,UPS]\n",
    "\n",
    "# train_data_list=[ESF,YMF,NQF,RTYF,ZBF,ZNF,CLF,GCF,HGF,SIF,CLF,NGF,ZCF,ZFF,ZTF,PLF,PAF,BZF,ZOF,KCF,CTF]\n",
    "# train_data_list=[ESF,YMF,NQF,ZBF,ZNF,GCF,HGF,SIF,CLF,ZCF,NGF,WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,CVS,KO,PEP]\n",
    "\n",
    "valid_data_list = [JNJ]\n",
    "\n",
    "\n",
    "symbol='JNJ'\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2023-10-08'\n",
    "start_date_short = '2023-01-01'\n",
    "end_date_short = '2023-10-08'\n",
    "target_col = 'Close'\n",
    "\n",
    "n_trials = 20\n",
    "n_top_models = 1\n",
    "n_predict = 5\n",
    "n_last_sequence = 100\n",
    "forward= 0\n",
    "\n",
    "\n",
    "save_directory = \"/content/drive/MyDrive/Colab Notebooks/loaded_model/y_seq_len/jnj_1\"\n",
    "# save_directory = \"/content/drive/MyDrive/Colab Notebooks/Fixed_Model/Indicators/merge_seq/gspc/gspc_1\"\n",
    "all_results_metrics_df, all_future_predictions_df, all_future_metrics_df = random_search(\n",
    "    data=data, train_data_list=train_data_list, valid_data_list=valid_data_list, symbol=symbol, start_date=start_date, end_date=end_date,\n",
    "    start_date_short=start_date_short, end_date_short=end_date_short, target_col=target_col, n_trials=n_trials, n_top_models=n_top_models,\n",
    "    model_save=True, save_directory=save_directory, plot_loss=False, predict_plot=True, future_plot=False, overall_future_plot=True,\n",
    "    use_target_col=False, future_predictions=None, n_last_sequence=n_last_sequence, forward=forward)\n",
    "\n",
    "# results_df, all_future_predictions_df, all_future_metrics_df = random_search(\n",
    "#     data=data, train_data_list=train_data_list, valid_data_list=valid_data_list, symbol=symbol, start_date=start_date, end_date=end_date,\n",
    "#     start_date_short=start_date_short, end_date_short=end_date_short, target_col=target_col, n_trials=n_trials, n_top_models=n_top_models,\n",
    "#     model_save=True, save_directory=save_directory, plot_loss=False, predict_plot=True, future_plot=False, future_plot=True,\n",
    "#     use_target_col=False, future_predictions=None, n_predict=n_predict, n_last_sequence=n_last_sequence, forward=forward)\n",
    "\n",
    "\n",
    "output_file_path = \"/content/drive/MyDrive/Colab Notebooks/loaded_model/y_seq_len/jnj_1\"\n",
    "# Save results_df\n",
    "all_results_metrics_df.to_csv(f'{output_file_path}_all_results_metrics.csv', index=True)\n",
    "\n",
    "# #Save top_models\n",
    "# top_models_df = pd.DataFrame(top_models, columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "# top_models_df.to_csv(f'{output_file_path}_top_models.csv', index=True)\n",
    "\n",
    "# Save the future_metrics DataFrame to a CSV file\n",
    "# all_future_metric_finals.to_csv(f'{output_file_path}_all_future_metrics.csv', index=True)\n",
    "# Save future_predictions\n",
    "all_future_predictions_df.to_csv(f'{output_file_path}_all_future_predictions.csv', index=True)\n",
    "# Save the overall_future_metrics DataFrame to a CSV file\n",
    "all_future_metrics_df.to_csv(f'{output_file_path}_all_future_metrics.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
