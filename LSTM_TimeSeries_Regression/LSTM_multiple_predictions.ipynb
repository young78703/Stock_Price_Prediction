{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import random\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_scoren\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "        \n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "    \n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "    \n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    return data.drop(['Adj Close', 'Volume'], axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, seq_len, target_col, symbol, start_date, end_date, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        train_data = train_data.copy()\n",
    "\n",
    "        # Create separate dataframes for prices and volume\n",
    "        train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # Reshape it back to original shape.\n",
    "        train_data[train_data.columns] = train_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # Shift target column by forward steps.\n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # Drop NA values if there are any due to shifting.\n",
    "        train_data.dropna(inplace=True)\n",
    "\n",
    "        # Drop original target column after creating shifted Target.\n",
    "        train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        # train_data = train_data.copy()\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        # train_data.dropna(inplace=True)\n",
    "        # train_data = train_data.drop(target_col_name, axis=1)\n",
    "        \n",
    "        # train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "        \n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "        \n",
    "        valid_data = valid_data.copy()\n",
    "\n",
    "        # Create separate dataframes for prices and volume\n",
    "        valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # Reshape it back to original shape.\n",
    "        valid_data[valid_data.columns] = valid_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # Shift target column by forward steps.\n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # Drop NA values if there are any due to shifting.\n",
    "        valid_data.dropna(inplace=True)\n",
    "\n",
    "        # Drop original target column after creating shifted Target.\n",
    "        valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        # valid_data = valid_data.copy()\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        # valid_data.dropna(inplace=True)\n",
    "        # valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "\n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "            \n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "  \n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data[test_data.columns] = test_data_transformed.reshape(-1, 4)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "   \n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, seq_len\n",
    "    \n",
    "def prepare_data_common(train_data, valid_data, test_data, seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, seq_len)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-1,:])\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, l2_regularization=0.0001, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def evaluate_model(model, X, y, use_target_col=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "        y = y.view(-1, y.shape[-1]).cpu()\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "\n",
    "        if use_target_col:\n",
    "            y = y[:,-1] # Pick the last column (target column)\n",
    "            y_pred = y_pred[:,-1]\n",
    "\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def plot_predictions(model, X_test, y_test, trial, n_predict, use_target_col=True, save_directory=None, \n",
    "                     future_predictions=None, scaler=None, col_label=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "    \n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "        \n",
    "    y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "    output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "    \n",
    "    # If given, transform future predictions back to the original scale\n",
    "    if future_predictions is not None:\n",
    "        gap = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No future predictions found.\")\n",
    "        gap = 0\n",
    "    \n",
    "    # If future_predictions is not None, plot the future predictions\n",
    "    # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "    if use_target_col:\n",
    "        # the existing time steps first\n",
    "        time_steps = list(range(len(y_test_org)))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(time_steps, y_test_org[:, 0, -1], label='Actual')\n",
    "        plt.plot(time_steps, output_org[:, 0, -1], label='Predicted')\n",
    "        \n",
    "        # generate the future time steps\n",
    "        future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "        print('Plotting future predictions...')\n",
    "        print(\"future_time_steps:\", future_time_steps)\n",
    "        last_future_prediction = future_predictions[-n_predict:]\n",
    "        print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "        plt.plot(future_time_steps, last_future_prediction[:, -1], label='Future Predicted')\n",
    "\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "        plt.title(f'Actual and Predicted Values for {col_label[-1]} (Trial {trial+1})')\n",
    "        plt.legend()\n",
    "        if save_directory:\n",
    "            save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial}.png\")\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    else:\n",
    "        for j in range(n_features):\n",
    "            time_steps = list(range(len(y_test_org)))\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            ax.plot(time_steps, y_test_org[:, 0, j], label='Actual')\n",
    "            ax.plot(time_steps, output_org[:, 0, j], label='Predicted')            \n",
    "          \n",
    "            # Generate the future time steps\n",
    "            future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "            print('Plotting future predictions...')\n",
    "            print(\"future_time_steps:\", future_time_steps)\n",
    "            last_future_prediction = future_predictions[-n_predict:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "            plt.plot(future_time_steps, last_future_prediction[:, j], label=f'Future Predicted for {col_label[j]}')\n",
    "\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1})')\n",
    "            ax.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "    \n",
    "    return mse, mae, r2\n",
    "          \n",
    "def predict_future(model, X_test, n_predict, n_last_sequence=1, scaler=None):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "        return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "    # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "    #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "    # Prepare the most recent input sequence\n",
    "    # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "    # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "    merge_future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "        future_predictions = []\n",
    "\n",
    "        for i in range(n_predict):\n",
    "            # Generate a prediction\n",
    "            recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "            with torch.no_grad():\n",
    "                input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "                output = model(input_seq).cpu().numpy() \n",
    "\n",
    "                future_prediction = output[0, 0, :]\n",
    "\n",
    "            # Append the prediction to the future_predictions list\n",
    "            future_predictions.append(future_prediction)         \n",
    "     \n",
    "            # Update the input sequence with the new prediction, if not the last iteration\n",
    "            if i < n_predict - 1:\n",
    "                recent_input_sequence = update_sequence(recent_input_sequence, future_prediction, sequence_length)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        future_predictions_array = np.array(future_predictions)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if merge_future_predictions is None:\n",
    "            merge_future_predictions = future_predictions_inverse\n",
    "            merge_future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "    return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True, \n",
    "                  future_plot=True, overall_future_plot=True, future_predictions=None, \n",
    "                  use_target_col=True, train_data_list=None, valid_data_list=None,\n",
    "                  symbol=None, start_date=None, end_date=None, valid_size=0.5, \n",
    "                  n_predict=5, seq_len=5, n_last_sequence=1, forward=-1):\n",
    "    \n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "    \n",
    "    top_models = []\n",
    "    all_future_predictions = [] # Initialize the list to save all future predictions from each trial\n",
    "    all_future_metrics =[]\n",
    "    all_overall_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        # Generate random hyperparameters and parameters\n",
    "        seq_len = random.choice(range(5, 6))\n",
    "        nlayers = random.choice(range(1, 2))\n",
    "        nneurons = random.choice(range(32, 33))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(300, 1000))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001])\n",
    "        patience = random.choice(range(20, 21))\n",
    "        min_delta = random.choice([0.00005])\n",
    "        l2_regularization = random.choice([0])\n",
    "\n",
    "        # Prepare and preprocess the data\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, seq_len=seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, seq_len= prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date,\n",
    "                                                                                        seq_len=seq_len, target_col=target_col, forward=forward)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_common(train_data=train_data, valid_data=valid_data, test_data=test_data, seq_len=seq_len)\n",
    "        \n",
    "        input_shape = (X_train.shape[0], seq_len, X_train.shape[2])\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, \n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, l2_regularization=l2_regularization)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "                     \n",
    "        # Evaluate the model on both train and test data\n",
    "        train_mse, train_mae, train_r2 = evaluate_model(model, X_train, y_train)\n",
    "        test_mse, test_mae, test_r2 = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_predict\": n_predict, \"n_last_sequence\": n_last_sequence, \"forward\": forward}        \n",
    "       \n",
    "        trial_results = [trial, params, round(train_mse, 5), round(train_mae, 5), round(train_r2, 5), round(test_mse, 5), round(test_mae, 5), round(test_r2, 5)]\n",
    "        results_df.loc[len(results_df)] = trial_results\n",
    "\n",
    "        if save_directory:\n",
    "            results_df.to_csv(os.path.join(save_directory, f\"results_{trial}.csv\"))\n",
    "            \n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "        \n",
    "        # Inverse transform the y_test to the original scale\n",
    "        test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)  \n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_predict > 0:\n",
    "            future_predictions, future_predictions_org = predict_future(loaded_model, X_test, n_predict=n_predict,\n",
    "                                                        n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        # print(f\"Future Predictions (Trial {trial+1}): {future_predictions.shape}\")\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(n_predict*n_predict):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "            plot_predictions(loaded_model, X_test, y_test, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler, col_label=col_label,\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        future_metrics_trial = []\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_all_features = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred_all_features = []\n",
    "        accumulated_y_pred = []\n",
    "        accumulated_y_actual_all_features = []\n",
    "        accumulated_y_actual = []\n",
    "        accumulated_y_predicted_all_features = []\n",
    "        accumulated_y_predicted = []\n",
    "\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > n_predict:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "                # else:\n",
    "                #     y_true_all_features = y_test_org[-(n_last_sequence-i):, -1]\n",
    "                #     y_true = y_true_all_features[:, -1]\n",
    "                #     y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                #     # Inverse transform the y_test to the original scale\n",
    "                #     y_actual_all_features = y_test[-(n_last_sequence-i):, -1]\n",
    "                #     y_actual = y_actual_all_features[:, -1]\n",
    "                #     y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "        # for i in range(n_last_sequence-n_predict):\n",
    "        #     # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "        #     if y_test_unshifted.shape[0] >= n_last_sequence:\n",
    "        #         if n_last_sequence-i > n_predict:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "        #         else:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):, -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):, -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_all_features.append(y_true_all_features)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred_all_features.append(y_pred_all_features)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "                accumulated_y_actual_all_features.append(y_actual_all_features)\n",
    "                accumulated_y_actual.append(y_actual)\n",
    "                accumulated_y_predicted_all_features.append(y_predicted_all_features)\n",
    "                accumulated_y_predicted.append(y_predicted)\n",
    "\n",
    "                # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "                mse_org, mae_org, r2_org = calculate_metrics(y_pred, y_true)\n",
    "                mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics(y_pred_all_features, y_true_all_features)\n",
    "                mse, mae, r2 = calculate_metrics(y_predicted, y_actual)\n",
    "                mse_all_features, mae_all_features, r2_all_features = calculate_metrics(y_predicted_all_features, y_actual_all_features)\n",
    "                # print(f\"y_pred: {y_pred}, y_true: {y_true}\")\n",
    "\n",
    "                residual = y_true - y_pred\n",
    "                error_percentage = (residual/y_true)*100\n",
    "                average_error_percentage = np.mean(error_percentage)\n",
    "        \n",
    "                # Convert arrays to lists for better CSV saving\n",
    "                y_true_list = y_true.tolist()   \n",
    "                y_pred_list = y_pred.tolist()\n",
    "                residual_list = residual.tolist()\n",
    "                error_percentage_list = error_percentage.tolist()\n",
    "                \n",
    "                # Round values for better readability if desired\n",
    "                y_true_list_rounded = [round(value ,4) for value in y_true_list]\n",
    "                y_pred_list_rounded = [round(value ,4) for value in y_pred_list]\n",
    "                residual_list_rounded=[round(value ,4) for value in residual_list]\n",
    "                error_percentage_list_rounded=[round(value ,2) for value in error_percentage_list]\n",
    "                \n",
    "                # Save future MSE and R2, actual values, predicted values, and residuals\n",
    "                future_metrics = {\n",
    "                    \"Trial\": [trial],\n",
    "                    \"Future MSE (org)\": [round(mse_org, 5)],\n",
    "                    \"Future MAE (org)\": [round(mae_org, 5)],\n",
    "                    \"Future R2 (org)\": [round(r2_org, 5)],\n",
    "                    \"Future MSE (org all features)\": [round(mse_org_all_features, 5)],\n",
    "                    \"Future MAE (org all features)\": [round(mae_org_all_features, 5)],\n",
    "                    \"Future R2 (org all features)\": [round(r2_org_all_features, 5)],\n",
    "                    \"Future MSE\": [round(mse, 5)],\n",
    "                    \"Future MAE\": [round(mae, 5)],\n",
    "                    \"Future R2\": [round(r2, 5)],\n",
    "                    \"Future MSE (all features)\": [round(mse_all_features, 5)],\n",
    "                    \"Future MAE (all features)\": [round(mae_all_features, 5)],\n",
    "                    \"Future R2 (all features)\": [round(r2_all_features, 5)],\n",
    "                    \"Actual\": [y_true_list_rounded],\n",
    "                    \"Predicted\": [y_pred_list_rounded],\n",
    "                    \"Residual\": [residual_list_rounded],\n",
    "                    \"Error Percentage\": [error_percentage_list_rounded],\n",
    "                    \"Average Error Percentage\": [round(average_error_percentage, 2)]\n",
    "                }\n",
    "\n",
    "                future_metrics_df = pd.DataFrame(future_metrics)\n",
    "\n",
    "                # Add an index column that represents each iteration\n",
    "                future_metrics_df['Trial'] = trial + 1\n",
    "                future_metrics_df['Index'] = i + 1\n",
    "\n",
    "            future_metrics_trial.append(future_metrics_df)\n",
    "\n",
    "            # Plot the actual and predicted values for the last sequence of true labels vs predicted labels\n",
    "            # Concatenate y_pred and future_predictions_target along rows\n",
    "            if future_plot:\n",
    "                \n",
    "                print(f\"Future MSE: {mse:.5f}, Future MAE: {mae:.5f}, Future R2: {r2:.5f}, Future MSE (all features): {mse_all_features:.5f}, \"\n",
    "                f\"Future MAE (all features): {mae_all_features:.5f}, Future R2 (all features): {r2_all_features:.5f}, Future MSE (org): {mse_org:.5f}, \"\n",
    "                f\"Future R2: {r2:.5f}, Average Error Percentage: {average_error_percentage:.3f}\")\n",
    "\n",
    "                combined_predictions = np.concatenate((y_pred, future_predictions_target))\n",
    "\n",
    "                # Create a new figure\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.plot(y_true, label='Actual')\n",
    "\n",
    "                # Plot combined predictions (past + future)\n",
    "                plt.plot(combined_predictions, label='Predicted')\n",
    "\n",
    "                # Add labels and title\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "                plt.title(f'Actual and Future_Predicted Values for {col_label[-1]} (Trial {trial+1}, Index {i+1})')\n",
    "                plt.legend()\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"future_predictions_plot_target_trial_{trial+1}_prdict_{i+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "        # Concatenate all the results into a single DataFrame after each trial\n",
    "        all_future_metrics_trial_df = pd.concat(future_metrics_trial)\n",
    "\n",
    "        # Reset index of final DataFrame for clarity after each trial and save it separately\n",
    "        all_future_metrics_trial_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        all_future_metrics.append(all_future_metrics_trial_df)\n",
    "\n",
    "        # Concatenate dataframes from all trials into a final dataframe.\n",
    "        all_future_metric_finals=pd.concat(all_future_metrics,axis=0)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_all_features = np.concatenate(accumulated_y_true_all_features)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred_all_features = np.concatenate(accumulated_y_pred_all_features)\n",
    "        accumulated_y_pred =np.concatenate (accumulated_y_pred )\n",
    "        accumulated_y_actual_all_features = np.concatenate(accumulated_y_actual_all_features)\n",
    "        accumulated_y_actual = np.concatenate(accumulated_y_actual)\n",
    "        accumulated_y_predicted_all_features = np.concatenate(accumulated_y_predicted_all_features)\n",
    "        accumulated_y_predicted = np.concatenate(accumulated_y_predicted)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_mse_org, overall_mae_org, overall_r2_org= calculate_metrics(accumulated_y_pred ,accumulated_y_true)\n",
    "        overall_mse_org_all_features, overall_mae_org_all_features, overall_r2_org_all_features = calculate_metrics(accumulated_y_pred_all_features ,accumulated_y_true_all_features)\n",
    "        overall_mse, overall_mae, overall_r2 = calculate_metrics(accumulated_y_predicted ,accumulated_y_actual)\n",
    "        overall_mse_all_features, overall_mae_all_features, overall_r2_all_features = calculate_metrics(accumulated_y_predicted_all_features ,accumulated_y_actual_all_features)\n",
    "        \n",
    "        overall_error_percentage = (overall_mae_org/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        overall_future_metrics  ={\n",
    "            \"Overall Trial\": [trial],\n",
    "            \"Overall Future MSE (org)\": [round(overall_mse_org, 5)],\n",
    "            \"Overall Future MAE (org)\": [round(overall_mae_org, 5)],\n",
    "            \"Overall Future R2 (org)\": [round(overall_r2_org, 5)],\n",
    "            \"Overall Future MSE (org all features)\": [round(overall_mse_org_all_features , 5)],\n",
    "            \"Overall Future MAE (org all features)\": [round(overall_mae_org_all_features, 5)],\n",
    "            \"Overall Future R2 (org all features)\": [round(overall_r2_org_all_features , 5)],\n",
    "            \"Overall Future MSE\": [round(overall_mse, 5)],\n",
    "            \"Overall Future MAE\": [round(overall_mae, 5)],\n",
    "            \"Overall Future R2\": [round(overall_r2, 5)],\n",
    "            \"Overall Future MSE (all features)\": [round(overall_mse_all_features, 5)],\n",
    "            \"Overall Future MAE (all features)\": [round(overall_mae_all_features, 5)],\n",
    "            \"Overall Future R2 (all features)\": [round(overall_r2_all_features, 5)],\n",
    "            \"Overall Future Error Percentage\": [round(overall_error_percentage, 3)]\n",
    "        }\n",
    "        all_overall_future_metrics.append(overall_future_metrics)\n",
    "        # Convert each dict in the list to a DataFrame\n",
    "        df_list = [pd.DataFrame(data=d) for d in all_overall_future_metrics]\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        all_overall_future_metrics_df = pd.concat(df_list, axis=0)\n",
    "        print(all_overall_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_overall_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "\n",
    "            combined_predictions = np.concatenate((accumulated_y_pred, future_predictions_target))\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(accumulated_y_true)),\n",
    "                    accumulated_y_true, label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)),\n",
    "                    combined_predictions, label='Predicted')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Overall Actual and Predicted Values (Trial {trial+1})')\n",
    "            plt.legend()\n",
    "\n",
    "            # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "            if save_directory:\n",
    "                save_path=os.path.join(save_directory,\n",
    "                                    f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "        # Add the resulting model to the \"top models\" list (sorted by Test MSE)\n",
    "        top_models.append((trial, params, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2))\n",
    "        top_models.sort(key=lambda x: x[6])\n",
    "        if len(top_models) > n_top_models:\n",
    "            top_models.pop()\n",
    "            \n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")        \n",
    "          \n",
    "    return results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_short test data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import random\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "        \n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "    \n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "    \n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    return data.drop(['Adj Close', 'Volume'], axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, seq_len, target_col, symbol, start_date, end_date, \n",
    "                          start_date_short=None, end_date_short=None, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        train_data = train_data.copy()\n",
    "\n",
    "        # Create separate dataframes for prices and volume\n",
    "        train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # Reshape it back to original shape.\n",
    "        train_data[train_data.columns] = train_data_transformed.reshape(train_data.shape)\n",
    "\n",
    "        # Shift target column by forward steps.\n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # Drop NA values if there are any due to shifting.\n",
    "        train_data.dropna(inplace=True)\n",
    "\n",
    "        # Drop original target column after creating shifted Target.\n",
    "        train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        # train_data = train_data.copy()\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        # train_data.dropna(inplace=True)\n",
    "        # train_data = train_data.drop(target_col_name, axis=1)\n",
    "        \n",
    "        # train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "        \n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "        \n",
    "        valid_data = valid_data.copy()\n",
    "\n",
    "        # Create separate dataframes for prices and volume\n",
    "        valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # Reshape it back to original shape.\n",
    "        valid_data[valid_data.columns] = valid_data_transformed.reshape(valid_data.shape)\n",
    "\n",
    "        # Shift target column by forward steps.\n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # Drop NA values if there are any due to shifting.\n",
    "        valid_data.dropna(inplace=True)\n",
    "\n",
    "        # Drop original target column after creating shifted Target.\n",
    "        valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        # valid_data = valid_data.copy()\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        # valid_data.dropna(inplace=True)\n",
    "        # valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "\n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "            \n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "  \n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data[test_data.columns] = test_data_transformed.reshape(test_data.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Fetch a fresh copy of a short test data\n",
    "    test_data_short = fetch_data(symbol=symbol,start_date=start_date_short,end_date=end_date_short)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short_unnormalized = test_data_short.copy()\n",
    "    test_data_short_unnormalized['Target'] = test_data_short_unnormalized[target_col_name]\n",
    "    test_data_short_unnormalized.dropna(inplace=True)\n",
    "    test_data_short_unnormalized = test_data_short_unnormalized.drop(target_col_name, axis=1)    \n",
    "\n",
    "    test_data_short = test_data_short.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_short_reshaped = test_data_short.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_short_transformed = scaler.fit_transform(test_data_short_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data_short[test_data_short.columns] = test_data_short_transformed.reshape(test_data_short.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data_short.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data_short.drop(target_col_name, axis=1, inplace=True)    \n",
    "   \n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, seq_len\n",
    "    \n",
    "def prepare_data_common(train_data, valid_data, test_data, test_data_short, seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, seq_len)\n",
    "    X_test_short, y_test_short = create_sequences(test_data_short, seq_len)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    X_test_short = torch.Tensor(X_test_short)\n",
    "    y_test_short = torch.Tensor(y_test_short)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short\n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-1,:])\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, l2_regularization=0.0001, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def evaluate_model(model, X, y, use_target_col=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "        y = y.view(-1, y.shape[-1]).cpu()\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "\n",
    "        if use_target_col:\n",
    "            y = y[:,-1] # Pick the last column (target column)\n",
    "            y_pred = y_pred[:,-1]\n",
    "\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def plot_predictions(model, X_test, y_test, trial, n_predict, use_target_col=True, save_directory=None, \n",
    "                     future_predictions=None, scaler=None, col_label=None, test_length=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "    \n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "        \n",
    "    y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "    output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "    \n",
    "    # If given, transform future predictions back to the original scale\n",
    "    if future_predictions is not None:\n",
    "        gap = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No future predictions found.\")\n",
    "        gap = 0\n",
    "    \n",
    "    # If future_predictions is not None, plot the future predictions\n",
    "    # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "    if use_target_col:\n",
    "        # the existing time steps first\n",
    "        time_steps = list(range(len(y_test_org)))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(time_steps, y_test_org[:, 0, -1], label='Actual')\n",
    "        plt.plot(time_steps, output_org[:, 0, -1], label='Predicted')\n",
    "        \n",
    "        # generate the future time steps\n",
    "        future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "        print('Plotting future predictions...')\n",
    "        print(\"future_time_steps:\", future_time_steps)\n",
    "        last_future_prediction = future_predictions[-n_predict:]\n",
    "        print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "        plt.plot(future_time_steps, last_future_prediction[:, -1], label='Future Predicted')\n",
    "\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "        plt.title(f'Actual and Predicted Values for {col_label[-1]} (Trial {trial+1}_{test_length})')\n",
    "        plt.legend()\n",
    "        if save_directory:\n",
    "            save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial+1}_{test_length}.png\")\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    else:\n",
    "        for j in range(n_features):\n",
    "            time_steps = list(range(len(y_test_org)))\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            ax.plot(time_steps, y_test_org[:, 0, j], label='Actual')\n",
    "            ax.plot(time_steps, output_org[:, 0, j], label='Predicted')            \n",
    "          \n",
    "            # Generate the future time steps\n",
    "            future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "            print('Plotting future predictions...')\n",
    "            print(\"future_time_steps:\", future_time_steps)\n",
    "            last_future_prediction = future_predictions[-n_predict:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "            plt.plot(future_time_steps, last_future_prediction[:, j], label=f'Future Predicted for {col_label[j]}')\n",
    "\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1}_{test_length})')\n",
    "            ax.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}-{test_length}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "    \n",
    "    return mse, mae, r2\n",
    "          \n",
    "def predict_future(model, X_test, n_predict, n_last_sequence=1, scaler=None):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "        return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "    # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "    #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "    # Prepare the most recent input sequence\n",
    "    # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "    # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "    merge_future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "        future_predictions = []\n",
    "\n",
    "        for i in range(n_predict):\n",
    "            # Generate a prediction\n",
    "            recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "            with torch.no_grad():\n",
    "                input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "                output = model(input_seq).cpu().numpy() \n",
    "\n",
    "                future_prediction = output[0, 0, :]\n",
    "\n",
    "            # Append the prediction to the future_predictions list\n",
    "            future_predictions.append(future_prediction)         \n",
    "     \n",
    "            # Update the input sequence with the new prediction, if not the last iteration\n",
    "            if i < n_predict - 1:\n",
    "                recent_input_sequence = update_sequence(recent_input_sequence, future_prediction, sequence_length)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        future_predictions_array = np.array(future_predictions)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if merge_future_predictions is None:\n",
    "            merge_future_predictions = future_predictions_inverse\n",
    "            merge_future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "    return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True, \n",
    "                  future_plot=True, overall_future_plot=True, future_predictions=None, \n",
    "                  use_target_col=True, train_data_list=None, valid_data_list=None,\n",
    "                  symbol=None, start_date=None, end_date=None, start_date_short=None, end_date_short=None,\n",
    "                  valid_size=0.5, n_predict=5, seq_len=5, n_last_sequence=1, forward=-1):\n",
    "    \n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "    \n",
    "    top_models = []\n",
    "    all_future_predictions = [] # Initialize the list to save all future predictions from each trial\n",
    "    all_future_metrics =[]\n",
    "    all_overall_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        # # Generate random hyperparameters and parameters\n",
    "        # seq_len = random.choice(range(5, 6))\n",
    "        # nlayers = random.choice(range(1, 2))\n",
    "        # nneurons = random.choice(range(32, 33))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # dropout = random.choice([0])\n",
    "        # optimizer = random.choice([torch.optim.Adam])\n",
    "        # n_epochs = random.choice(range(300, 1000))\n",
    "        # batch_size = random.choice(range(256, 512))\n",
    "        # learning_rate = random.choice([0.0001])\n",
    "        # patience = random.choice(range(20, 21))\n",
    "        # min_delta = random.choice([0.00005])\n",
    "        # l2_regularization = random.choice([0])\n",
    "        \n",
    "        # Generate random hyperparameters and parameters\n",
    "        seq_len = random.choice(range(5, 31))\n",
    "        nlayers = random.choice(range(1, 6))\n",
    "        nneurons = random.choice(range(16, 128))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0.1, 0.2, 0.3, 0.4])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(300, 1000))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001, 0.0005, 0.001])\n",
    "        patience = random.choice(range(5, 21))\n",
    "        min_delta = random.choice([0.0001, 0.0002])\n",
    "        l2_regularization = random.choice([0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1])\n",
    "\n",
    "        # Prepare and preprocess the data\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, seq_len=seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, seq_len= prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date, start_date_short=start_date_short, \n",
    "                                                                                        end_date_short=end_date_short, seq_len=seq_len, target_col=target_col, forward=forward)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short = prepare_data_common(train_data=train_data, valid_data=valid_data, \n",
    "                                                                                                                 test_data=test_data, test_data_short=test_data_short, seq_len=seq_len)\n",
    "        \n",
    "        input_shape = (X_train.shape[0], seq_len, X_train.shape[2])\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, \n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, l2_regularization=l2_regularization)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "                     \n",
    "        # Evaluate the model on both train and test data\n",
    "        train_mse, train_mae, train_r2 = evaluate_model(model, X_train, y_train)\n",
    "        test_mse, test_mae, test_r2 = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_predict\": n_predict, \"n_last_sequence\": n_last_sequence, \"forward\": forward}        \n",
    "       \n",
    "        trial_results = [trial, params, round(train_mse, 5), round(train_mae, 5), round(train_r2, 5), round(test_mse, 5), round(test_mae, 5), round(test_r2, 5)]\n",
    "        results_df.loc[len(results_df)] = trial_results\n",
    "\n",
    "        if save_directory:\n",
    "            results_df.to_csv(os.path.join(save_directory, f\"results_{trial}.csv\"))\n",
    "            \n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "        \n",
    "        # Inverse transform the y_test to the original scale\n",
    "        test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)  \n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "        \n",
    "        test_data_short_unnormalized_reshaped = test_data_short_unnormalized.values.reshape(-1, 1)\n",
    "        test_scaler_short = StandardScaler().fit(test_data_short_unnormalized_reshaped)\n",
    "        \n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_predict > 0:\n",
    "            future_predictions, future_predictions_org = predict_future(loaded_model, X_test, n_predict=n_predict,\n",
    "                                                        n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        # print(f\"Future Predictions (Trial {trial+1}): {future_predictions.shape}\")\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(n_predict*n_predict):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "            plot_predictions(loaded_model, X_test, y_test, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler, col_label=col_label, test_length=\"long\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "            \n",
    "            plot_predictions(loaded_model, X_test_short, y_test_short, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler_short, col_label=col_label, test_length=\"short\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        future_metrics_trial = []\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_all_features = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred_all_features = []\n",
    "        accumulated_y_pred = []\n",
    "        accumulated_y_actual_all_features = []\n",
    "        accumulated_y_actual = []\n",
    "        accumulated_y_predicted_all_features = []\n",
    "        accumulated_y_predicted = []\n",
    "\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > n_predict:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "                # else:\n",
    "                #     y_true_all_features = y_test_org[-(n_last_sequence-i):, -1]\n",
    "                #     y_true = y_true_all_features[:, -1]\n",
    "                #     y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                #     # Inverse transform the y_test to the original scale\n",
    "                #     y_actual_all_features = y_test[-(n_last_sequence-i):, -1]\n",
    "                #     y_actual = y_actual_all_features[:, -1]\n",
    "                #     y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "        # for i in range(n_last_sequence-n_predict):\n",
    "        #     # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "        #     if y_test_unshifted.shape[0] >= n_last_sequence:\n",
    "        #         if n_last_sequence-i > n_predict:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "        #         else:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):, -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):, -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_all_features.append(y_true_all_features)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred_all_features.append(y_pred_all_features)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "                accumulated_y_actual_all_features.append(y_actual_all_features)\n",
    "                accumulated_y_actual.append(y_actual)\n",
    "                accumulated_y_predicted_all_features.append(y_predicted_all_features)\n",
    "                accumulated_y_predicted.append(y_predicted)\n",
    "\n",
    "                # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "                mse_org, mae_org, r2_org = calculate_metrics(y_pred, y_true)\n",
    "                mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics(y_pred_all_features, y_true_all_features)\n",
    "                mse, mae, r2 = calculate_metrics(y_predicted, y_actual)\n",
    "                mse_all_features, mae_all_features, r2_all_features = calculate_metrics(y_predicted_all_features, y_actual_all_features)\n",
    "                # print(f\"y_pred: {y_pred}, y_true: {y_true}\")\n",
    "\n",
    "                residual = y_true - y_pred\n",
    "                error_percentage = (residual/y_true)*100\n",
    "                average_error_percentage = np.mean(error_percentage)\n",
    "        \n",
    "                # Convert arrays to lists for better CSV saving\n",
    "                y_true_list = y_true.tolist()   \n",
    "                y_pred_list = y_pred.tolist()\n",
    "                residual_list = residual.tolist()\n",
    "                error_percentage_list = error_percentage.tolist()\n",
    "                \n",
    "                # Round values for better readability if desired\n",
    "                y_true_list_rounded = [round(value ,4) for value in y_true_list]\n",
    "                y_pred_list_rounded = [round(value ,4) for value in y_pred_list]\n",
    "                residual_list_rounded=[round(value ,4) for value in residual_list]\n",
    "                error_percentage_list_rounded=[round(value ,2) for value in error_percentage_list]\n",
    "                \n",
    "                # Save future MSE and R2, actual values, predicted values, and residuals\n",
    "                future_metrics = {\n",
    "                    \"Trial\": [trial],\n",
    "                    \"Future MSE (org)\": [round(mse_org, 5)],\n",
    "                    \"Future MAE (org)\": [round(mae_org, 5)],\n",
    "                    \"Future R2 (org)\": [round(r2_org, 5)],\n",
    "                    \"Future MSE (org all features)\": [round(mse_org_all_features, 5)],\n",
    "                    \"Future MAE (org all features)\": [round(mae_org_all_features, 5)],\n",
    "                    \"Future R2 (org all features)\": [round(r2_org_all_features, 5)],\n",
    "                    \"Future MSE\": [round(mse, 5)],\n",
    "                    \"Future MAE\": [round(mae, 5)],\n",
    "                    \"Future R2\": [round(r2, 5)],\n",
    "                    \"Future MSE (all features)\": [round(mse_all_features, 5)],\n",
    "                    \"Future MAE (all features)\": [round(mae_all_features, 5)],\n",
    "                    \"Future R2 (all features)\": [round(r2_all_features, 5)],\n",
    "                    \"Actual\": [y_true_list_rounded],\n",
    "                    \"Predicted\": [y_pred_list_rounded],\n",
    "                    \"Residual\": [residual_list_rounded],\n",
    "                    \"Error Percentage\": [error_percentage_list_rounded],\n",
    "                    \"Average Error Percentage\": [round(average_error_percentage, 2)]\n",
    "                }\n",
    "\n",
    "                future_metrics_df = pd.DataFrame(future_metrics)\n",
    "\n",
    "                # Add an index column that represents each iteration\n",
    "                future_metrics_df['Trial'] = trial + 1\n",
    "                future_metrics_df['Index'] = i + 1\n",
    "\n",
    "            future_metrics_trial.append(future_metrics_df)\n",
    "\n",
    "            # Plot the actual and predicted values for the last sequence of true labels vs predicted labels\n",
    "            # Concatenate y_pred and future_predictions_target along rows\n",
    "            if future_plot:\n",
    "                \n",
    "                print(f\"Future MSE: {mse:.5f}, Future MAE: {mae:.5f}, Future R2: {r2:.5f}, Future MSE (all features): {mse_all_features:.5f}, \"\n",
    "                f\"Future MAE (all features): {mae_all_features:.5f}, Future R2 (all features): {r2_all_features:.5f}, Future MSE (org): {mse_org:.5f}, \"\n",
    "                f\"Future R2: {r2:.5f}, Average Error Percentage: {average_error_percentage:.3f}\")\n",
    "\n",
    "                combined_predictions = np.concatenate((y_pred, future_predictions_target))\n",
    "\n",
    "                # Create a new figure\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.plot(y_true, label='Actual')\n",
    "\n",
    "                # Plot combined predictions (past + future)\n",
    "                plt.plot(combined_predictions, label='Predicted')\n",
    "\n",
    "                # Add labels and title\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "                plt.title(f'Actual and Future_Predicted Values for {col_label[-1]} (Trial {trial+1}, Index {i+1})')\n",
    "                plt.legend()\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"future_predictions_plot_target_trial_{trial+1}_prdict_{i+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "        # Concatenate all the results into a single DataFrame after each trial\n",
    "        all_future_metrics_trial_df = pd.concat(future_metrics_trial)\n",
    "\n",
    "        # Reset index of final DataFrame for clarity after each trial and save it separately\n",
    "        all_future_metrics_trial_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        all_future_metrics.append(all_future_metrics_trial_df)\n",
    "\n",
    "        # Concatenate dataframes from all trials into a final dataframe.\n",
    "        all_future_metric_finals=pd.concat(all_future_metrics,axis=0)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_all_features = np.concatenate(accumulated_y_true_all_features)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred_all_features = np.concatenate(accumulated_y_pred_all_features)\n",
    "        accumulated_y_pred =np.concatenate (accumulated_y_pred )\n",
    "        accumulated_y_actual_all_features = np.concatenate(accumulated_y_actual_all_features)\n",
    "        accumulated_y_actual = np.concatenate(accumulated_y_actual)\n",
    "        accumulated_y_predicted_all_features = np.concatenate(accumulated_y_predicted_all_features)\n",
    "        accumulated_y_predicted = np.concatenate(accumulated_y_predicted)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_mse_org, overall_mae_org, overall_r2_org= calculate_metrics(accumulated_y_pred ,accumulated_y_true)\n",
    "        overall_mse_org_all_features, overall_mae_org_all_features, overall_r2_org_all_features = calculate_metrics(accumulated_y_pred_all_features ,accumulated_y_true_all_features)\n",
    "        overall_mse, overall_mae, overall_r2 = calculate_metrics(accumulated_y_predicted ,accumulated_y_actual)\n",
    "        overall_mse_all_features, overall_mae_all_features, overall_r2_all_features = calculate_metrics(accumulated_y_predicted_all_features ,accumulated_y_actual_all_features)\n",
    "        \n",
    "        overall_error_percentage = (overall_mae_org/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        overall_future_metrics  ={\n",
    "            \"Overall Trial\": [trial],\n",
    "            \"Overall Future MSE (org)\": [round(overall_mse_org, 5)],\n",
    "            \"Overall Future MAE (org)\": [round(overall_mae_org, 5)],\n",
    "            \"Overall Future R2 (org)\": [round(overall_r2_org, 5)],\n",
    "            \"Overall Future MSE (org all features)\": [round(overall_mse_org_all_features , 5)],\n",
    "            \"Overall Future MAE (org all features)\": [round(overall_mae_org_all_features, 5)],\n",
    "            \"Overall Future R2 (org all features)\": [round(overall_r2_org_all_features , 5)],\n",
    "            \"Overall Future MSE\": [round(overall_mse, 5)],\n",
    "            \"Overall Future MAE\": [round(overall_mae, 5)],\n",
    "            \"Overall Future R2\": [round(overall_r2, 5)],\n",
    "            \"Overall Future MSE (all features)\": [round(overall_mse_all_features, 5)],\n",
    "            \"Overall Future MAE (all features)\": [round(overall_mae_all_features, 5)],\n",
    "            \"Overall Future R2 (all features)\": [round(overall_r2_all_features, 5)],\n",
    "            \"Overall Future Error Percentage\": [round(overall_error_percentage, 3)]\n",
    "        }\n",
    "        all_overall_future_metrics.append(overall_future_metrics)\n",
    "        # Convert each dict in the list to a DataFrame\n",
    "        df_list = [pd.DataFrame(data=d) for d in all_overall_future_metrics]\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        all_overall_future_metrics_df = pd.concat(df_list, axis=0)\n",
    "        print(all_overall_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_overall_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "\n",
    "            combined_predictions = np.concatenate((accumulated_y_pred, future_predictions_target))\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(accumulated_y_true)),\n",
    "                    accumulated_y_true, label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)),\n",
    "                    combined_predictions, label='Predicted')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Overall Actual and Predicted Values (Trial {trial+1})')\n",
    "            plt.legend()\n",
    "\n",
    "            # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "            if save_directory:\n",
    "                save_path=os.path.join(save_directory,\n",
    "                                    f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "        # Add the resulting model to the \"top models\" list (sorted by Test MSE)\n",
    "        top_models.append((trial, params, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2))\n",
    "        top_models.sort(key=lambda x: x[6])\n",
    "        if len(top_models) > n_top_models:\n",
    "            top_models.pop()\n",
    "            \n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")        \n",
    "          \n",
    "    return results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_2_short test data_+data_shift\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import random\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "        \n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "    \n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "    \n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    return data.drop(['Adj Close', 'Volume'], axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, seq_len, target_col, symbol, start_date, end_date, \n",
    "                          start_date_short=None, end_date_short=None, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        train_data = train_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        # train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # train_data[train_data.columns] = train_data_transformed.reshape(train_data.shape)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # train_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        train_data = train_data.copy()\n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        train_data.dropna(inplace=True)\n",
    "        train_data = train_data.drop(target_col_name, axis=1)\n",
    "        \n",
    "        train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "        \n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "        \n",
    "        # valid_data = valid_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        # valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # valid_data[valid_data.columns] = valid_data_transformed.reshape(valid_data.shape)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # valid_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        valid_data = valid_data.copy()\n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        valid_data.dropna(inplace=True)\n",
    "        valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "\n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "            \n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "    \n",
    "    test_data_unshifted = test_data.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_unshifted_reshaped = test_data_unshifted.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_unshifted_transformed = scaler.fit_transform(test_data_unshifted_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data_unshifted[test_data_unshifted.columns] = test_data_unshifted_transformed.reshape(test_data_unshifted.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data_unshifted.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data_unshifted.drop(target_col_name, axis=1, inplace=True)\n",
    " \n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data[test_data.columns] = test_data_transformed.reshape(test_data.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    # Fetch a fresh copy of a short test data\n",
    "    test_data_short = fetch_data(symbol=symbol,start_date=start_date_short,end_date=end_date_short)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short_unnormalized = test_data_short.copy()\n",
    "    test_data_short_unnormalized['Target'] = test_data_short_unnormalized[target_col_name]\n",
    "    test_data_short_unnormalized.dropna(inplace=True)\n",
    "    test_data_short_unnormalized = test_data_short_unnormalized.drop(target_col_name, axis=1)    \n",
    "\n",
    "    test_data_short = test_data_short.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_short_reshaped = test_data_short.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_short_transformed = scaler.fit_transform(test_data_short_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data_short[test_data_short.columns] = test_data_short_transformed.reshape(test_data_short.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data_short.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data_short.drop(target_col_name, axis=1, inplace=True)    \n",
    "   \n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, test_data_unshifted, test_data_short, test_data_short_unnormalized, seq_len\n",
    "    \n",
    "def prepare_data_common(train_data, valid_data, test_data, test_data_unshifted, test_data_short, seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, seq_len)\n",
    "    X_test_unshifted, y_test_unshifted = create_sequences(test_data_unshifted, seq_len)\n",
    "    X_test_short, y_test_short = create_sequences(test_data_short, seq_len)\n",
    "    \n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    X_test_unshifted = torch.Tensor(X_test_unshifted)\n",
    "    y_test_unshifted = torch.Tensor(y_test_unshifted)\n",
    "    X_test_short = torch.Tensor(X_test_short)\n",
    "    y_test_short = torch.Tensor(y_test_short)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_unshifted, y_test_unshifted, X_test_short, y_test_short\n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-1,:])\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, l2_regularization=0.0001, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def evaluate_model(model, X, y, use_target_col=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "        y = y.view(-1, y.shape[-1]).cpu()\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "\n",
    "        if use_target_col:\n",
    "            y = y[:,-1] # Pick the last column (target column)\n",
    "            y_pred = y_pred[:,-1]\n",
    "\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def plot_predictions(model, X_test, y_test, trial, n_predict, use_target_col=True, save_directory=None, \n",
    "                     future_predictions=None, scaler=None, col_label=None, test_length=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "    \n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "        \n",
    "    y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "    output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "    \n",
    "    # If given, transform future predictions back to the original scale\n",
    "    if future_predictions is not None:\n",
    "        gap = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No future predictions found.\")\n",
    "        gap = 0\n",
    "    \n",
    "    # If future_predictions is not None, plot the future predictions\n",
    "    # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "    if use_target_col:\n",
    "        # the existing time steps first\n",
    "        time_steps = list(range(len(y_test_org)))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(time_steps, y_test_org[:, 0, -1], label='Actual')\n",
    "        plt.plot(time_steps, output_org[:, 0, -1], label='Predicted')\n",
    "        \n",
    "        # generate the future time steps\n",
    "        future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "        print('Plotting future predictions...')\n",
    "        print(\"future_time_steps:\", future_time_steps)\n",
    "        last_future_prediction = future_predictions[-n_predict:]\n",
    "        print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "        plt.plot(future_time_steps, last_future_prediction[:, -1], label='Future Predicted')\n",
    "\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "        plt.title(f'Actual and Predicted Values for {col_label[-1]} (Trial {trial+1}_{test_length})')\n",
    "        plt.legend()\n",
    "        if save_directory:\n",
    "            save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial+1}_{test_length}.png\")\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    else:\n",
    "        for j in range(n_features):\n",
    "            time_steps = list(range(len(y_test_org)))\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            ax.plot(time_steps, y_test_org[:, 0, j], label='Actual')\n",
    "            ax.plot(time_steps, output_org[:, 0, j], label='Predicted')            \n",
    "          \n",
    "            # Generate the future time steps\n",
    "            future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "            print('Plotting future predictions...')\n",
    "            print(\"future_time_steps:\", future_time_steps)\n",
    "            last_future_prediction = future_predictions[-n_predict:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "            plt.plot(future_time_steps, last_future_prediction[:, j], label=f'Future Predicted for {col_label[j]}')\n",
    "\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1}_{test_length})')\n",
    "            ax.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}-{test_length}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "    \n",
    "    return mse, mae, r2\n",
    "          \n",
    "# def predict_future(model, X_test, y_test_unshifted, n_predict=5, forward=-1, n_last_sequence=1, scaler=None):\n",
    "#     n_features = X_test.shape[2]\n",
    "#     sequence_length = X_test.shape[1]\n",
    "#     torch.cuda.empty_cache()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "#         return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "#     # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "#     #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "#     # Prepare the most recent input sequence\n",
    "#     # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "#     # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "#     # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "#     y_test_unshifted_sequences = y_test_unshifted[-(n_last_sequence+forward):, :, :]\n",
    "    \n",
    "#     last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "#     last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "#     merge_future_predictions = None\n",
    "\n",
    "#     for idx, recent_input_sequence in enumerate(last_sequences):\n",
    "\n",
    "#         future_predictions = []\n",
    "\n",
    "#         for i in range(n_predict):\n",
    "#             # Generate a prediction\n",
    "#             recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "#             with torch.no_grad():\n",
    "#                 input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "#                 output = model(input_seq).cpu().numpy() \n",
    "#                 # Use only the last feature from output and substitute other features with those from recent_input_sequence\n",
    "#                 last_feature_prediction = output[0, 0, -1:]  # Shape should be (1,)\n",
    "                \n",
    "#                 future_prediction = output[0, 0, :]\n",
    "\n",
    "#             # Append the prediction to the future_predictions list\n",
    "#             future_predictions.append(future_prediction)         \n",
    "     \n",
    "#             # Update the input sequence with the new prediction, if not the last iteration\n",
    "#             if idx + i + 1 < len(y_test_unshifted_sequences):\n",
    "                \n",
    "#                 other_features_from_y_test_unshifted = y_test_unshifted_sequences[idx+i+1, -1:, :-1]  # Shape should be (n_features-1,)\n",
    "#                 future_next_prediction = np.concatenate([other_features_from_y_test_unshifted.flatten(), last_feature_prediction])\n",
    "                \n",
    "#                 recent_input_sequence = update_sequence(recent_input_sequence, future_next_prediction, sequence_length)\n",
    "#             else:\n",
    "#                 break\n",
    "        \n",
    "#         future_predictions_array = np.array(future_predictions)\n",
    "#         future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "#         if merge_future_predictions is None:\n",
    "#             merge_future_predictions = future_predictions_inverse\n",
    "#             merge_future_predictions_org = future_predictions_array\n",
    "#         else:\n",
    "#             merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "#             merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "#     return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def predict_future(model, X_test, n_predict, n_last_sequence=1, scaler=None):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "        return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "    # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "    #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "    # Prepare the most recent input sequence\n",
    "    # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "    # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "    merge_future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "        future_predictions = []\n",
    "\n",
    "        for i in range(n_predict):\n",
    "            # Generate a prediction\n",
    "            recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "            with torch.no_grad():\n",
    "                input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "                output = model(input_seq).cpu().numpy() \n",
    "\n",
    "                future_prediction = output[0, 0, :]\n",
    "\n",
    "            # Append the prediction to the future_predictions list\n",
    "            future_predictions.append(future_prediction)         \n",
    "     \n",
    "            # Update the input sequence with the new prediction, if not the last iteration\n",
    "            if i < n_predict - 1:\n",
    "                recent_input_sequence = update_sequence(recent_input_sequence, future_prediction, sequence_length)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        future_predictions_array = np.array(future_predictions)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if merge_future_predictions is None:\n",
    "            merge_future_predictions = future_predictions_inverse\n",
    "            merge_future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "    return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True, \n",
    "                  future_plot=True, overall_future_plot=True, future_predictions=None, \n",
    "                  use_target_col=True, train_data_list=None, valid_data_list=None,\n",
    "                  symbol=None, start_date=None, end_date=None, start_date_short=None, end_date_short=None,\n",
    "                  valid_size=0.5, n_predict=5, seq_len=5, n_last_sequence=1, forward=-1):\n",
    "    \n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "    \n",
    "    top_models = []\n",
    "    all_future_predictions = [] # Initialize the list to save all future predictions from each trial\n",
    "    all_future_metrics =[]\n",
    "    all_overall_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        # Generate random hyperparameters and parameters\n",
    "        seq_len = random.choice(range(5, 6))\n",
    "        nlayers = random.choice(range(1, 2))\n",
    "        nneurons = random.choice(range(32, 33))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(500, 1000))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001])\n",
    "        patience = random.choice(range(50, 51))\n",
    "        min_delta = random.choice([0.00005])\n",
    "        l2_regularization = random.choice([0])\n",
    "        \n",
    "        # # Generate random hyperparameters and parameters\n",
    "        # seq_len = random.choice(range(10, 11))\n",
    "        # nlayers = random.choice(range(1, 2))\n",
    "        # nneurons = random.choice(range(3000, 3001))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # dropout = random.choice([0.1, 0.2, 0.3, 0.4])\n",
    "        # optimizer = random.choice([torch.optim.Adam])\n",
    "        # n_epochs = random.choice(range(200,201))\n",
    "        # batch_size = random.choice(range(256, 257))\n",
    "        # learning_rate = random.choice([0.0001, 0.0005, 0.001])\n",
    "        # patience = random.choice(range(20, 21))\n",
    "        # min_delta = random.choice([0.0001, 0.0002])\n",
    "        # l2_regularization = random.choice([0.01, 0.05, 0.1])\n",
    "\n",
    "        # Prepare and preprocess the data\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, seq_len=seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, test_data_unshifted, test_data_short, test_data_short_unnormalized, seq_len= prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date, start_date_short=start_date_short, \n",
    "                                                                                        end_date_short=end_date_short, seq_len=seq_len, target_col=target_col, forward=forward)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_unshifted, y_test_unshifted, X_test_short, y_test_short = prepare_data_common(train_data=train_data, valid_data=valid_data, test_data=test_data, \n",
    "                                                                                                                                                     test_data_unshifted=test_data_unshifted, test_data_short=test_data_short, seq_len=seq_len)\n",
    "        \n",
    "        input_shape = (X_train.shape[0], seq_len, X_train.shape[2])\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, \n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, l2_regularization=l2_regularization)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "                     \n",
    "        # Evaluate the model on both train and test data\n",
    "        train_mse, train_mae, train_r2 = evaluate_model(model, X_train, y_train)\n",
    "        test_mse, test_mae, test_r2 = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_predict\": n_predict, \"n_last_sequence\": n_last_sequence, \"forward\": forward}        \n",
    "       \n",
    "        trial_results = [trial, params, round(train_mse, 5), round(train_mae, 5), round(train_r2, 5), round(test_mse, 5), round(test_mae, 5), round(test_r2, 5)]\n",
    "        results_df.loc[len(results_df)] = trial_results\n",
    "\n",
    "        if save_directory:\n",
    "            results_df.to_csv(os.path.join(save_directory, f\"results_{trial}.csv\"))\n",
    "            \n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "        \n",
    "        # Inverse transform the y_test to the original scale\n",
    "        test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)  \n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "        \n",
    "        test_data_short_unnormalized_reshaped = test_data_short_unnormalized.values.reshape(-1, 1)\n",
    "        test_scaler_short = StandardScaler().fit(test_data_short_unnormalized_reshaped)\n",
    "        \n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_predict > 0:\n",
    "            # future_predictions, future_predictions_org = predict_future(loaded_model, X_test, y_test_unshifted, n_predict=n_predict,\n",
    "            #                                             forward=forward, n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "                future_predictions, future_predictions_org = predict_future(loaded_model, X_test, n_predict=n_predict,\n",
    "                                                n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        # print(f\"Future Predictions (Trial {trial+1}): {future_predictions.shape}\")\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(n_predict):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "            plot_predictions(loaded_model, X_test, y_test, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler, col_label=col_label, test_length=\"long\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "            \n",
    "            plot_predictions(loaded_model, X_test_short, y_test_short, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler_short, col_label=col_label, test_length=\"short\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        future_metrics_trial = []\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_all_features = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred_all_features = []\n",
    "        accumulated_y_pred = []\n",
    "        accumulated_y_actual_all_features = []\n",
    "        accumulated_y_actual = []\n",
    "        accumulated_y_predicted_all_features = []\n",
    "        accumulated_y_predicted = []\n",
    "\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > n_predict:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "                else:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence-i):, -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence-i):, -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "        # for i in range(n_last_sequence-n_predict):\n",
    "        #     # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "        #     if y_test_unshifted.shape[0] >= n_last_sequence:\n",
    "        #         if n_last_sequence-i > n_predict:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "        #         else:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):, -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):, -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_all_features.append(y_true_all_features)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred_all_features.append(y_pred_all_features)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "                accumulated_y_actual_all_features.append(y_actual_all_features)\n",
    "                accumulated_y_actual.append(y_actual)\n",
    "                accumulated_y_predicted_all_features.append(y_predicted_all_features)\n",
    "                accumulated_y_predicted.append(y_predicted)\n",
    "\n",
    "                # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "                mse_org, mae_org, r2_org = calculate_metrics(y_pred, y_true)\n",
    "                mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics(y_pred_all_features, y_true_all_features)\n",
    "                mse, mae, r2 = calculate_metrics(y_predicted, y_actual)\n",
    "                mse_all_features, mae_all_features, r2_all_features = calculate_metrics(y_predicted_all_features, y_actual_all_features)\n",
    "                # print(f\"y_pred: {y_pred}, y_true: {y_true}\")\n",
    "\n",
    "                residual = y_true - y_pred\n",
    "                error_percentage = (residual/y_true)*100\n",
    "                average_error_percentage = np.mean(error_percentage)\n",
    "        \n",
    "                # Convert arrays to lists for better CSV saving\n",
    "                y_true_list = y_true.tolist()   \n",
    "                y_pred_list = y_pred.tolist()\n",
    "                residual_list = residual.tolist()\n",
    "                error_percentage_list = error_percentage.tolist()\n",
    "                \n",
    "                # Round values for better readability if desired\n",
    "                y_true_list_rounded = [round(value ,4) for value in y_true_list]\n",
    "                y_pred_list_rounded = [round(value ,4) for value in y_pred_list]\n",
    "                residual_list_rounded=[round(value ,4) for value in residual_list]\n",
    "                error_percentage_list_rounded=[round(value ,2) for value in error_percentage_list]\n",
    "                \n",
    "                # Save future MSE and R2, actual values, predicted values, and residuals\n",
    "                future_metrics = {\n",
    "                    \"Trial\": [trial],\n",
    "                    \"Future MSE (org)\": [round(mse_org, 5)],\n",
    "                    \"Future MAE (org)\": [round(mae_org, 5)],\n",
    "                    \"Future R2 (org)\": [round(r2_org, 5)],\n",
    "                    \"Future MSE (org all features)\": [round(mse_org_all_features, 5)],\n",
    "                    \"Future MAE (org all features)\": [round(mae_org_all_features, 5)],\n",
    "                    \"Future R2 (org all features)\": [round(r2_org_all_features, 5)],\n",
    "                    \"Future MSE\": [round(mse, 5)],\n",
    "                    \"Future MAE\": [round(mae, 5)],\n",
    "                    \"Future R2\": [round(r2, 5)],\n",
    "                    \"Future MSE (all features)\": [round(mse_all_features, 5)],\n",
    "                    \"Future MAE (all features)\": [round(mae_all_features, 5)],\n",
    "                    \"Future R2 (all features)\": [round(r2_all_features, 5)],\n",
    "                    \"Actual\": [y_true_list_rounded],\n",
    "                    \"Predicted\": [y_pred_list_rounded],\n",
    "                    \"Residual\": [residual_list_rounded],\n",
    "                    \"Error Percentage\": [error_percentage_list_rounded],\n",
    "                    \"Average Error Percentage\": [round(average_error_percentage, 2)]\n",
    "                }\n",
    "\n",
    "                future_metrics_df = pd.DataFrame(future_metrics)\n",
    "\n",
    "                # Add an index column that represents each iteration\n",
    "                future_metrics_df['Trial'] = trial + 1\n",
    "                future_metrics_df['Index'] = i + 1\n",
    "\n",
    "            future_metrics_trial.append(future_metrics_df)\n",
    "\n",
    "            # Plot the actual and predicted values for the last sequence of true labels vs predicted labels\n",
    "            # Concatenate y_pred and future_predictions_target along rows\n",
    "            if future_plot:\n",
    "                \n",
    "                print(f\"Future MSE: {mse:.5f}, Future MAE: {mae:.5f}, Future R2: {r2:.5f}, Future MSE (all features): {mse_all_features:.5f}, \"\n",
    "                f\"Future MAE (all features): {mae_all_features:.5f}, Future R2 (all features): {r2_all_features:.5f}, Future MSE (org): {mse_org:.5f}, \"\n",
    "                f\"Future R2: {r2:.5f}, Average Error Percentage: {average_error_percentage:.3f}\")\n",
    "\n",
    "                combined_predictions = np.concatenate((y_pred, future_predictions_target))\n",
    "\n",
    "                # Create a new figure\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.plot(y_true, label='Actual')\n",
    "\n",
    "                # Plot combined predictions (past + future)\n",
    "                plt.plot(combined_predictions, label='Predicted')\n",
    "\n",
    "                # Add labels and title\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "                plt.title(f'Actual and Future_Predicted Values for {col_label[-1]} (Trial {trial+1}, Index {i+1})')\n",
    "                plt.legend()\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"future_predictions_plot_target_trial_{trial+1}_prdict_{i+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "        # Concatenate all the results into a single DataFrame after each trial\n",
    "        all_future_metrics_trial_df = pd.concat(future_metrics_trial)\n",
    "\n",
    "        # Reset index of final DataFrame for clarity after each trial and save it separately\n",
    "        all_future_metrics_trial_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        all_future_metrics.append(all_future_metrics_trial_df)\n",
    "\n",
    "        # Concatenate dataframes from all trials into a final dataframe.\n",
    "        all_future_metric_finals=pd.concat(all_future_metrics,axis=0)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_all_features = np.concatenate(accumulated_y_true_all_features)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred_all_features = np.concatenate(accumulated_y_pred_all_features)\n",
    "        accumulated_y_pred =np.concatenate (accumulated_y_pred )\n",
    "        accumulated_y_actual_all_features = np.concatenate(accumulated_y_actual_all_features)\n",
    "        accumulated_y_actual = np.concatenate(accumulated_y_actual)\n",
    "        accumulated_y_predicted_all_features = np.concatenate(accumulated_y_predicted_all_features)\n",
    "        accumulated_y_predicted = np.concatenate(accumulated_y_predicted)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_mse_org, overall_mae_org, overall_r2_org= calculate_metrics(accumulated_y_pred ,accumulated_y_true)\n",
    "        overall_mse_org_all_features, overall_mae_org_all_features, overall_r2_org_all_features = calculate_metrics(accumulated_y_pred_all_features ,accumulated_y_true_all_features)\n",
    "        overall_mse, overall_mae, overall_r2 = calculate_metrics(accumulated_y_predicted ,accumulated_y_actual)\n",
    "        overall_mse_all_features, overall_mae_all_features, overall_r2_all_features = calculate_metrics(accumulated_y_predicted_all_features ,accumulated_y_actual_all_features)\n",
    "        \n",
    "        overall_error_percentage = (overall_mae_org/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        overall_future_metrics  ={\n",
    "            \"Overall Trial\": [trial],\n",
    "            \"Overall Future MSE (org)\": [round(overall_mse_org, 5)],\n",
    "            \"Overall Future MAE (org)\": [round(overall_mae_org, 5)],\n",
    "            \"Overall Future R2 (org)\": [round(overall_r2_org, 5)],\n",
    "            \"Overall Future MSE (org all features)\": [round(overall_mse_org_all_features , 5)],\n",
    "            \"Overall Future MAE (org all features)\": [round(overall_mae_org_all_features, 5)],\n",
    "            \"Overall Future R2 (org all features)\": [round(overall_r2_org_all_features , 5)],\n",
    "            \"Overall Future MSE\": [round(overall_mse, 5)],\n",
    "            \"Overall Future MAE\": [round(overall_mae, 5)],\n",
    "            \"Overall Future R2\": [round(overall_r2, 5)],\n",
    "            \"Overall Future MSE (all features)\": [round(overall_mse_all_features, 5)],\n",
    "            \"Overall Future MAE (all features)\": [round(overall_mae_all_features, 5)],\n",
    "            \"Overall Future R2 (all features)\": [round(overall_r2_all_features, 5)],\n",
    "            \"Overall Future Error Percentage\": [round(overall_error_percentage, 3)]\n",
    "        }\n",
    "        all_overall_future_metrics.append(overall_future_metrics)\n",
    "        # Convert each dict in the list to a DataFrame\n",
    "        df_list = [pd.DataFrame(data=d) for d in all_overall_future_metrics]\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        all_overall_future_metrics_df = pd.concat(df_list, axis=0)\n",
    "        print(all_overall_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_overall_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "\n",
    "            combined_predictions = np.concatenate((accumulated_y_pred, future_predictions_target))\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(accumulated_y_true)),\n",
    "                    accumulated_y_true, label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)),\n",
    "                    combined_predictions, label='Predicted')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Overall Actual and Predicted Values (Trial {trial+1})')\n",
    "            plt.legend()\n",
    "\n",
    "            # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "            if save_directory:\n",
    "                save_path=os.path.join(save_directory,\n",
    "                                    f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "        # Add the resulting model to the \"top models\" list (sorted by Test MSE)\n",
    "        top_models.append((trial, params, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2))\n",
    "        top_models.sort(key=lambda x: x[6])\n",
    "        if len(top_models) > n_top_models:\n",
    "            top_models.pop()\n",
    "            \n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")        \n",
    "          \n",
    "    return results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_2_short test data_+data_shift\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import random\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def set_seeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "        \n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "    \n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "    \n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    return data.drop(['Adj Close', 'Volume', 'Open', 'High', 'Low'], axis=1)\n",
    "    # return data.drop('Adj Close', axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, seq_len, target_col, symbol, start_date, end_date, \n",
    "                          start_date_short=None, end_date_short=None, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        train_data = train_data.copy()\n",
    "\n",
    "        # Create separate dataframes for prices and volume\n",
    "        train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # Reshape it back to original shape.\n",
    "        train_data[train_data.columns] = train_data_transformed.reshape(train_data.shape)\n",
    "\n",
    "        # Shift target column by forward steps.\n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # Drop NA values if there are any due to shifting.\n",
    "        train_data.dropna(inplace=True)\n",
    "\n",
    "        # Drop original target column after creating shifted Target.\n",
    "        train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        # train_data = train_data.copy()\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        # train_data.dropna(inplace=True)\n",
    "        # train_data = train_data.drop(target_col_name, axis=1)\n",
    "        \n",
    "        # train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "        \n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "        \n",
    "        valid_data = valid_data.copy()\n",
    "\n",
    "        # Create separate dataframes for prices and volume\n",
    "        valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # Reshape it back to original shape.\n",
    "        valid_data[valid_data.columns] = valid_data_transformed.reshape(valid_data.shape)\n",
    "\n",
    "        # Shift target column by forward steps.\n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # Drop NA values if there are any due to shifting.\n",
    "        valid_data.dropna(inplace=True)\n",
    "\n",
    "        # Drop original target column after creating shifted Target.\n",
    "        valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        # valid_data = valid_data.copy()\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        # valid_data.dropna(inplace=True)\n",
    "        # valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "\n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "            \n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "    \n",
    "    test_data_unshifted = test_data.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_unshifted_reshaped = test_data_unshifted.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_unshifted_transformed = scaler.fit_transform(test_data_unshifted_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data_unshifted[test_data_unshifted.columns] = test_data_unshifted_transformed.reshape(test_data_unshifted.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data_unshifted.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data_unshifted.drop(target_col_name, axis=1, inplace=True)\n",
    " \n",
    "    test_data = test_data.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data[test_data.columns] = test_data_transformed.reshape(test_data.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # # Fetch a fresh copy of the test data\n",
    "    # test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # # Scale test data\n",
    "    # test_data_unnormalized = test_data.copy()\n",
    "    # test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    # test_data_unnormalized.dropna(inplace=True)\n",
    "    # test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "\n",
    "    # test_data_unshifted = test_data.copy()\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "    # test_data_unshifted = test_data_unshifted.drop(target_col_name, axis=1)\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = scaler.fit_transform(test_data_unshifted)\n",
    "\n",
    "    # # Scale test data\n",
    "    # test_data = test_data.copy()\n",
    "    # test_data[test_data.columns] = scaler.fit_transform(test_data)\n",
    "    # test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "    # test_data.dropna(inplace=True)\n",
    "    # test_data = test_data.drop(target_col_name, axis=1)    \n",
    "    \n",
    "    # Fetch a fresh copy of a short test data\n",
    "    test_data_short = fetch_data(symbol=symbol,start_date=start_date_short,end_date=end_date_short)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short_unnormalized = test_data_short.copy()\n",
    "    test_data_short_unnormalized['Target'] = test_data_short_unnormalized[target_col_name]\n",
    "    test_data_short_unnormalized.dropna(inplace=True)\n",
    "    test_data_short_unnormalized = test_data_short_unnormalized.drop(target_col_name, axis=1)    \n",
    "\n",
    "    test_data_short = test_data_short.copy()\n",
    "\n",
    "    # Create separate dataframes for prices and volume\n",
    "    test_data_short_reshaped = test_data_short.values.reshape(-1, 1)\n",
    "\n",
    "    test_data_short_transformed = scaler.fit_transform(test_data_short_reshaped)\n",
    "\n",
    "    # Reshape it back to original shape.\n",
    "    test_data_short[test_data_short.columns] = test_data_short_transformed.reshape(test_data_short.shape)\n",
    "\n",
    "    # Shift target column by forward steps.\n",
    "    test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "\n",
    "    # Drop NA values if there are any due to shifting.\n",
    "    test_data_short.dropna(inplace=True)\n",
    "\n",
    "    # Drop original target column after creating shifted Target.\n",
    "    test_data_short.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # # Scale test data\n",
    "    # test_data_short = test_data_short.copy()\n",
    "    # test_data_short[test_data_short.columns] = scaler.fit_transform(test_data_short)\n",
    "    # test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "    # test_data_short.dropna(inplace=True)\n",
    "    # test_data_short = test_data_short.drop(target_col_name, axis=1) \n",
    "   \n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, test_data_unshifted, test_data_short, test_data_short_unnormalized, seq_len\n",
    "    \n",
    "def prepare_data_common(train_data, valid_data, test_data, test_data_unshifted, test_data_short, seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, seq_len)\n",
    "    X_test_unshifted, y_test_unshifted = create_sequences(test_data_unshifted, seq_len)\n",
    "    X_test_short, y_test_short = create_sequences(test_data_short, seq_len)\n",
    "    \n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    X_test_unshifted = torch.Tensor(X_test_unshifted)\n",
    "    y_test_unshifted = torch.Tensor(y_test_unshifted)\n",
    "    X_test_short = torch.Tensor(X_test_short)\n",
    "    y_test_short = torch.Tensor(y_test_short)\n",
    "    \n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, X_valid shape: {X_valid.shape}, y_valid shape: {y_valid.shape}, X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_unshifted, y_test_unshifted, X_test_short, y_test_short\n",
    "   \n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-1,:])\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, l2_regularization=0.0001, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def evaluate_model(model, X, y, use_target_col=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "        y = y.view(-1, y.shape[-1]).cpu()\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "        print(f\"y_shape: {y.shape}, y_pred_shape: {y_pred.shape}\\n\")\n",
    "        if use_target_col:\n",
    "            y = y[:,-1] # Pick the last column (target column)\n",
    "            y_pred = y_pred[:,-1]\n",
    "        print(f\"y_shape: {y.shape}, y_pred_shape: {y_pred.shape}\\n\")\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def plot_predictions(model, X_test, y_test, trial, n_predict, use_target_col=True, save_directory=None, \n",
    "                     future_predictions=None, scaler=None, col_label=None, test_length=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "    \n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "        \n",
    "    y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "    output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "    \n",
    "    # If given, transform future predictions back to the original scale\n",
    "    if future_predictions is not None:\n",
    "        gap = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No future predictions found.\")\n",
    "        gap = 0\n",
    "    \n",
    "    # If future_predictions is not None, plot the future predictions\n",
    "    # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "    if use_target_col:\n",
    "        # the existing time steps first\n",
    "        time_steps = list(range(len(y_test_org)))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(time_steps, y_test_org[:, 0, -1], label='Actual')\n",
    "        plt.plot(time_steps, output_org[:, 0, -1], label='Predicted')\n",
    "        \n",
    "        # generate the future time steps\n",
    "        future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "        print('Plotting future predictions...')\n",
    "        print(\"future_time_steps:\", future_time_steps)\n",
    "        last_future_prediction = future_predictions[-n_predict:]\n",
    "        print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "        plt.plot(future_time_steps, last_future_prediction[:, -1], label='Future Predicted')\n",
    "\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "        plt.title(f'Actual and Predicted Values for {col_label[-1]} (Trial {trial+1}_{test_length})')\n",
    "        plt.legend()\n",
    "        if save_directory:\n",
    "            save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial+1}_{test_length}.png\")\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    else:\n",
    "        for j in range(n_features):\n",
    "            time_steps = list(range(len(y_test_org)))\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            ax.plot(time_steps, y_test_org[:, 0, j], label='Actual')\n",
    "            ax.plot(time_steps, output_org[:, 0, j], label='Predicted')            \n",
    "          \n",
    "            # Generate the future time steps\n",
    "            future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "            print('Plotting future predictions...')\n",
    "            print(\"future_time_steps:\", future_time_steps)\n",
    "            last_future_prediction = future_predictions[-n_predict:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "            plt.plot(future_time_steps, last_future_prediction[:, j], label=f'Future Predicted for {col_label[j]}')\n",
    "\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1}_{test_length})')\n",
    "            ax.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}-{test_length}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "    \n",
    "    return mse, mae, r2\n",
    "          \n",
    "def predict_future(model, X_test, n_predict, n_last_sequence=1, scaler=None):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "        return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "    # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "    #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "    # Prepare the most recent input sequence\n",
    "    # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "    # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "    merge_future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "        future_predictions = []\n",
    "\n",
    "        for i in range(n_predict):\n",
    "            # Generate a prediction\n",
    "            recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "            with torch.no_grad():\n",
    "                input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "                output = model(input_seq).cpu().numpy() \n",
    "\n",
    "                future_prediction = output[0, 0, :]\n",
    "\n",
    "            # Append the prediction to the future_predictions list\n",
    "            future_predictions.append(future_prediction)         \n",
    "     \n",
    "            # Update the input sequence with the new prediction, if not the last iteration\n",
    "            if i < n_predict - 1:\n",
    "                recent_input_sequence = update_sequence(recent_input_sequence, future_prediction, sequence_length)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        future_predictions_array = np.array(future_predictions)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if merge_future_predictions is None:\n",
    "            merge_future_predictions = future_predictions_inverse\n",
    "            merge_future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "    return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True, \n",
    "                  future_plot=True, overall_future_plot=True, future_predictions=None, \n",
    "                  use_target_col=True, train_data_list=None, valid_data_list=None,\n",
    "                  symbol=None, start_date=None, end_date=None, start_date_short=None, end_date_short=None,\n",
    "                  valid_size=0.5, n_predict=5, seq_len=5, n_last_sequence=1, forward=-1):\n",
    "    \n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "    \n",
    "    top_models = []\n",
    "    all_future_predictions = [] # Initialize the list to save all future predictions from each trial\n",
    "    all_future_metrics =[]\n",
    "    all_overall_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        # Generate random hyperparameters and parameters\n",
    "        seq_len = random.choice(range(10, 11))\n",
    "        nlayers = random.choice(range(1, 2))\n",
    "        nneurons = random.choice(range(100, 101))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(300, 500))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001, 0.0005])\n",
    "        patience = random.choice(range(50, 51))\n",
    "        min_delta = random.choice([0.0001])\n",
    "        l2_regularization = random.choice([0])\n",
    "        \n",
    "        # # Generate random hyperparameters and parameters\n",
    "        # seq_len = random.choice(range(20, 61))\n",
    "        # nlayers = random.choice(range(1, 6))\n",
    "        # nneurons = random.choice(range(100, 256))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # dropout = random.choice([0.1, 0.2, 0.3, 0.4])\n",
    "        # optimizer = random.choice([torch.optim.Adam])\n",
    "        # n_epochs = random.choice(range(300, 500))\n",
    "        # batch_size = random.choice(range(128, 256))\n",
    "        # learning_rate = random.choice([0.0001, 0.0005, 0.001])\n",
    "        # patience = random.choice(range(5, 21))\n",
    "        # min_delta = random.choice([0.0001, 0.0002])\n",
    "        # l2_regularization = random.choice([0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1])\n",
    "\n",
    "        # Prepare and preprocess the data\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, seq_len=seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, test_data_unshifted, test_data_short, test_data_short_unnormalized, seq_len= prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date, start_date_short=start_date_short, \n",
    "                                                                                        end_date_short=end_date_short, seq_len=seq_len, target_col=target_col, forward=forward)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_unshifted, y_test_unshifted, X_test_short, y_test_short = prepare_data_common(train_data=train_data, valid_data=valid_data, test_data=test_data, \n",
    "                                                                                                                                                     test_data_unshifted=test_data_unshifted, test_data_short=test_data_short, seq_len=seq_len)\n",
    "        \n",
    "        input_shape = (X_train.shape[0], seq_len, X_train.shape[2])\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, \n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, l2_regularization=l2_regularization)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "                     \n",
    "        # Evaluate the model on both train and test data\n",
    "        train_mse, train_mae, train_r2 = evaluate_model(model, X_train, y_train)\n",
    "        test_mse, test_mae, test_r2 = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_predict\": n_predict, \"n_last_sequence\": n_last_sequence, \"forward\": forward}        \n",
    "       \n",
    "        trial_results = [trial, params, round(train_mse, 5), round(train_mae, 5), round(train_r2, 5), round(test_mse, 5), round(test_mae, 5), round(test_r2, 5)]\n",
    "        results_df.loc[len(results_df)] = trial_results\n",
    "\n",
    "        if save_directory:\n",
    "            results_df.to_csv(os.path.join(save_directory, f\"results_{trial}.csv\"))\n",
    "            \n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "        \n",
    "        # Inverse transform the y_test to the original scale\n",
    "        test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)  \n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "        \n",
    "        test_data_short_unnormalized_reshaped = test_data_short_unnormalized.values.reshape(-1, 1)\n",
    "        test_scaler_short = StandardScaler().fit(test_data_short_unnormalized_reshaped)\n",
    "        \n",
    "        # # Inverse transform the y_test to the original scale \n",
    "        # test_scaler = StandardScaler().fit(test_data_unnormalized)\n",
    "        \n",
    "        # test_scaler_short = StandardScaler().fit(test_data_short_unnormalized)\n",
    "        \n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_predict > 0:\n",
    "            future_predictions, future_predictions_org = predict_future(loaded_model, X_test, n_predict=n_predict, \n",
    "                                                        n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        # print(f\"Future Predictions (Trial {trial+1}): {future_predictions.shape}\")\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(n_predict*n_predict):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "            plot_predictions(loaded_model, X_test, y_test, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler, col_label=col_label, test_length=\"long\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "            \n",
    "            plot_predictions(loaded_model, X_test_short, y_test_short, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler_short, col_label=col_label, test_length=\"short\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        future_metrics_trial = []\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_all_features = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred_all_features = []\n",
    "        accumulated_y_pred = []\n",
    "        accumulated_y_actual_all_features = []\n",
    "        accumulated_y_actual = []\n",
    "        accumulated_y_predicted_all_features = []\n",
    "        accumulated_y_predicted = []\n",
    "\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > n_predict:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "                # else:\n",
    "                #     y_true_all_features = y_test_org[-(n_last_sequence+1-i):, -1]\n",
    "                #     y_true = y_true_all_features[:, -1]\n",
    "                #     y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                #     # Inverse transform the y_test to the original scale\n",
    "                #     y_actual_all_features = y_test[-(n_last_sequence+1-i):, -1]\n",
    "                #     y_actual = y_actual_all_features[:, -1]\n",
    "                #     y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "        # for i in range(n_last_sequence-n_predict):\n",
    "        #     # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "        #     if y_test_unshifted.shape[0] >= n_last_sequence:\n",
    "        #         if n_last_sequence-i > n_predict:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "        #         else:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):, -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):, -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_all_features.append(y_true_all_features)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred_all_features.append(y_pred_all_features)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "                accumulated_y_actual_all_features.append(y_actual_all_features)\n",
    "                accumulated_y_actual.append(y_actual)\n",
    "                accumulated_y_predicted_all_features.append(y_predicted_all_features)\n",
    "                accumulated_y_predicted.append(y_predicted)\n",
    "\n",
    "                # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "                mse_org, mae_org, r2_org = calculate_metrics(y_pred, y_true)\n",
    "                mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics(y_pred_all_features, y_true_all_features)\n",
    "                mse, mae, r2 = calculate_metrics(y_predicted, y_actual)\n",
    "                mse_all_features, mae_all_features, r2_all_features = calculate_metrics(y_predicted_all_features, y_actual_all_features)\n",
    "                # print(f\"y_pred: {y_pred}, y_true: {y_true}\")\n",
    "\n",
    "                residual = y_true - y_pred\n",
    "                error_percentage = (residual/y_true)*100\n",
    "                average_error_percentage = np.mean(error_percentage)\n",
    "        \n",
    "                # Convert arrays to lists for better CSV saving\n",
    "                y_true_list = y_true.tolist()   \n",
    "                y_pred_list = y_pred.tolist()\n",
    "                residual_list = residual.tolist()\n",
    "                error_percentage_list = error_percentage.tolist()\n",
    "                \n",
    "                # Round values for better readability if desired\n",
    "                y_true_list_rounded = [round(value ,4) for value in y_true_list]\n",
    "                y_pred_list_rounded = [round(value ,4) for value in y_pred_list]\n",
    "                residual_list_rounded=[round(value ,4) for value in residual_list]\n",
    "                error_percentage_list_rounded=[round(value ,2) for value in error_percentage_list]\n",
    "                \n",
    "                # Save future MSE and R2, actual values, predicted values, and residuals\n",
    "                future_metrics = {\n",
    "                    \"Trial\": [trial],\n",
    "                    \"Future MSE (org)\": [round(mse_org, 5)],\n",
    "                    \"Future MAE (org)\": [round(mae_org, 5)],\n",
    "                    \"Future R2 (org)\": [round(r2_org, 5)],\n",
    "                    \"Future MSE (org all features)\": [round(mse_org_all_features, 5)],\n",
    "                    \"Future MAE (org all features)\": [round(mae_org_all_features, 5)],\n",
    "                    \"Future R2 (org all features)\": [round(r2_org_all_features, 5)],\n",
    "                    \"Future MSE\": [round(mse, 5)],\n",
    "                    \"Future MAE\": [round(mae, 5)],\n",
    "                    \"Future R2\": [round(r2, 5)],\n",
    "                    \"Future MSE (all features)\": [round(mse_all_features, 5)],\n",
    "                    \"Future MAE (all features)\": [round(mae_all_features, 5)],\n",
    "                    \"Future R2 (all features)\": [round(r2_all_features, 5)],\n",
    "                    \"Actual\": [y_true_list_rounded],\n",
    "                    \"Predicted\": [y_pred_list_rounded],\n",
    "                    \"Residual\": [residual_list_rounded],\n",
    "                    \"Error Percentage\": [error_percentage_list_rounded],\n",
    "                    \"Average Error Percentage\": [round(average_error_percentage, 2)]\n",
    "                }\n",
    "\n",
    "                future_metrics_df = pd.DataFrame(future_metrics)\n",
    "\n",
    "                # Add an index column that represents each iteration\n",
    "                future_metrics_df['Trial'] = trial + 1\n",
    "                future_metrics_df['Index'] = i + 1\n",
    "\n",
    "            future_metrics_trial.append(future_metrics_df)\n",
    "\n",
    "            # Plot the actual and predicted values for the last sequence of true labels vs predicted labels\n",
    "            # Concatenate y_pred and future_predictions_target along rows\n",
    "            if future_plot:\n",
    "                \n",
    "                print(f\"Future MSE: {mse:.5f}, Future MAE: {mae:.5f}, Future R2: {r2:.5f}, Future MSE (all features): {mse_all_features:.5f}, \"\n",
    "                f\"Future MAE (all features): {mae_all_features:.5f}, Future R2 (all features): {r2_all_features:.5f}, Future MSE (org): {mse_org:.5f}, \"\n",
    "                f\"Future R2: {r2:.5f}, Average Error Percentage: {average_error_percentage:.3f}\")\n",
    "\n",
    "                combined_predictions = np.concatenate((y_pred, future_predictions_target))\n",
    "\n",
    "                # Create a new figure\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.plot(y_true, label='Actual')\n",
    "\n",
    "                # Plot combined predictions (past + future)\n",
    "                plt.plot(combined_predictions, label='Predicted')\n",
    "\n",
    "                # Add labels and title\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "                plt.title(f'Actual and Future_Predicted Values for {col_label[-1]} (Trial {trial+1}, Index {i+1})')\n",
    "                plt.legend()\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"future_predictions_plot_target_trial_{trial+1}_prdict_{i+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "        # Concatenate all the results into a single DataFrame after each trial\n",
    "        all_future_metrics_trial_df = pd.concat(future_metrics_trial)\n",
    "\n",
    "        # Reset index of final DataFrame for clarity after each trial and save it separately\n",
    "        all_future_metrics_trial_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        all_future_metrics.append(all_future_metrics_trial_df)\n",
    "\n",
    "        # Concatenate dataframes from all trials into a final dataframe.\n",
    "        all_future_metric_finals=pd.concat(all_future_metrics,axis=0)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_all_features = np.concatenate(accumulated_y_true_all_features)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred_all_features = np.concatenate(accumulated_y_pred_all_features)\n",
    "        accumulated_y_pred =np.concatenate (accumulated_y_pred )\n",
    "        accumulated_y_actual_all_features = np.concatenate(accumulated_y_actual_all_features)\n",
    "        accumulated_y_actual = np.concatenate(accumulated_y_actual)\n",
    "        accumulated_y_predicted_all_features = np.concatenate(accumulated_y_predicted_all_features)\n",
    "        accumulated_y_predicted = np.concatenate(accumulated_y_predicted)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_mse_org, overall_mae_org, overall_r2_org= calculate_metrics(accumulated_y_pred ,accumulated_y_true)\n",
    "        overall_mse_org_all_features, overall_mae_org_all_features, overall_r2_org_all_features = calculate_metrics(accumulated_y_pred_all_features ,accumulated_y_true_all_features)\n",
    "        overall_mse, overall_mae, overall_r2 = calculate_metrics(accumulated_y_predicted ,accumulated_y_actual)\n",
    "        overall_mse_all_features, overall_mae_all_features, overall_r2_all_features = calculate_metrics(accumulated_y_predicted_all_features ,accumulated_y_actual_all_features)\n",
    "        \n",
    "        overall_error_percentage = (overall_mae_org/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        overall_future_metrics  ={\n",
    "            \"Overall Trial\": [trial],\n",
    "            \"Overall Future MSE (org)\": [round(overall_mse_org, 5)],\n",
    "            \"Overall Future MAE (org)\": [round(overall_mae_org, 5)],\n",
    "            \"Overall Future R2 (org)\": [round(overall_r2_org, 5)],\n",
    "            \"Overall Future MSE (org all features)\": [round(overall_mse_org_all_features , 5)],\n",
    "            \"Overall Future MAE (org all features)\": [round(overall_mae_org_all_features, 5)],\n",
    "            \"Overall Future R2 (org all features)\": [round(overall_r2_org_all_features , 5)],\n",
    "            \"Overall Future MSE\": [round(overall_mse, 5)],\n",
    "            \"Overall Future MAE\": [round(overall_mae, 5)],\n",
    "            \"Overall Future R2\": [round(overall_r2, 5)],\n",
    "            \"Overall Future MSE (all features)\": [round(overall_mse_all_features, 5)],\n",
    "            \"Overall Future MAE (all features)\": [round(overall_mae_all_features, 5)],\n",
    "            \"Overall Future R2 (all features)\": [round(overall_r2_all_features, 5)],\n",
    "            \"Overall Future Error Percentage\": [round(overall_error_percentage, 3)]\n",
    "        }\n",
    "        all_overall_future_metrics.append(overall_future_metrics)\n",
    "        # Convert each dict in the list to a DataFrame\n",
    "        df_list = [pd.DataFrame(data=d) for d in all_overall_future_metrics]\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        all_overall_future_metrics_df = pd.concat(df_list, axis=0)\n",
    "        print(all_overall_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_overall_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "\n",
    "            combined_predictions = np.concatenate((accumulated_y_pred, future_predictions_target))\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(accumulated_y_true)),\n",
    "                    accumulated_y_true, label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)),\n",
    "                    combined_predictions, label='Predicted')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Overall Actual and Predicted Values (Trial {trial+1})')\n",
    "            plt.legend()\n",
    "\n",
    "            # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "            if save_directory:\n",
    "                save_path=os.path.join(save_directory,\n",
    "                                    f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "        # Add the resulting model to the \"top models\" list (sorted by Test MSE)\n",
    "        top_models.append((trial, params, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2))\n",
    "        top_models.sort(key=lambda x: x[6])\n",
    "        if len(top_models) > n_top_models:\n",
    "            top_models.pop()\n",
    "            \n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")        \n",
    "          \n",
    "    return results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_2_short test data_+data_shift\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import random\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "def create_differences(data):\n",
    "    # Check if the required columns exist in the data\n",
    "    if set(['Open', 'Close', 'High', 'Low']).issubset(data.columns):\n",
    "        # Create new columns\n",
    "        data['Diff_1'] = data['Close'] - data['Open']\n",
    "        data['Diff_2'] = data['High'] - data['Low']\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=['Open', 'Low', 'High'])\n",
    "        \n",
    "    else:\n",
    "        print(\"One or more of the required columns ('Open', 'Close', 'High', 'Low') are not present in the input dataframe.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calculate_indicators(data, rsi_period=14, short_ema_period=12, long_ema_period=26, signal_period=9, vol_period=20):\n",
    "    # Check if 'Close' column exists in the data\n",
    "    if 'Close' in data.columns:\n",
    "            \n",
    "        # Calculate 90 days moving average of 'Close'\n",
    "        data['90D_MA_Close'] = data['Close'].rolling(window=90).mean()\n",
    "\n",
    "        # Calculate 25 days moving average of the newly created '90D_MA_Close'\n",
    "        data['25D_MA_of_90D'] = data['90D_MA_Close'].rolling(window=25).mean()\n",
    "        \n",
    "        # Calculate 20 days moving average of 'Close'\n",
    "        data['25D_MA_Close'] = data['Close'].rolling(window=25).mean()\n",
    "        \n",
    "        # Calculate 14 days moving average of 'Close'\n",
    "        data['14D_MA_Close'] = data['Close'].rolling(window=14).mean()\n",
    "        \n",
    "        # # Calculate daily returns        \n",
    "        # data['Return'] = data['Close'].pct_change()\n",
    "\n",
    "        # Calculate RSI\n",
    "        delta = data['Close'].diff()\n",
    "        up, down = delta.copy(), delta.copy()\n",
    "        \n",
    "        up[up < 0] = 0\n",
    "        down[down > 0] = 0\n",
    "\n",
    "        average_gain = up.rolling(window=rsi_period).mean()\n",
    "        average_loss = abs(down.rolling(window=rsi_period).mean())\n",
    "\n",
    "        rs = average_gain / average_loss\n",
    "\n",
    "        data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # Calculate MACD Line: (12-day EMA - 26-day EMA)\n",
    "        EMA_short = data['Close'].ewm(span=short_ema_period).mean() \n",
    "        EMA_long = data['Close'].ewm(span=long_ema_period).mean() \n",
    "        data['MACD_Line'] = EMA_short - EMA_long\n",
    "\n",
    "        # Calculate Signal Line: a n-day MA of MACD Line \n",
    "        data['Signal_Line'] = data[\"MACD_Line\"].ewm(span=signal_period).mean()\n",
    "\n",
    "        # Calculate Volatility as rolling standard deviation of log returns\n",
    "        data[\"Log_Return\"] = np.log(data[\"Close\"]).diff()\n",
    "        data[\"Volatility\"] = data[\"Log_Return\"].rolling(window=vol_period).std()\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=['Open', 'Low', 'High', 'Log_Return'])  \n",
    "\n",
    "    else:\n",
    "         print(\"'Close' column is not present in the input dataframe.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "        \n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "    \n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "    \n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    # return data.drop(['Adj Close', 'Volume'], axis=1)\n",
    "    return data.drop('Adj Close', axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, seq_len, target_col, symbol, start_date, end_date, \n",
    "                          start_date_short=None, end_date_short=None, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        # train_data = train_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        # train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # train_data[train_data.columns] = train_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # train_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        train_data = train_data.copy()\n",
    "        train_data = calculate_indicators(train_data)\n",
    "        \n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        train_data.dropna(inplace=True)\n",
    "        train_data = train_data.drop(target_col_name, axis=1)\n",
    "        \n",
    "        train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "        \n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "        \n",
    "        # valid_data = valid_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        # valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # valid_data[valid_data.columns] = valid_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # valid_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        valid_data = valid_data.copy()\n",
    "        valid_data = calculate_indicators(valid_data)\n",
    "        \n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        valid_data.dropna(inplace=True)\n",
    "        valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "        valid_data[valid_data.columns] = scaler.fit_transform(valid_data)\n",
    "        \n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "            \n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized = calculate_indicators(test_data_unnormalized)\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "    \n",
    "    # test_data_unshifted = test_data.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_unshifted_reshaped = test_data_unshifted.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_unshifted_transformed = scaler.fit_transform(test_data_unshifted_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = test_data_unshifted_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data_unshifted.drop(target_col_name, axis=1, inplace=True)\n",
    " \n",
    "    # test_data = test_data.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data[test_data.columns] = test_data_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # # Fetch a fresh copy of the test data\n",
    "    # test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized = calculate_indicators(test_data_unnormalized)\n",
    "    \n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "\n",
    "    # test_data_unshifted = test_data.copy()\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "    # test_data_unshifted = test_data_unshifted.drop(target_col_name, axis=1)\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = scaler.fit_transform(test_data_unshifted)\n",
    "\n",
    "    # Scale test data\n",
    "    test_data = test_data.copy()\n",
    "    test_data = calculate_indicators(test_data)\n",
    "    \n",
    "    test_data[test_data.columns] = scaler.fit_transform(test_data)\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "    test_data.dropna(inplace=True)\n",
    "    test_data = test_data.drop(target_col_name, axis=1)    \n",
    "    \n",
    "    # Fetch a fresh copy of a short test data\n",
    "    test_data_short = fetch_data(symbol=symbol,start_date=start_date_short,end_date=end_date_short)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short_unnormalized = test_data_short.copy()\n",
    "    test_data_short_unnormalized = calculate_indicators(test_data_short_unnormalized)\n",
    "    \n",
    "    test_data_short_unnormalized['Target'] = test_data_short_unnormalized[target_col_name]\n",
    "    test_data_short_unnormalized.dropna(inplace=True)\n",
    "    test_data_short_unnormalized = test_data_short_unnormalized.drop(target_col_name, axis=1)    \n",
    "\n",
    "    # test_data_short = test_data_short.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_short_reshaped = test_data_short.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_short_transformed = scaler.fit_transform(test_data_short_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data_short[test_data_short.columns] = test_data_short_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data_short.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data_short.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short = test_data_short.copy()\n",
    "    test_data_short = calculate_indicators(test_data_short)\n",
    "    test_data_short[test_data_short.columns] = scaler.fit_transform(test_data_short)\n",
    "    test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "    test_data_short.dropna(inplace=True)\n",
    "    test_data_short = test_data_short.drop(target_col_name, axis=1) \n",
    "   \n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, seq_len\n",
    "    \n",
    "def prepare_data_common(train_data, valid_data, test_data, test_data_short, seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, seq_len)\n",
    "    X_test_short, y_test_short = create_sequences(test_data_short, seq_len)\n",
    "    \n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    X_test_short = torch.Tensor(X_test_short)\n",
    "    y_test_short = torch.Tensor(y_test_short)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short\n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-1,:])\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, l2_regularization=0.0001, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def evaluate_model(model, X, y, use_target_col=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "        y = y.view(-1, y.shape[-1]).cpu()\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "\n",
    "        if use_target_col:\n",
    "            y = y[:,-1] # Pick the last column (target column)\n",
    "            y_pred = y_pred[:,-1]\n",
    "\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def plot_predictions(model, X_test, y_test, trial, n_predict, use_target_col=True, save_directory=None, \n",
    "                     future_predictions=None, scaler=None, col_label=None, test_length=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "    \n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    \n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "        \n",
    "    y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "    output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "    \n",
    "    # If given, transform future predictions back to the original scale\n",
    "    if future_predictions is not None:\n",
    "        gap = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No future predictions found.\")\n",
    "        gap = 0\n",
    "    \n",
    "    # If future_predictions is not None, plot the future predictions\n",
    "    # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "    if use_target_col:\n",
    "        # the existing time steps first\n",
    "        time_steps = list(range(len(y_test_org)))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(time_steps, y_test_org[:, 0, -1], label='Actual')\n",
    "        plt.plot(time_steps, output_org[:, 0, -1], label='Predicted')\n",
    "        \n",
    "        # generate the future time steps\n",
    "        future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "        print('Plotting future predictions...')\n",
    "        print(\"future_time_steps:\", future_time_steps)\n",
    "        last_future_prediction = future_predictions[-n_predict:]\n",
    "        print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "        plt.plot(future_time_steps, last_future_prediction[:, -1], label='Future Predicted')\n",
    "\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "        plt.title(f'Actual and Predicted Values for {col_label[-1]} (Trial {trial+1}_{test_length})')\n",
    "        plt.legend()\n",
    "        if save_directory:\n",
    "            save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial+1}_{test_length}.png\")\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    else:\n",
    "        for j in range(n_features):\n",
    "            time_steps = list(range(len(y_test_org)))\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            ax.plot(time_steps, y_test_org[:, 0, j], label='Actual')\n",
    "            ax.plot(time_steps, output_org[:, 0, j], label='Predicted')            \n",
    "          \n",
    "            # Generate the future time steps\n",
    "            future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + n_predict + gap))\n",
    "            print('Plotting future predictions...')\n",
    "            print(\"future_time_steps:\", future_time_steps)\n",
    "            last_future_prediction = future_predictions[-n_predict:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "            plt.plot(future_time_steps, last_future_prediction[:, j], label=f'Future Predicted for {col_label[j]}')\n",
    "\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1}_{test_length})')\n",
    "            ax.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}-{test_length}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "    \n",
    "    return mse, mae, r2\n",
    "\n",
    "def predict_future(model, X_test, n_predict, n_last_sequence=1, scaler=None):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "        return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "    # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "    #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "    # Prepare the most recent input sequence\n",
    "    # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "    # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "    merge_future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "        future_predictions = []\n",
    "\n",
    "        for i in range(n_predict):\n",
    "            # Generate a prediction\n",
    "            recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "            with torch.no_grad():\n",
    "                input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "                output = model(input_seq).cpu().numpy() \n",
    "\n",
    "                future_prediction = output[0, 0, :]\n",
    "\n",
    "            # Append the prediction to the future_predictions list\n",
    "            future_predictions.append(future_prediction)         \n",
    "     \n",
    "            # Update the input sequence with the new prediction, if not the last iteration\n",
    "            if i < n_predict - 1:\n",
    "                recent_input_sequence = update_sequence(recent_input_sequence, future_prediction, sequence_length)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        future_predictions_array = np.array(future_predictions)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if merge_future_predictions is None:\n",
    "            merge_future_predictions = future_predictions_inverse\n",
    "            merge_future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "    return merge_future_predictions, merge_future_predictions_org          \n",
    "# def predict_future(model, X_test, y_test_unshifted, n_predict=5, forward=-1, n_last_sequence=1, scaler=None):\n",
    "#     n_features = X_test.shape[2]\n",
    "#     sequence_length = X_test.shape[1]\n",
    "#     torch.cuda.empty_cache()\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     def update_sequence(recent_input_sequence, future_next_prediction, sequence_length):\n",
    "#         return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_next_prediction[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "#     # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "#     #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "#     # Prepare the most recent input sequence\n",
    "#     # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "#     # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "#     # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "#     y_test_unshifted_sequences = y_test_unshifted[-(n_last_sequence-forward):, :, :]\n",
    "    \n",
    "#     last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "#     last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "#     merge_future_predictions = None\n",
    "\n",
    "#     for idx, recent_input_sequence in enumerate(last_sequences):\n",
    "\n",
    "#         future_predictions = []\n",
    "\n",
    "#         for i in range(n_predict):\n",
    "#             # Generate a prediction\n",
    "#             recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "#             with torch.no_grad():\n",
    "#                 input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "#                 output = model(input_seq).cpu().numpy() \n",
    "#                 # Use only the last feature from output and substitute other features with those from recent_input_sequence\n",
    "#                 last_feature_prediction = output[0, 0, -1:]  # Shape should be (1,)\n",
    "                \n",
    "#                 future_prediction = output[0, 0, :]\n",
    "\n",
    "#             # Append the prediction to the future_predictions list\n",
    "#             future_predictions.append(future_prediction)         \n",
    "     \n",
    "#             # Update the input sequence with the new prediction, if not the last iteration\n",
    "#             if idx + i + 1 < len(y_test_unshifted_sequences):\n",
    "                \n",
    "#                 other_features_from_y_test_unshifted = y_test_unshifted_sequences[idx+i+1, -1:, :-1]  # Shape should be (n_features-1,)\n",
    "#                 future_next_prediction = np.concatenate([other_features_from_y_test_unshifted.flatten(), last_feature_prediction])\n",
    "                \n",
    "#                 recent_input_sequence = update_sequence(recent_input_sequence, future_next_prediction, sequence_length)\n",
    "#             else:\n",
    "#                 break\n",
    "        \n",
    "#         future_predictions_array = np.array(future_predictions)\n",
    "#         future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "#         if merge_future_predictions is None:\n",
    "#             merge_future_predictions = future_predictions_inverse\n",
    "#             merge_future_predictions_org = future_predictions_array\n",
    "#         else:\n",
    "#             merge_future_predictions = np.vstack((np.round(merge_future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "#             merge_future_predictions_org = np.vstack((np.round(merge_future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "\n",
    "        \n",
    "#     return merge_future_predictions, merge_future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True, \n",
    "                  future_plot=True, overall_future_plot=True, future_predictions=None, \n",
    "                  use_target_col=True, train_data_list=None, valid_data_list=None,\n",
    "                  symbol=None, start_date=None, end_date=None, start_date_short=None, end_date_short=None,\n",
    "                  valid_size=0.5, n_predict=5, seq_len=5, n_last_sequence=1, forward=-1):\n",
    "    \n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "    \n",
    "    top_models = []\n",
    "    all_future_predictions = [] # Initialize the list to save all future predictions from each trial\n",
    "    all_future_metrics =[]\n",
    "    all_overall_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        # Generate random hyperparameters and parameters\n",
    "        seq_len = random.choice(range(5, 16))\n",
    "        nlayers = random.choice(range(1, 5))\n",
    "        nneurons = random.choice(range(32, 501))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0.1])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(300, 500))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001, 0.0005, 0.001])\n",
    "        patience = random.choice(range(20, 21))\n",
    "        min_delta = random.choice([0.0001])\n",
    "        l2_regularization = random.choice([0.0001, 0.001])\n",
    "        \n",
    "        # # Generate random hyperparameters and parameters\n",
    "        # seq_len = random.choice(range(10, 11))\n",
    "        # nlayers = random.choice(range(2, 3))\n",
    "        # nneurons = random.choice(range(100, 101))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # dropout = random.choice([0])\n",
    "        # optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW, torch.optim.Adagrad])\n",
    "        # n_epochs = random.choice(range(300, 500))\n",
    "        # batch_size = random.choice(range(256, 512))\n",
    "        # learning_rate = random.choice([0.0001])\n",
    "        # patience = random.choice(range(20, 21))\n",
    "        # min_delta = random.choice([0.0001])\n",
    "        # l2_regularization = random.choice([0])\n",
    "\n",
    "        # Prepare and preprocess the data\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, seq_len=seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, seq_len= prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date, start_date_short=start_date_short, \n",
    "                                                                                        end_date_short=end_date_short, seq_len=seq_len, target_col=target_col, forward=forward)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short = prepare_data_common(train_data=train_data, valid_data=valid_data, test_data=test_data, test_data_short=test_data_short, seq_len=seq_len)\n",
    "        \n",
    "        input_shape = (X_train.shape[0], seq_len, X_train.shape[2])\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, \n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, l2_regularization=l2_regularization)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "                     \n",
    "        # Evaluate the model on both train and test data\n",
    "        train_mse, train_mae, train_r2 = evaluate_model(model, X_train, y_train)\n",
    "        test_mse, test_mae, test_r2 = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_predict\": n_predict, \"n_last_sequence\": n_last_sequence, \"forward\": forward}        \n",
    "       \n",
    "        trial_results = [trial, params, round(train_mse, 5), round(train_mae, 5), round(train_r2, 5), round(test_mse, 5), round(test_mae, 5), round(test_r2, 5)]\n",
    "        results_df.loc[len(results_df)] = trial_results\n",
    "\n",
    "        if save_directory:\n",
    "            results_df.to_csv(os.path.join(save_directory, f\"results_{trial}.csv\"))\n",
    "            \n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "        \n",
    "        # # Inverse transform the y_test to the original scale\n",
    "        # test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)  \n",
    "        # test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "        \n",
    "        # test_data_short_unnormalized_reshaped = test_data_short_unnormalized.values.reshape(-1, 1)\n",
    "        # test_scaler_short = StandardScaler().fit(test_data_short_unnormalized_reshaped)\n",
    "        \n",
    "        # Inverse transform the y_test to the original scale \n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized)\n",
    "        \n",
    "        test_scaler_short = StandardScaler().fit(test_data_short_unnormalized)\n",
    "        \n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_predict > 0:\n",
    "            future_predictions, future_predictions_org = predict_future(loaded_model, X_test, n_predict=n_predict,\n",
    "                                                        n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        # print(f\"Future Predictions (Trial {trial+1}): {future_predictions.shape}\")\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(n_predict*n_predict):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "        \n",
    "        # Generate future predictions\n",
    "        if n_last_sequence > 0:\n",
    "            short_future_predictions, short_future_predictions_org = predict_future(loaded_model, X_test_short, n_predict=n_predict,\n",
    "                                                                                    n_last_sequence=n_last_sequence, scaler=test_scaler_short)\n",
    "        short_future_predictions = np.squeeze(short_future_predictions)\n",
    "        short_future_predictions_org = np.squeeze(short_future_predictions_org) \n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "            plot_predictions(loaded_model, X_test, y_test, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler, col_label=col_label, test_length=\"long\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "            \n",
    "            plot_predictions(loaded_model, X_test_short, y_test_short, trial, n_predict, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler_short, col_label=col_label, test_length=\"short\",\n",
    "                             future_predictions=short_future_predictions if len(short_future_predictions) > 0 else None)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        future_metrics_trial = []\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_all_features = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred_all_features = []\n",
    "        accumulated_y_pred = []\n",
    "        accumulated_y_actual_all_features = []\n",
    "        accumulated_y_actual = []\n",
    "        accumulated_y_predicted_all_features = []\n",
    "        accumulated_y_predicted = []\n",
    "\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > n_predict:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence+1-i):-((n_last_sequence+1-i)-n_predict), -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "                # else:\n",
    "                #     y_true_all_features = y_test_org[-(n_last_sequence-i):, -1]\n",
    "                #     y_true = y_true_all_features[:, -1]\n",
    "                #     y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                #     # Inverse transform the y_test to the original scale\n",
    "                #     y_actual_all_features = y_test[-(n_last_sequence-i):, -1]\n",
    "                #     y_actual = y_actual_all_features[:, -1]\n",
    "                #     y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "        # for i in range(n_last_sequence-n_predict):\n",
    "        #     # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "        #     if y_test_unshifted.shape[0] >= n_last_sequence:\n",
    "        #         if n_last_sequence-i > n_predict:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "        #         else:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):, -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):, -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_all_features.append(y_true_all_features)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred_all_features.append(y_pred_all_features)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "                accumulated_y_actual_all_features.append(y_actual_all_features)\n",
    "                accumulated_y_actual.append(y_actual)\n",
    "                accumulated_y_predicted_all_features.append(y_predicted_all_features)\n",
    "                accumulated_y_predicted.append(y_predicted)\n",
    "\n",
    "                # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "                mse_org, mae_org, r2_org = calculate_metrics(y_pred, y_true)\n",
    "                mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics(y_pred_all_features, y_true_all_features)\n",
    "                mse, mae, r2 = calculate_metrics(y_predicted, y_actual)\n",
    "                mse_all_features, mae_all_features, r2_all_features = calculate_metrics(y_predicted_all_features, y_actual_all_features)\n",
    "                # print(f\"y_pred: {y_pred}, y_true: {y_true}\")\n",
    "\n",
    "                residual = y_true - y_pred\n",
    "                error_percentage = (residual/y_true)*100\n",
    "                average_error_percentage = np.mean(error_percentage)\n",
    "        \n",
    "                # Convert arrays to lists for better CSV saving\n",
    "                y_true_list = y_true.tolist()   \n",
    "                y_pred_list = y_pred.tolist()\n",
    "                residual_list = residual.tolist()\n",
    "                error_percentage_list = error_percentage.tolist()\n",
    "                \n",
    "                # Round values for better readability if desired\n",
    "                y_true_list_rounded = [round(value ,4) for value in y_true_list]\n",
    "                y_pred_list_rounded = [round(value ,4) for value in y_pred_list]\n",
    "                residual_list_rounded=[round(value ,4) for value in residual_list]\n",
    "                error_percentage_list_rounded=[round(value ,2) for value in error_percentage_list]\n",
    "                \n",
    "                # Save future MSE and R2, actual values, predicted values, and residuals\n",
    "                future_metrics = {\n",
    "                    \"Trial\": [trial],\n",
    "                    \"Future MSE (org)\": [round(mse_org, 5)],\n",
    "                    \"Future MAE (org)\": [round(mae_org, 5)],\n",
    "                    \"Future R2 (org)\": [round(r2_org, 5)],\n",
    "                    \"Future MSE (org all features)\": [round(mse_org_all_features, 5)],\n",
    "                    \"Future MAE (org all features)\": [round(mae_org_all_features, 5)],\n",
    "                    \"Future R2 (org all features)\": [round(r2_org_all_features, 5)],\n",
    "                    \"Future MSE\": [round(mse, 5)],\n",
    "                    \"Future MAE\": [round(mae, 5)],\n",
    "                    \"Future R2\": [round(r2, 5)],\n",
    "                    \"Future MSE (all features)\": [round(mse_all_features, 5)],\n",
    "                    \"Future MAE (all features)\": [round(mae_all_features, 5)],\n",
    "                    \"Future R2 (all features)\": [round(r2_all_features, 5)],\n",
    "                    \"Actual\": [y_true_list_rounded],\n",
    "                    \"Predicted\": [y_pred_list_rounded],\n",
    "                    \"Residual\": [residual_list_rounded],\n",
    "                    \"Error Percentage\": [error_percentage_list_rounded],\n",
    "                    \"Average Error Percentage\": [round(average_error_percentage, 2)]\n",
    "                }\n",
    "\n",
    "                future_metrics_df = pd.DataFrame(future_metrics)\n",
    "\n",
    "                # Add an index column that represents each iteration\n",
    "                future_metrics_df['Trial'] = trial + 1\n",
    "                future_metrics_df['Index'] = i + 1\n",
    "\n",
    "            future_metrics_trial.append(future_metrics_df)\n",
    "\n",
    "            # Plot the actual and predicted values for the last sequence of true labels vs predicted labels\n",
    "            # Concatenate y_pred and future_predictions_target along rows\n",
    "            if future_plot:\n",
    "                \n",
    "                print(f\"Future MSE: {mse:.5f}, Future MAE: {mae:.5f}, Future R2: {r2:.5f}, Future MSE (all features): {mse_all_features:.5f}, \"\n",
    "                f\"Future MAE (all features): {mae_all_features:.5f}, Future R2 (all features): {r2_all_features:.5f}, Future MSE (org): {mse_org:.5f}, \"\n",
    "                f\"Future R2: {r2:.5f}, Average Error Percentage: {average_error_percentage:.3f}\")\n",
    "\n",
    "                combined_predictions = np.concatenate((y_pred, future_predictions_target))\n",
    "\n",
    "                # Create a new figure\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.plot(y_true, label='Actual')\n",
    "\n",
    "                # Plot combined predictions (past + future)\n",
    "                plt.plot(combined_predictions, label='Predicted')\n",
    "\n",
    "                # Add labels and title\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "                plt.title(f'Actual and Future_Predicted Values for {col_label[-1]} (Trial {trial+1}, Index {i+1})')\n",
    "                plt.legend()\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"future_predictions_plot_target_trial_{trial+1}_prdict_{i+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "        # Concatenate all the results into a single DataFrame after each trial\n",
    "        all_future_metrics_trial_df = pd.concat(future_metrics_trial)\n",
    "\n",
    "        # Reset index of final DataFrame for clarity after each trial and save it separately\n",
    "        all_future_metrics_trial_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        all_future_metrics.append(all_future_metrics_trial_df)\n",
    "\n",
    "        # Concatenate dataframes from all trials into a final dataframe.\n",
    "        all_future_metric_finals=pd.concat(all_future_metrics,axis=0)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_all_features = np.concatenate(accumulated_y_true_all_features)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred_all_features = np.concatenate(accumulated_y_pred_all_features)\n",
    "        accumulated_y_pred =np.concatenate (accumulated_y_pred )\n",
    "        accumulated_y_actual_all_features = np.concatenate(accumulated_y_actual_all_features)\n",
    "        accumulated_y_actual = np.concatenate(accumulated_y_actual)\n",
    "        accumulated_y_predicted_all_features = np.concatenate(accumulated_y_predicted_all_features)\n",
    "        accumulated_y_predicted = np.concatenate(accumulated_y_predicted)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_mse_org, overall_mae_org, overall_r2_org= calculate_metrics(accumulated_y_pred ,accumulated_y_true)\n",
    "        overall_mse_org_all_features, overall_mae_org_all_features, overall_r2_org_all_features = calculate_metrics(accumulated_y_pred_all_features ,accumulated_y_true_all_features)\n",
    "        overall_mse, overall_mae, overall_r2 = calculate_metrics(accumulated_y_predicted ,accumulated_y_actual)\n",
    "        overall_mse_all_features, overall_mae_all_features, overall_r2_all_features = calculate_metrics(accumulated_y_predicted_all_features ,accumulated_y_actual_all_features)\n",
    "        \n",
    "        overall_error_percentage = (overall_mae_org/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        overall_future_metrics  ={\n",
    "            \"Overall Trial\": [trial],\n",
    "            \"Overall Future MSE (org)\": [round(overall_mse_org, 5)],\n",
    "            \"Overall Future MAE (org)\": [round(overall_mae_org, 5)],\n",
    "            \"Overall Future R2 (org)\": [round(overall_r2_org, 5)],\n",
    "            \"Overall Future MSE (org all features)\": [round(overall_mse_org_all_features , 5)],\n",
    "            \"Overall Future MAE (org all features)\": [round(overall_mae_org_all_features, 5)],\n",
    "            \"Overall Future R2 (org all features)\": [round(overall_r2_org_all_features , 5)],\n",
    "            \"Overall Future MSE\": [round(overall_mse, 5)],\n",
    "            \"Overall Future MAE\": [round(overall_mae, 5)],\n",
    "            \"Overall Future R2\": [round(overall_r2, 5)],\n",
    "            \"Overall Future MSE (all features)\": [round(overall_mse_all_features, 5)],\n",
    "            \"Overall Future MAE (all features)\": [round(overall_mae_all_features, 5)],\n",
    "            \"Overall Future R2 (all features)\": [round(overall_r2_all_features, 5)],\n",
    "            \"Overall Future Error Percentage\": [round(overall_error_percentage, 3)]\n",
    "        }\n",
    "        all_overall_future_metrics.append(overall_future_metrics)\n",
    "        # Convert each dict in the list to a DataFrame\n",
    "        df_list = [pd.DataFrame(data=d) for d in all_overall_future_metrics]\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        all_overall_future_metrics_df = pd.concat(df_list, axis=0)\n",
    "        print(all_overall_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_overall_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "\n",
    "            combined_predictions = np.concatenate((accumulated_y_pred, future_predictions_target))\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(accumulated_y_true)),\n",
    "                    accumulated_y_true, label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)),\n",
    "                    combined_predictions, label='Predicted')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Overall Actual and Predicted Values (Trial {trial+1})')\n",
    "            plt.legend()\n",
    "\n",
    "            # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "            if save_directory:\n",
    "                save_path=os.path.join(save_directory,\n",
    "                                    f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "        # Add the resulting model to the \"top models\" list (sorted by Test MSE)\n",
    "        top_models.append((trial, params, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2))\n",
    "        top_models.sort(key=lambda x: x[6])\n",
    "        if len(top_models) > n_top_models:\n",
    "            top_models.pop()\n",
    "            \n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")        \n",
    "          \n",
    "    return results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified_2_short test data_+data_shift\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import random\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_sequences(data, X_seq_len, y_seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(data) - X_seq_len - y_seq_len + 1):\n",
    "        X.append(data[i : i + X_seq_len])\n",
    "        y.append(data[i + X_seq_len : i + X_seq_len + y_seq_len])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_differences(data):\n",
    "    # Check if the required columns exist in the data\n",
    "    if set(['Open', 'Close', 'High', 'Low']).issubset(data.columns):\n",
    "        # Create new columns\n",
    "        data['Diff_1'] = data['Close'] - data['Open']\n",
    "        data['Diff_2'] = data['High'] - data['Low']\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=['Open', 'Low', 'High'])\n",
    "        \n",
    "    else:\n",
    "        print(\"One or more of the required columns ('Open', 'Close', 'High', 'Low') are not present in the input dataframe.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def calculate_indicators(data, rsi_period=14, short_ema_period=12, long_ema_period=26, signal_period=9, vol_period=20):\n",
    "    # Check if 'Close' column exists in the data\n",
    "    if 'Close' in data.columns:\n",
    "        # Calculate daily returns\n",
    "        data['Diff_1'] = data['Close'] - data['Open']\n",
    "        data['Diff_2'] = data['High'] - data['Low']\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=['Open', 'Low', 'High'])\n",
    "        \n",
    "        data['Return'] = data['Close'].pct_change()\n",
    "\n",
    "        # Calculate RSI\n",
    "        delta = data['Close'].diff()\n",
    "        up, down = delta.copy(), delta.copy()\n",
    "\n",
    "        up[up < 0] = 0\n",
    "        down[down > 0] = 0\n",
    "\n",
    "        average_gain = up.rolling(window=rsi_period).mean()\n",
    "        average_loss = abs(down.rolling(window=rsi_period).mean())\n",
    "\n",
    "        rs = average_gain / average_loss\n",
    "\n",
    "        data['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "        # Calculate MACD Line: (12-day EMA - 26-day EMA)\n",
    "        EMA_short = data['Close'].ewm(span=short_ema_period).mean() \n",
    "        EMA_long = data['Close'].ewm(span=long_ema_period).mean() \n",
    "        data['MACD_Line'] = EMA_short - EMA_long\n",
    "\n",
    "        # Calculate Signal Line: a n-day MA of MACD Line \n",
    "        data['Signal_Line'] = data[\"MACD_Line\"].ewm(span=signal_period).mean()\n",
    "\n",
    "        # Calculate Volatility as rolling standard deviation of log returns\n",
    "        data[\"Log_Return\"] = np.log(data[\"Close\"]).diff()\n",
    "        data[\"Volatility\"] = data[\"Log_Return\"].rolling(window=vol_period).std()\n",
    "\n",
    "    else:\n",
    "         print(\"'Close' column is not present in the input dataframe.\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_data_whole(data, seq_len, target_col, scaler=StandardScaler, valid_size=0.2, forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = data.columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "        \n",
    "    data = data.copy()\n",
    "    data['Target'] = data[target_col_name].shift(forward)\n",
    "    data.dropna(inplace=True)\n",
    "    data = data.drop(target_col_name, axis=1)\n",
    "    \n",
    "    data[data.columns] = scaler().fit_transform(data)\n",
    "    \n",
    "    train_data, test_valid_data = train_test_split(data, test_size=valid_size, shuffle=False)\n",
    "    valid_data, test_data = train_test_split(test_valid_data, test_size=0.5, shuffle=False)\n",
    "\n",
    "    return prepare_data_common(train_data, valid_data, test_data, seq_len)\n",
    "\n",
    "def fetch_data(symbol, start_date, end_date):\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)\n",
    "    # return data.drop(['Adj Close', 'Volume'], axis=1)\n",
    "    return data.drop('Adj Close', axis=1)\n",
    "\n",
    "def prepare_data_separate(train_data_list, valid_data_list, X_seq_len, y_seq_len, target_col, symbol, start_date, end_date, \n",
    "                          start_date_short=None, end_date_short=None, scaler=StandardScaler(), forward=-1):\n",
    "    if isinstance(target_col, int):\n",
    "        target_col_name = train_data_list[0].columns[target_col]\n",
    "    else:\n",
    "        target_col_name = target_col\n",
    "\n",
    "    # Scale train data\n",
    "    combined_train_data = None\n",
    "    for train_data in train_data_list:\n",
    "        # train_data = train_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # train_data_reshaped = train_data.values.reshape(-1, 1)\n",
    "\n",
    "        # train_data_transformed = scaler.fit_transform(train_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # train_data[train_data.columns] = train_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # train_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # train_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        \n",
    "        train_data = train_data.copy()\n",
    "        train_data = calculate_indicators(train_data)\n",
    "        \n",
    "        train_data['Target'] = train_data[target_col_name].shift(forward)\n",
    "        train_data.dropna(inplace=True)\n",
    "        train_data = train_data.drop(target_col_name, axis=1)\n",
    "        \n",
    "        train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "\n",
    "        \n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    # Scale valid data\n",
    "    combined_valid_data = None\n",
    "    for valid_data in valid_data_list:\n",
    "        \n",
    "        # valid_data = valid_data.copy()\n",
    "\n",
    "        # # Create separate dataframes for prices and volume\n",
    "        # valid_data_reshaped = valid_data.values.reshape(-1, 1)\n",
    "\n",
    "        # valid_data_transformed = scaler.fit_transform(valid_data_reshaped)\n",
    "\n",
    "        # # Reshape it back to original shape.\n",
    "        # valid_data[valid_data.columns] = valid_data_transformed.reshape(-1, 4)\n",
    "\n",
    "        # # Shift target column by forward steps.\n",
    "        # valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "\n",
    "        # # Drop NA values if there are any due to shifting.\n",
    "        # valid_data.dropna(inplace=True)\n",
    "\n",
    "        # # Drop original target column after creating shifted Target.\n",
    "        # valid_data.drop(target_col_name, axis=1, inplace=True)\n",
    "        \n",
    "        valid_data = valid_data.copy()\n",
    "        valid_data = calculate_indicators(valid_data)\n",
    "        \n",
    "        valid_data['Target'] = valid_data[target_col_name].shift(forward)\n",
    "        valid_data.dropna(inplace=True)\n",
    "        valid_data = valid_data.drop(target_col_name, axis=1)\n",
    "        valid_data[valid_data.columns] = scaler.fit_transform(valid_data)\n",
    "        \n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "            \n",
    "    # Fetch a fresh copy of the test data\n",
    "    test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized = calculate_indicators(test_data_unnormalized)\n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "    \n",
    "    # test_data_unshifted = test_data.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_unshifted_reshaped = test_data_unshifted.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_unshifted_transformed = scaler.fit_transform(test_data_unshifted_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = test_data_unshifted_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data_unshifted.drop(target_col_name, axis=1, inplace=True)\n",
    " \n",
    "    # test_data = test_data.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_reshaped = test_data.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_transformed = scaler.fit_transform(test_data_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data[test_data.columns] = test_data_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # # Fetch a fresh copy of the test data\n",
    "    # test_data = fetch_data(symbol=symbol,start_date=start_date,end_date=end_date)\n",
    "    # Scale test data\n",
    "    test_data_unnormalized = test_data.copy()\n",
    "    test_data_unnormalized = calculate_indicators(test_data_unnormalized)\n",
    "    \n",
    "    test_data_unnormalized['Target'] = test_data_unnormalized[target_col_name]\n",
    "    test_data_unnormalized.dropna(inplace=True)\n",
    "    test_data_unnormalized = test_data_unnormalized.drop(target_col_name, axis=1)\n",
    "\n",
    "    # test_data_unshifted = test_data.copy()\n",
    "    # test_data_unshifted['Target'] = test_data_unshifted[target_col_name]\n",
    "    # test_data_unshifted.dropna(inplace=True)\n",
    "    # test_data_unshifted = test_data_unshifted.drop(target_col_name, axis=1)\n",
    "    # test_data_unshifted[test_data_unshifted.columns] = scaler.fit_transform(test_data_unshifted)\n",
    "\n",
    "    # Scale test data\n",
    "    test_data = test_data.copy()\n",
    "    test_data = calculate_indicators(test_data)\n",
    "    \n",
    "    test_data[test_data.columns] = scaler.fit_transform(test_data)\n",
    "    test_data['Target'] = test_data[target_col_name].shift(forward)\n",
    "    test_data.dropna(inplace=True)\n",
    "    test_data = test_data.drop(target_col_name, axis=1)    \n",
    "    \n",
    "    # Fetch a fresh copy of a short test data\n",
    "    test_data_short = fetch_data(symbol=symbol,start_date=start_date_short,end_date=end_date_short)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short_unnormalized = test_data_short.copy()\n",
    "    test_data_short_unnormalized = calculate_indicators(test_data_short_unnormalized)\n",
    "    \n",
    "    test_data_short_unnormalized['Target'] = test_data_short_unnormalized[target_col_name]\n",
    "    test_data_short_unnormalized.dropna(inplace=True)\n",
    "    test_data_short_unnormalized = test_data_short_unnormalized.drop(target_col_name, axis=1)    \n",
    "\n",
    "    # test_data_short = test_data_short.copy()\n",
    "\n",
    "    # # Create separate dataframes for prices and volume\n",
    "    # test_data_short_reshaped = test_data_short.values.reshape(-1, 1)\n",
    "\n",
    "    # test_data_short_transformed = scaler.fit_transform(test_data_short_reshaped)\n",
    "\n",
    "    # # Reshape it back to original shape.\n",
    "    # test_data_short[test_data_short.columns] = test_data_short_transformed.reshape(-1, 4)\n",
    "\n",
    "    # # Shift target column by forward steps.\n",
    "    # test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "\n",
    "    # # Drop NA values if there are any due to shifting.\n",
    "    # test_data_short.dropna(inplace=True)\n",
    "\n",
    "    # # Drop original target column after creating shifted Target.\n",
    "    # test_data_short.drop(target_col_name, axis=1, inplace=True)\n",
    "    \n",
    "    # Scale test data\n",
    "    test_data_short = test_data_short.copy()\n",
    "    test_data_short = calculate_indicators(test_data_short)\n",
    "    test_data_short[test_data_short.columns] = scaler.fit_transform(test_data_short)\n",
    "    test_data_short['Target'] = test_data_short[target_col_name].shift(forward)\n",
    "    test_data_short.dropna(inplace=True)\n",
    "    test_data_short = test_data_short.drop(target_col_name, axis=1) \n",
    "   \n",
    "    return combined_train_data, combined_valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, X_seq_len, y_seq_len\n",
    "    \n",
    "def prepare_data_common(train_data, valid_data, test_data, test_data_short, X_seq_len, y_seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, X_seq_len, y_seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, X_seq_len, y_seq_len)\n",
    "    X_test, y_test = create_sequences(test_data, X_seq_len, y_seq_len)\n",
    "    X_test_short, y_test_short = create_sequences(test_data_short, X_seq_len, y_seq_len)\n",
    "    \n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_test = torch.Tensor(y_test)\n",
    "    X_test_short = torch.Tensor(X_test_short)\n",
    "    y_test_short = torch.Tensor(y_test_short)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short\n",
    "\n",
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_shape, nlayers=2,\n",
    "                 nneurons=64, dropout=0.2, y_seq_len=5):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        self.y_seq_len = y_seq_len\n",
    "        \n",
    "        for _ in range(nlayers):\n",
    "            lstm_layer = nn.LSTM(input_size=input_shape[-1] if _ == 0 else nneurons,\n",
    "                                 hidden_size=nneurons,\n",
    "                                 batch_first=True)\n",
    "            self.hidden_layers.append(lstm_layer)\n",
    "            self.hidden_layers.append(self.dropout)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Linear(nneurons, input_shape[-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(0,len(self.hidden_layers),2):  # Step size of 2 because we have an LSTM and Dropout at each step.\n",
    "          x,_=self.hidden_layers[i](x)\n",
    "          x=self.hidden_layers[i+1](x)   # Applying dropout after each LSTM layer\n",
    "\n",
    "        output=self.output(x[:,-self.y_seq_len:,:])\n",
    "        # output = output.unsqueeze(1)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3,\n",
    "                l2_regularization=0.0001, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate, weight_decay=l2_regularization)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.4f}, Val Loss = {val_losses[-1]:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def evaluate_model(model, X, y, use_target_col=True):\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # Reshape the tensors to 2D and move them back to the CPU before computing metrics\n",
    "        y = y.view(-1, y.shape[-1]).cpu()\n",
    "        y_pred = y_pred.view(-1, y_pred.shape[-1]).cpu()\n",
    "\n",
    "        if use_target_col:\n",
    "            y = y[:,-1] # Pick the last column (target column)\n",
    "            y_pred = y_pred[:,-1]\n",
    "\n",
    "        mse = mean_squared_error(y, y_pred)\n",
    "        mae = mean_absolute_error(y, y_pred)\n",
    "        r2 = r2_score(y, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def inverse_transform_wrapper(data, orgshape, scaler):\n",
    "    data_reshaped = data.reshape(-1, data.shape[-1])\n",
    "    data_inv = scaler.inverse_transform(data_reshaped)\n",
    "    data_inv_origshape = data_inv.reshape(orgshape)\n",
    "    return data_inv_origshape\n",
    "\n",
    "def plot_predictions(model, X_test, y_test, trial, y_seq_len=None, use_target_col=True, save_directory=None, \n",
    "                     future_predictions=None, scaler=None, col_label=None, test_length=None):\n",
    "    torch.cuda.empty_cache()\n",
    "    # Get n_features from X_test\n",
    "    n_features = X_test.shape[2]\n",
    "    \n",
    "    # Move the model and input tensor to the same device.\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "\n",
    "    # Run the model on the input tensor and move the predictions back to the CPU, if needed.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(X_test).cpu()\n",
    "   \n",
    "    y_test_org = inverse_transform_wrapper (y_test, y_test.shape, scaler=scaler)\n",
    "    output_org = inverse_transform_wrapper (output, output.shape, scaler=scaler)\n",
    "    \n",
    "    # If given, transform future predictions back to the original scale\n",
    "    if future_predictions is not None:\n",
    "        gap = 0\n",
    "\n",
    "    else:\n",
    "        print(\"No future predictions found.\")\n",
    "        gap = 0\n",
    "    \n",
    "    # If future_predictions is not None, plot the future predictions\n",
    "    # If use_target_col is True, only plot the target column, otherwise plot all feature columns\n",
    "    if use_target_col:\n",
    "        # the existing time steps first\n",
    "        time_steps = list(range(len(y_test_org)))\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.plot(time_steps, y_test_org[:, :, -1], label='Actual')\n",
    "        plt.plot(time_steps, output_org[:, :, -1], label='Predicted')\n",
    "        \n",
    "        # generate the future time steps\n",
    "        future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + y_seq_len + gap))\n",
    "        print('Plotting future predictions...')\n",
    "        print(\"future_time_steps:\", future_time_steps)\n",
    "        last_future_prediction = future_predictions[-y_seq_len:]\n",
    "        print(\"future_predictions:\", last_future_prediction[:, -1])\n",
    "        plt.plot(future_time_steps, last_future_prediction[:, -1], label='Future Predicted')\n",
    "\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "        plt.title(f'Actual and Predicted Values for {col_label[-1]} (Trial {trial+1}_{test_length})')\n",
    "        plt.legend()\n",
    "        if save_directory:\n",
    "            save_path = os.path.join(save_directory, f\"predictions_plot_target_trial_{trial+1}_{test_length}.png\")\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    else:\n",
    "        for j in range(n_features):\n",
    "            time_steps = list(range(len(y_test_org)))\n",
    "            fig, ax = plt.subplots(figsize=(15, 8))\n",
    "            ax.plot(time_steps, y_test_org[:, :, j], label='Actual')\n",
    "            ax.plot(time_steps, output_org[:, :, j], label='Predicted')            \n",
    "          \n",
    "            # Generate the future time steps\n",
    "            future_time_steps = list(range(len(y_test_org) + gap, len(y_test_org) + y_seq_len + gap))\n",
    "            print('Plotting future predictions...')\n",
    "            print(\"future_time_steps:\", future_time_steps)\n",
    "            last_future_prediction = future_predictions[-y_seq_len:]\n",
    "            print(\"future_predictions:\", last_future_prediction[:, j])\n",
    "            plt.plot(future_time_steps, last_future_prediction[:, j], label=f'Future Predicted for {col_label[j]}')\n",
    "\n",
    "            ax.set_xlabel('Time Step')\n",
    "            ax.set_ylabel('Value')\n",
    "            # ax.set_title(f'Actual and Predicted Values for Variable {j + 1} (Trial {trial+1})')\n",
    "            plt.title(f'Actual and Predicted Values for {col_label[j]} (Trial {trial+1}_{test_length})')\n",
    "            ax.legend()\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"predictions_plot_var_{j + 1}_trial_{trial}-{test_length}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray , y_pred: np.ndarray):\n",
    "    mse = mean_squared_error(y_true=y_true,y_pred=y_pred)\n",
    "    mae = mean_absolute_error(y_true=y_true,y_pred=y_pred)\n",
    "    r2 = r2_score(y_true=y_true,y_pred=y_pred)\n",
    "    \n",
    "    return mse, mae, r2\n",
    "          \n",
    "def predict_future(model, X_test, n_last_sequence=1, scaler=None):\n",
    "    n_features = X_test.shape[2]\n",
    "    sequence_length = X_test.shape[1]\n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # def update_sequence(recent_input_sequence, future_update, sequence_length):\n",
    "    #     return np.concatenate([recent_input_sequence[:, -(sequence_length-1):, :], future_update[np.newaxis, np.newaxis, :]], axis=1)\n",
    "        \n",
    "    # def new_sequence(last_sequences, y_test, sequence_length):\n",
    "    #     return np.concatenate([last_sequences[:, -(sequence_length-1):, :], y_test[:, :, :]], axis=1)\n",
    "        \n",
    "    # Prepare the most recent input sequence\n",
    "    # x_test_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    # y_test_sequences = y_test[-(n_last_sequence):, :, :]\n",
    "    \n",
    "    # merge_sequences = new_sequence(x_test_sequences, y_test_sequences, sequence_length)\n",
    "    last_sequences = X_test[-(n_last_sequence):, :, :]\n",
    "    last_sequences = torch.Tensor(last_sequences)\n",
    "    \n",
    "    future_predictions = None\n",
    "\n",
    "    for recent_input_sequence in last_sequences:\n",
    "        # Generate a prediction\n",
    "        recent_input_sequence = recent_input_sequence.reshape(1, sequence_length, n_features) \n",
    "        with torch.no_grad():\n",
    "            input_seq = torch.Tensor(recent_input_sequence).to(device)\n",
    "            output = model(input_seq).cpu().numpy() \n",
    "\n",
    "            future_prediction = output[0, :, :]\n",
    "            # future_update = output[0, 0, 1]\n",
    "    \n",
    "        # # Update the input sequence with the new prediction, if not the last iteration\n",
    "        # recent_input_sequence = update_sequence(recent_input_sequence, future_update, sequence_length)\n",
    "       \n",
    "        future_predictions_array = np.array(future_prediction)\n",
    "        future_predictions_inverse = inverse_transform_wrapper(future_predictions_array, future_predictions_array.shape, scaler=scaler)\n",
    "\n",
    "        if future_predictions is None:\n",
    "            future_predictions = future_predictions_inverse\n",
    "            future_predictions_org = future_predictions_array\n",
    "        else:\n",
    "            future_predictions = np.vstack((np.round(future_predictions, 5), np.round(future_predictions_inverse, 5)))\n",
    "            future_predictions_org = np.vstack((np.round(future_predictions_org, 5), np.round(future_predictions_array, 5)))\n",
    "        \n",
    "    return future_predictions, future_predictions_org\n",
    "\n",
    "def random_search(data, target_col=None, n_trials=1, n_top_models=1,\n",
    "                   model_save=True, save_directory=None, plot_loss=True, predict_plot=True, \n",
    "                  future_plot=True, overall_future_plot=True, future_predictions=None, \n",
    "                  use_target_col=True, train_data_list=None, valid_data_list=None,\n",
    "                  symbol=None, start_date=None, end_date=None, start_date_short=None, end_date_short=None,\n",
    "                  valid_size=0.5, X_seq_len=10, y_seq_len=5, n_last_sequence=1, forward=-1):\n",
    "    \n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    results_df = pd.DataFrame(columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "    \n",
    "    top_models = []\n",
    "    all_future_predictions = [] # Initialize the list to save all future predictions from each trial\n",
    "    all_future_metrics =[]\n",
    "    all_overall_future_metrics = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "        start = timeit.default_timer()\n",
    "    \n",
    "        # Generate random hyperparameters and parameters\n",
    "        X_seq_len = random.choice(range(10, 11))\n",
    "        y_seq_len = random.choice(range(4, 5))\n",
    "        nlayers = random.choice(range(1, 3))\n",
    "        nneurons = random.choice(range(200, 301))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        dropout = random.choice([0])\n",
    "        optimizer = random.choice([torch.optim.Adam])\n",
    "        n_epochs = random.choice(range(500, 1001))\n",
    "        batch_size = random.choice(range(256, 512))\n",
    "        learning_rate = random.choice([0.0001])\n",
    "        patience = random.choice(range(5, 6))\n",
    "        min_delta = random.choice([0.0001])\n",
    "        l2_regularization = random.choice([0])\n",
    "        \n",
    "        # # Generate random hyperparameters and parameters\n",
    "        # seq_len = random.choice(range(20, 61))\n",
    "        # nlayers = random.choice(range(1, 6))\n",
    "        # nneurons = random.choice(range(100, 256))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # dropout = random.choice([0.1, 0.2, 0.3, 0.4])\n",
    "        # optimizer = random.choice([torch.optim.Adam])\n",
    "        # n_epochs = random.choice(range(300, 500))\n",
    "        # batch_size = random.choice(range(128, 256))\n",
    "        # learning_rate = random.choice([0.0001, 0.0005, 0.001])\n",
    "        # patience = random.choice(range(5, 21))\n",
    "        # min_delta = random.choice([0.0001, 0.0002])\n",
    "        # l2_regularization = random.choice([0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1])\n",
    "\n",
    "        # Prepare and preprocess the data\n",
    "        if data is not None:\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test = prepare_data_whole(data=data, X_seq_len=X_seq_len, y_seq_len=y_seq_len,\n",
    "                                                                target_col=target_col, valid_size=valid_size,forward=forward)\n",
    "\n",
    "        if train_data_list is not None:\n",
    "            train_data, valid_data, test_data, test_data_unnormalized, test_data_short, test_data_short_unnormalized, X_seq_len, y_seq_len = prepare_data_separate(train_data_list=train_data_list, valid_data_list=valid_data_list,\n",
    "                                                                                        symbol=symbol,start_date=start_date,end_date=end_date, start_date_short=start_date_short, \n",
    "                                                                                        end_date_short=end_date_short, X_seq_len=X_seq_len, y_seq_len=y_seq_len, target_col=target_col)\n",
    "\n",
    "            # Call prepare_data_common() with test_data_unnormalized\n",
    "            X_train, y_train, X_valid, y_valid, X_test, y_test, X_test_short, y_test_short = prepare_data_common(train_data=train_data, valid_data=valid_data, test_data=test_data, \n",
    "                                                                                                                test_data_short=test_data_short, X_seq_len=X_seq_len, y_seq_len=y_seq_len)\n",
    "        \n",
    "        input_shape = (X_train.shape[0], X_seq_len, X_train.shape[2])\n",
    "        \n",
    "        # Initialize the model\n",
    "        model = LSTMRegression(input_shape=input_shape, nlayers=nlayers, nneurons=nneurons, dropout=dropout, y_seq_len=y_seq_len)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, \n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate, l2_regularization=l2_regularization)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "                     \n",
    "        # Evaluate the model on both train and test data\n",
    "        train_mse, train_mae, train_r2 = evaluate_model(model, X_train, y_train)\n",
    "        test_mse, test_mae, test_r2 = evaluate_model(model, X_test, y_test)\n",
    "        \n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"X_seq_len\": X_seq_len, \"y_seq_len\": y_seq_len, \"nlayers\": nlayers, \"nneurons\": nneurons, \n",
    "                  \"dropout\": dropout, \"optimizer\": optimizer, \"n_epochs\": n_epochs,\n",
    "                  \"batch_size\": batch_size, \"learning_rate\": learning_rate,\n",
    "                  \"patience\": patience, \"min_delta\": min_delta, \"l2_regularization\": l2_regularization,\n",
    "                  \"n_last_sequence\": n_last_sequence, \"forward\": forward}        \n",
    "       \n",
    "        trial_results = [trial, params, round(train_mse, 5), round(train_mae, 5), round(train_r2, 5), round(test_mse, 5), round(test_mae, 5), round(test_r2, 5)]\n",
    "        results_df.loc[len(results_df)] = trial_results\n",
    "\n",
    "        if save_directory:\n",
    "            results_df.to_csv(os.path.join(save_directory, f\"results_{trial}.csv\"))\n",
    "            \n",
    "        # initialize variables to store most recently saved model's path\n",
    "        most_recent_save_path = None\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "            most_recent_save_path = save_path\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # Load the most recently saved model\n",
    "        if most_recent_save_path:\n",
    "            loaded_model = torch.load(most_recent_save_path)\n",
    "            loaded_model = loaded_model.to(device)\n",
    "            loaded_model.eval()\n",
    "        \n",
    "        # # Inverse transform the y_test to the original scale\n",
    "        # test_data_unnormalized_reshaped = test_data_unnormalized.values.reshape(-1, 1)  \n",
    "        # test_scaler = StandardScaler().fit(test_data_unnormalized_reshaped)\n",
    "        \n",
    "        # test_data_short_unnormalized_reshaped = test_data_short_unnormalized.values.reshape(-1, 1)\n",
    "        # test_scaler_short = StandardScaler().fit(test_data_short_unnormalized_reshaped)\n",
    "        \n",
    "        # Inverse transform the y_test to the original scale \n",
    "        test_scaler = StandardScaler().fit(test_data_unnormalized)\n",
    "        \n",
    "        test_scaler_short = StandardScaler().fit(test_data_short_unnormalized)\n",
    "        \n",
    "        # Get the column names\n",
    "        col_label = test_data_unnormalized.columns\n",
    "\n",
    "        # Generate future predictions\n",
    "        if n_last_sequence > 0:\n",
    "            future_predictions, future_predictions_org = predict_future(loaded_model, X_test,  \n",
    "                                                        n_last_sequence=n_last_sequence, scaler=test_scaler)\n",
    "        future_predictions_df = pd.DataFrame(future_predictions, columns=[f\"Future_Predicted_{col_label[i]}\" for i in range(X_test.shape[2])])\n",
    "        future_predictions_all_features = future_predictions_df.iloc[-(y_seq_len*y_seq_len):]\n",
    "        future_predictions_target = future_predictions_all_features.iloc[:, -1]\n",
    "\n",
    "        # Create a DataFrame for future_predictions_target with a 'Trial' column\n",
    "        future_predictions_target_df = future_predictions_target.to_frame(name='Future_Predicted_Target')\n",
    "        future_predictions_target_df['Trial'] = trial + 1\n",
    "\n",
    "        # Append the new DataFrame to the list\n",
    "        all_future_predictions.append(future_predictions_target_df)\n",
    "\n",
    "        # Concatenate all the future predictions into a single DataFrame\n",
    "        all_future_predictions_df = pd.concat(all_future_predictions, axis=0)\n",
    "        print(f\"Future Predictions (Trial {trial+1}): {future_predictions_target_df}\")\n",
    "\n",
    "        # Plot prediction results\n",
    "        if predict_plot:\n",
    "            plot_predictions(loaded_model, X_test, y_test, trial, y_seq_len=y_seq_len, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler, col_label=col_label, test_length=\"long\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "            \n",
    "            plot_predictions(loaded_model, X_test=X_test_short, y_test=y_test_short, trial=trial, y_seq_len=y_seq_len, use_target_col=use_target_col,\n",
    "                             save_directory=save_directory, scaler=test_scaler_short, col_label=col_label, test_length=\"short\",\n",
    "                             future_predictions=future_predictions if len(future_predictions) > 0 else None)\n",
    "\n",
    "        # Inverse transform the y_test to the original scale\n",
    "        y_test_org = inverse_transform_wrapper(y_test, y_test.shape, scaler=test_scaler)\n",
    "\n",
    "        future_metrics_trial = []\n",
    "\n",
    "        # Initialize accumulators\n",
    "        accumulated_y_true_all_features = []\n",
    "        accumulated_y_true = []\n",
    "        accumulated_y_pred_all_features = []\n",
    "        accumulated_y_pred = []\n",
    "        accumulated_y_actual_all_features = []\n",
    "        accumulated_y_actual = []\n",
    "        accumulated_y_predicted_all_features = []\n",
    "        accumulated_y_predicted = []\n",
    "\n",
    "\n",
    "        for i in range(n_last_sequence):\n",
    "            # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "            if y_test_org.shape[0] >= n_last_sequence:\n",
    "                if n_last_sequence-i > y_seq_len:\n",
    "                    y_true_all_features = y_test_org[-(n_last_sequence-i):-((n_last_sequence-i)-y_seq_len), -1]\n",
    "                    y_true = y_true_all_features[:, -1]\n",
    "                    y_pred_all_features = np.array(future_predictions[-(y_seq_len*(n_last_sequence-i)):-(y_seq_len*(n_last_sequence-(i+1)))])\n",
    "                    y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                    # Inverse transform the y_test to the original scale\n",
    "                    y_actual_all_features = y_test[-(n_last_sequence-i):-((n_last_sequence-i)-y_seq_len), -1]\n",
    "                    y_actual = y_actual_all_features[:, -1]\n",
    "                    y_predicted_all_features = np.array(future_predictions_org[-(y_seq_len*(n_last_sequence-i)):-(y_seq_len*(n_last_sequence-(i+1)))])\n",
    "                    y_predicted = y_predicted_all_features[:, -1]\n",
    "                # else:\n",
    "                #     y_true_all_features = y_test_org[-(n_last_sequence+1-i):, -1]\n",
    "                #     y_true = y_true_all_features[:, -1]\n",
    "                #     y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "                #     # Inverse transform the y_test to the original scale\n",
    "                #     y_actual_all_features = y_test[-(n_last_sequence+1-i):, -1]\n",
    "                #     y_actual = y_actual_all_features[:, -1]\n",
    "                #     y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "                #     y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "        # for i in range(n_last_sequence-n_predict):\n",
    "        #     # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "        #     if y_test_unshifted.shape[0] >= n_last_sequence:\n",
    "        #         if n_last_sequence-i > n_predict:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):-((n_last_sequence-i)-n_predict), -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-(n_predict*(n_last_sequence-(i+1)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "        #         else:\n",
    "        #             y_true_all_features = y_test_unshifted_org[-(n_last_sequence-i):, -1]\n",
    "        #             y_true = y_true_all_features[:, -1]\n",
    "        #             y_pred_all_features = np.array(future_predictions[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_pred = y_pred_all_features[:, -1]\n",
    "\n",
    "        #             # Inverse transform the y_test to the original scale\n",
    "        #             y_actual_all_features = y_test_unshifted[-(n_last_sequence-i):, -1]\n",
    "        #             y_actual = y_actual_all_features[:, -1]\n",
    "        #             y_predicted_all_features = np.array(future_predictions_org[-(n_predict*(n_last_sequence-i)):-((n_predict*(n_last_sequence-(i+1)))-((n_last_sequence-i)-(n_predict)))])\n",
    "        #             y_predicted = y_predicted_all_features[:, -1]\n",
    "\n",
    "                # Add these lines inside both conditions above, after calculating y_* variables.\n",
    "                accumulated_y_true_all_features.append(y_true_all_features)\n",
    "                accumulated_y_true.append(y_true)\n",
    "                accumulated_y_pred_all_features.append(y_pred_all_features)\n",
    "                accumulated_y_pred.append(y_pred)\n",
    "                accumulated_y_actual_all_features.append(y_actual_all_features)\n",
    "                accumulated_y_actual.append(y_actual)\n",
    "                accumulated_y_predicted_all_features.append(y_predicted_all_features)\n",
    "                accumulated_y_predicted.append(y_predicted)\n",
    "\n",
    "                # Calculate metrics for the last sequence of true labels vs predicted labels\n",
    "                mse_org, mae_org, r2_org = calculate_metrics(y_pred, y_true)\n",
    "                mse_org_all_features, mae_org_all_features, r2_org_all_features = calculate_metrics(y_pred_all_features, y_true_all_features)\n",
    "                mse, mae, r2 = calculate_metrics(y_predicted, y_actual)\n",
    "                mse_all_features, mae_all_features, r2_all_features = calculate_metrics(y_predicted_all_features, y_actual_all_features)\n",
    "                # print(f\"y_pred: {y_pred}, y_true: {y_true}\")\n",
    "\n",
    "                residual = y_true - y_pred\n",
    "                error_percentage = (residual/y_true)*100\n",
    "                average_error_percentage = np.mean(error_percentage)\n",
    "        \n",
    "                # Convert arrays to lists for better CSV saving\n",
    "                y_true_list = y_true.tolist()   \n",
    "                y_pred_list = y_pred.tolist()\n",
    "                residual_list = residual.tolist()\n",
    "                error_percentage_list = error_percentage.tolist()\n",
    "                \n",
    "                # Round values for better readability if desired\n",
    "                y_true_list_rounded = [round(value ,4) for value in y_true_list]\n",
    "                y_pred_list_rounded = [round(value ,4) for value in y_pred_list]\n",
    "                residual_list_rounded=[round(value ,4) for value in residual_list]\n",
    "                error_percentage_list_rounded=[round(value ,2) for value in error_percentage_list]\n",
    "                \n",
    "                # Save future MSE and R2, actual values, predicted values, and residuals\n",
    "                future_metrics = {\n",
    "                    \"Trial\": [trial],\n",
    "                    \"Future MSE (org)\": [round(mse_org, 5)],\n",
    "                    \"Future MAE (org)\": [round(mae_org, 5)],\n",
    "                    \"Future R2 (org)\": [round(r2_org, 5)],\n",
    "                    \"Future MSE (org all features)\": [round(mse_org_all_features, 5)],\n",
    "                    \"Future MAE (org all features)\": [round(mae_org_all_features, 5)],\n",
    "                    \"Future R2 (org all features)\": [round(r2_org_all_features, 5)],\n",
    "                    \"Future MSE\": [round(mse, 5)],\n",
    "                    \"Future MAE\": [round(mae, 5)],\n",
    "                    \"Future R2\": [round(r2, 5)],\n",
    "                    \"Future MSE (all features)\": [round(mse_all_features, 5)],\n",
    "                    \"Future MAE (all features)\": [round(mae_all_features, 5)],\n",
    "                    \"Future R2 (all features)\": [round(r2_all_features, 5)],\n",
    "                    \"Actual\": [y_true_list_rounded],\n",
    "                    \"Predicted\": [y_pred_list_rounded],\n",
    "                    \"Residual\": [residual_list_rounded],\n",
    "                    \"Error Percentage\": [error_percentage_list_rounded],\n",
    "                    \"Average Error Percentage\": [round(average_error_percentage, 2)]\n",
    "                }\n",
    "\n",
    "                future_metrics_df = pd.DataFrame(future_metrics)\n",
    "\n",
    "                # Add an index column that represents each iteration\n",
    "                future_metrics_df['Trial'] = trial + 1\n",
    "                future_metrics_df['Index'] = i + 1\n",
    "\n",
    "            future_metrics_trial.append(future_metrics_df)\n",
    "\n",
    "            # Plot the actual and predicted values for the last sequence of true labels vs predicted labels\n",
    "            # Concatenate y_pred and future_predictions_target along rows\n",
    "            if future_plot:\n",
    "                \n",
    "                print(f\"Future MSE: {mse:.5f}, Future MAE: {mae:.5f}, Future R2: {r2:.5f}, Future MSE (all features): {mse_all_features:.5f}, \"\n",
    "                f\"Future MAE (all features): {mae_all_features:.5f}, Future R2 (all features): {r2_all_features:.5f}, Future MSE (org): {mse_org:.5f}, \"\n",
    "                f\"Future R2: {r2:.5f}, Average Error Percentage: {average_error_percentage:.3f}\")\n",
    "\n",
    "                combined_predictions = np.concatenate((y_pred, future_predictions_target))\n",
    "\n",
    "                # Create a new figure\n",
    "                plt.figure(figsize=(15, 8))\n",
    "                plt.plot(y_true, label='Actual')\n",
    "\n",
    "                # Plot combined predictions (past + future)\n",
    "                plt.plot(combined_predictions, label='Predicted')\n",
    "\n",
    "                # Add labels and title\n",
    "                plt.xlabel('Time Step')\n",
    "                plt.ylabel('Value')\n",
    "                # plt.title(f'Actual and Predicted Values for Target Variable (Trial {trial+1})')\n",
    "                plt.title(f'Actual and Future_Predicted Values for {col_label[-1]} (Trial {trial+1}, Index {i+1})')\n",
    "                plt.legend()\n",
    "                if save_directory:\n",
    "                    save_path = os.path.join(save_directory, f\"future_predictions_plot_target_trial_{trial+1}_prdict_{i+1}.png\")\n",
    "                    plt.savefig(save_path)\n",
    "                plt.show()\n",
    "\n",
    "        # Concatenate all the results into a single DataFrame after each trial\n",
    "        all_future_metrics_trial_df = pd.concat(future_metrics_trial)\n",
    "\n",
    "        # Reset index of final DataFrame for clarity after each trial and save it separately\n",
    "        all_future_metrics_trial_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        all_future_metrics.append(all_future_metrics_trial_df)\n",
    "\n",
    "        # Concatenate dataframes from all trials into a final dataframe.\n",
    "        all_future_metric_finals=pd.concat(all_future_metrics,axis=0)\n",
    "\n",
    "        # After your loop, convert accumulators into numpy arrays\n",
    "        accumulated_y_true_all_features = np.concatenate(accumulated_y_true_all_features)\n",
    "        accumulated_y_true = np.concatenate(accumulated_y_true)\n",
    "        accumulated_y_pred_all_features = np.concatenate(accumulated_y_pred_all_features)\n",
    "        accumulated_y_pred =np.concatenate (accumulated_y_pred )\n",
    "        accumulated_y_actual_all_features = np.concatenate(accumulated_y_actual_all_features)\n",
    "        accumulated_y_actual = np.concatenate(accumulated_y_actual)\n",
    "        accumulated_y_predicted_all_features = np.concatenate(accumulated_y_predicted_all_features)\n",
    "        accumulated_y_predicted = np.concatenate(accumulated_y_predicted)\n",
    "\n",
    "        # Calculate overall metrics\n",
    "        overall_mse_org, overall_mae_org, overall_r2_org= calculate_metrics(accumulated_y_pred ,accumulated_y_true)\n",
    "        overall_mse_org_all_features, overall_mae_org_all_features, overall_r2_org_all_features = calculate_metrics(accumulated_y_pred_all_features ,accumulated_y_true_all_features)\n",
    "        overall_mse, overall_mae, overall_r2 = calculate_metrics(accumulated_y_predicted ,accumulated_y_actual)\n",
    "        overall_mse_all_features, overall_mae_all_features, overall_r2_all_features = calculate_metrics(accumulated_y_predicted_all_features ,accumulated_y_actual_all_features)\n",
    "        \n",
    "        overall_error_percentage = (overall_mae_org/accumulated_y_true.mean())*100\n",
    "\n",
    "        # Create a dictionary for overall future metrics\n",
    "        overall_future_metrics  ={\n",
    "            \"Overall Trial\": [trial],\n",
    "            \"Overall Future MSE (org)\": [round(overall_mse_org, 5)],\n",
    "            \"Overall Future MAE (org)\": [round(overall_mae_org, 5)],\n",
    "            \"Overall Future R2 (org)\": [round(overall_r2_org, 5)],\n",
    "            \"Overall Future MSE (org all features)\": [round(overall_mse_org_all_features , 5)],\n",
    "            \"Overall Future MAE (org all features)\": [round(overall_mae_org_all_features, 5)],\n",
    "            \"Overall Future R2 (org all features)\": [round(overall_r2_org_all_features , 5)],\n",
    "            \"Overall Future MSE\": [round(overall_mse, 5)],\n",
    "            \"Overall Future MAE\": [round(overall_mae, 5)],\n",
    "            \"Overall Future R2\": [round(overall_r2, 5)],\n",
    "            \"Overall Future MSE (all features)\": [round(overall_mse_all_features, 5)],\n",
    "            \"Overall Future MAE (all features)\": [round(overall_mae_all_features, 5)],\n",
    "            \"Overall Future R2 (all features)\": [round(overall_r2_all_features, 5)],\n",
    "            \"Overall Future Error Percentage\": [round(overall_error_percentage, 3)]\n",
    "        }\n",
    "        all_overall_future_metrics.append(overall_future_metrics)\n",
    "        # Convert each dict in the list to a DataFrame\n",
    "        df_list = [pd.DataFrame(data=d) for d in all_overall_future_metrics]\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        all_overall_future_metrics_df = pd.concat(df_list, axis=0)\n",
    "        print(all_overall_future_metrics_df)\n",
    "\n",
    "        if save_directory:\n",
    "            all_overall_future_metrics_df.to_csv(f'{save_directory}/{trial}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "        # # Convert dictionary into DataFrame and append it to final results dataframe\n",
    "        if overall_future_plot:\n",
    "\n",
    "            combined_predictions = np.concatenate((accumulated_y_pred, future_predictions_target))\n",
    "            plt.figure(figsize=(15,8))\n",
    "            plt.plot(np.arange(len(accumulated_y_true)),\n",
    "                    accumulated_y_true, label='Actual')\n",
    "            plt.plot(np.arange(len(combined_predictions)),\n",
    "                    combined_predictions, label='Predicted')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title(f'Overall Actual and Predicted Values (Trial {trial+1})')\n",
    "            plt.legend()\n",
    "\n",
    "            # plt.savefig(f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "            if save_directory:\n",
    "                save_path=os.path.join(save_directory,\n",
    "                                    f\"overall_predictions_plot_trial_{trial+1}.png\")\n",
    "                plt.savefig(save_path)\n",
    "            plt.show()\n",
    "\n",
    "        # Add the resulting model to the \"top models\" list (sorted by Test MSE)\n",
    "        top_models.append((trial, params, train_mse, train_mae, train_r2, test_mse, test_mae, test_r2))\n",
    "        top_models.sort(key=lambda x: x[6])\n",
    "        if len(top_models) > n_top_models:\n",
    "            top_models.pop()\n",
    "            \n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")        \n",
    "          \n",
    "    return results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chip Maker\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "start_date = '2018-01-01'\n",
    "end_date = '2023-09-26'\n",
    "symbol = 'TSM'\n",
    "TSM = yf.download(symbol, start=start_date, end=end_date)\n",
    "TSM = TSM.drop('Adj Close', axis=1)\n",
    "symbol = 'INTC'\n",
    "INTC = yf.download(symbol, start=start_date, end=end_date)\n",
    "INTC = INTC.drop('Adj Close', axis=1)\n",
    "symbol = 'ASML'\n",
    "ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASML = ASML.drop('Adj Close', axis=1)\n",
    "symbol = 'MU'\n",
    "MU = yf.download(symbol, start=start_date, end=end_date)\n",
    "MU = MU.drop('Adj Close', axis=1)\n",
    "symbol = 'NVDA'\n",
    "NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "NVDA = NVDA.drop('Adj Close', axis=1)\n",
    "symbol = 'AMD'\n",
    "AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMD = AMD.drop('Adj Close', axis=1)\n",
    "symbol = 'QCOM'\n",
    "QCOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "QCOM = QCOM.drop('Adj Close', axis=1)\n",
    "symbol = 'SNPS'\n",
    "SNPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "SNPS = SNPS.drop('Adj Close', axis=1)\n",
    "symbol = 'MRVL'\n",
    "MRVL = yf.download(symbol, start=start_date, end=end_date)\n",
    "MRVL = MRVL.drop('Adj Close', axis=1)\n",
    "symbol = '^IXIC'\n",
    "IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "IXIC = IXIC.drop('Adj Close', axis=1)\n",
    "\n",
    "# information technology\n",
    "\n",
    "symbol = 'AAPL'\n",
    "AAPL = yf.download(symbol, start=start_date, end=end_date)\n",
    "AAPL = AAPL.drop('Adj Close', axis=1)\n",
    "symbol = 'MSFT'\n",
    "MSFT = yf.download(symbol, start=start_date, end=end_date)\n",
    "MSFT = MSFT.drop('Adj Close', axis=1)\n",
    "symbol = 'TSLA'\n",
    "TSLA = yf.download(symbol, start=start_date, end=end_date)\n",
    "TSLA = TSLA.drop('Adj Close', axis=1)\n",
    "symbol = 'GOOGL'\n",
    "GOOGL = yf.download(symbol, start=start_date, end=end_date)\n",
    "GOOGL = GOOGL.drop('Adj Close', axis=1)\n",
    "symbol = 'GOOG'\n",
    "GOOG = yf.download(symbol, start=start_date, end=end_date)\n",
    "GOOG = GOOG.drop('Adj Close', axis=1)\n",
    "symbol = 'AMZN'\n",
    "AMZN = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMZN = AMZN.drop('Adj Close', axis=1)\n",
    "symbol = 'META'\n",
    "META = yf.download(symbol, start=start_date, end=end_date)\n",
    "META = META.drop('Adj Close', axis=1)\n",
    "symbol = 'AMD'\n",
    "AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMD = AMD.drop('Adj Close', axis=1)\n",
    "symbol = 'ASML'\n",
    "ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASML = ASML.drop('Adj Close', axis=1)\n",
    "symbol = 'NVDA'\n",
    "NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "NVDA = NVDA.drop('Adj Close', axis=1)\n",
    "symbol = 'IBM'\n",
    "IBM = yf.download(symbol, start=start_date, end=end_date)\n",
    "IBM = IBM.drop('Adj Close', axis=1)\n",
    "symbol = 'NFLX'\n",
    "NFLX = yf.download(symbol, start=start_date, end=end_date)\n",
    "NFLX = NFLX.drop('Adj Close', axis=1)\n",
    "\n",
    "# Consumer\n",
    "\n",
    "\n",
    "symbol = 'WMT'\n",
    "WMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "WMT = WMT.drop('Adj Close', axis=1)\n",
    "symbol = 'TGT'\n",
    "TGT = yf.download(symbol, start=start_date, end=end_date)\n",
    "TGT = TGT.drop('Adj Close', axis=1)\n",
    "symbol = 'COST'\n",
    "COST = yf.download(symbol, start=start_date, end=end_date)\n",
    "COST = COST.drop('Adj Close', axis=1)\n",
    "symbol = 'HD'\n",
    "HD = yf.download(symbol, start=start_date, end=end_date)\n",
    "HD = HD.drop('Adj Close', axis=1)\n",
    "symbol = 'LOW'\n",
    "LOW = yf.download(symbol, start=start_date, end=end_date)\n",
    "LOW = LOW.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'PG'\n",
    "PG = yf.download(symbol, start=start_date, end=end_date)\n",
    "PG = PG.drop('Adj Close', axis=1)\n",
    "symbol = 'JNJ'\n",
    "JNJ = yf.download(symbol, start=start_date, end=end_date)\n",
    "JNJ = JNJ.drop('Adj Close', axis=1)\n",
    "symbol = 'PFE'\n",
    "PFE = yf.download(symbol, start=start_date, end=end_date)\n",
    "PFE = PFE.drop('Adj Close', axis=1)\n",
    "symbol = 'CVS'\n",
    "CVS = yf.download(symbol, start=start_date, end=end_date)\n",
    "CVS = CVS.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'KO'\n",
    "KO = yf.download(symbol, start=start_date, end=end_date)\n",
    "KO = KO.drop('Adj Close', axis=1)\n",
    "symbol = 'PEP'\n",
    "PEP = yf.download(symbol, start=start_date, end=end_date)\n",
    "PEP = PEP.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'NKE'\n",
    "NKE = yf.download(symbol, start=start_date, end=end_date)\n",
    "NKE = NKE.drop('Adj Close', axis=1)\n",
    "symbol = 'MCD'\n",
    "MCD = yf.download(symbol, start=start_date, end=end_date)\n",
    "MCD = MCD.drop('Adj Close', axis=1)\n",
    "symbol = 'SBUX'\n",
    "SBUX = yf.download(symbol, start=start_date, end=end_date)\n",
    "SBUX = SBUX.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'VZ'\n",
    "VZ = yf.download(symbol, start=start_date, end=end_date)\n",
    "VZ = VZ.drop('Adj Close', axis=1)\n",
    "symbol = 'T'\n",
    "T = yf.download(symbol, start=start_date, end=end_date)\n",
    "T = T.drop('Adj Close', axis=1)\n",
    "symbol = 'FOX'\n",
    "FOX = yf.download(symbol, start=start_date, end=end_date)\n",
    "FOX = FOX.drop('Adj Close', axis=1)\n",
    "symbol = 'WBD'\n",
    "WBD = yf.download(symbol, start=start_date, end=end_date)\n",
    "WBD = WBD.drop('Adj Close', axis=1)\n",
    "symbol = 'DIS'\n",
    "DIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "DIS = DIS.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'UPS'\n",
    "UPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "UPS = UPS.drop('Adj Close', axis=1)\n",
    "symbol = 'FDX'\n",
    "FDX = yf.download(symbol, start=start_date, end=end_date)\n",
    "FDX = FDX.drop('Adj Close', axis=1)\n",
    "symbol = 'DAL'\n",
    "DAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "DAL = DAL.drop('Adj Close', axis=1)\n",
    "symbol = 'AAL'\n",
    "AAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "AAL = AAL.drop('Adj Close', axis=1)\n",
    "symbol = 'XOM'\n",
    "XOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "XOM = XOM.drop('Adj Close', axis=1)\n",
    "symbol = 'CVX'\n",
    "CVX = yf.download(symbol, start=start_date, end=end_date)\n",
    "CVX = CVX.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'BAC'\n",
    "BAC = yf.download(symbol, start=start_date, end=end_date)\n",
    "BAC = BAC.drop('Adj Close', axis=1)\n",
    "symbol = 'JPM'\n",
    "JPM = yf.download(symbol, start=start_date, end=end_date)\n",
    "JPM = JPM.drop('Adj Close', axis=1)\n",
    "symbol = 'MA'\n",
    "MA = yf.download(symbol, start=start_date, end=end_date)\n",
    "MA = MA.drop('Adj Close', axis=1)\n",
    "symbol = 'V'\n",
    "V = yf.download(symbol, start=start_date, end=end_date)\n",
    "V = V.drop('Adj Close', axis=1)\n",
    "symbol = 'SPG'\n",
    "SPG = yf.download(symbol, start=start_date, end=end_date)\n",
    "SPG = SPG.drop('Adj Close', axis=1)\n",
    "symbol = 'VNO'\n",
    "VNO = yf.download(symbol, start=start_date, end=end_date)\n",
    "VNO = VNO.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol = 'MMM'\n",
    "MMM = yf.download(symbol, start=start_date, end=end_date)\n",
    "MMM = MMM.drop('Adj Close', axis=1)\n",
    "symbol = 'GE'\n",
    "GE = yf.download(symbol, start=start_date, end=end_date)\n",
    "GE = GE.drop('Adj Close', axis=1)\n",
    "symbol = 'F'\n",
    "F = yf.download(symbol, start=start_date, end=end_date)\n",
    "F = F.drop('Adj Close', axis=1)\n",
    "symbol = 'GM'\n",
    "GM = yf.download(symbol, start=start_date, end=end_date)\n",
    "GM = GM.drop('Adj Close', axis=1)\n",
    "symbol = 'HON'\n",
    "HON = yf.download(symbol, start=start_date, end=end_date)\n",
    "HON = HON.drop('Adj Close', axis=1)\n",
    "symbol = 'LMT'\n",
    "LMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "LMT = LMT.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# Futures\n",
    "\n",
    "\n",
    "\n",
    "symbol = 'ES=F'\n",
    "ESF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ESF = ESF.drop('Adj Close', axis=1)\n",
    "symbol = 'YM=F'\n",
    "YMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "YMF = YMF.drop('Adj Close', axis=1)\n",
    "symbol = 'NQ=F'\n",
    "NQF = yf.download(symbol, start=start_date, end=end_date)\n",
    "NQF = NQF.drop('Adj Close', axis=1)\n",
    "symbol = 'RTY=F'\n",
    "RTYF = yf.download(symbol, start=start_date, end=end_date)\n",
    "RTYF = RTYF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZB=F'\n",
    "ZBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZBF = ZBF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZN=F'\n",
    "ZNF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZNF = ZNF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZF=F'\n",
    "ZFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZFF = ZFF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZT=F'\n",
    "ZTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZTF = ZTF.drop('Adj Close', axis=1)\n",
    "symbol = 'GC=F'\n",
    "GCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "GCF = GCF.drop('Adj Close', axis=1)\n",
    "symbol = 'HG=F'\n",
    "HGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "HGF = HGF.drop('Adj Close', axis=1)\n",
    "symbol = 'SI=F'\n",
    "SIF = yf.download(symbol, start=start_date, end=end_date)\n",
    "SIF = SIF.drop('Adj Close', axis=1)\n",
    "symbol = 'PL=F'\n",
    "PLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "PLF = PLF.drop('Adj Close', axis=1)\n",
    "CLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "CLF = CLF.drop('Adj Close', axis=1)\n",
    "symbol = 'NG=F'\n",
    "NGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "NGF = NGF.drop('Adj Close', axis=1)\n",
    "symbol = 'BZ=F'\n",
    "BZF = yf.download(symbol, start=start_date, end=end_date)\n",
    "BZF = BZF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZC=F'\n",
    "ZCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZCF = ZCF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZO=F'\n",
    "ZOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZOF = ZOF.drop('Adj Close', axis=1)\n",
    "symbol = 'KE=F'\n",
    "KEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "KEF = KEF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZR=F'\n",
    "ZRF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZRF = ZRF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZM=F'\n",
    "ZMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZMF = ZMF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZL=F'\n",
    "ZLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZLF = ZLF.drop('Adj Close', axis=1)\n",
    "symbol = 'ZS=F'\n",
    "ZSF = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZSF = ZSF.drop('Adj Close', axis=1)\n",
    "symbol = 'GF=F'\n",
    "GFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "GFF = GFF.drop('Adj Close', axis=1)\n",
    "symbol = 'HE=F'\n",
    "HEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "HEF = HEF.drop('Adj Close', axis=1)\n",
    "symbol = 'HO=F'\n",
    "HOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "HOF = HOF.drop('Adj Close', axis=1)\n",
    "symbol = 'LE=F'\n",
    "LFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "LFF = LFF.drop('Adj Close', axis=1)\n",
    "symbol = 'CC=F'\n",
    "CCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "CCF = CCF.drop('Adj Close', axis=1)\n",
    "symbol = 'KC=F'\n",
    "KCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "KCF = KCF.drop('Adj Close', axis=1)\n",
    "symbol = 'CT=F'\n",
    "CTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "CTF = CTF.drop('Adj Close', axis=1)\n",
    "symbol = 'OJ=F'\n",
    "OJF = yf.download(symbol, start=start_date, end=end_date)\n",
    "OJF = OJF.drop('Adj Close', axis=1)\n",
    "symbol = 'SB=F'\n",
    "SBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "SBF = SBF.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# EFTs\n",
    "\n",
    "symbol = 'KBA'\n",
    "KBA = yf.download(symbol, start=start_date, end=end_date)\n",
    "KBA = KBA.drop('Adj Close', axis=1)\n",
    "symbol = 'CHIQ'\n",
    "CHIQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIQ = CHIQ.drop('Adj Close', axis=1)\n",
    "symbol = 'CNTX'\n",
    "CNTX = yf.download(symbol, start=start_date, end=end_date)\n",
    "CNTX = CNTX.drop('Adj Close', axis=1)\n",
    "symbol = 'CHIS'\n",
    "CHIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIS = CHIS.drop('Adj Close', axis=1)\n",
    "symbol = 'CNYA'\n",
    "CNYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "CNYA = CNYA.drop('Adj Close', axis=1)\n",
    "symbol = 'ASHX'\n",
    "ASHX = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASHX = ASHX.drop('Adj Close', axis=1)\n",
    "symbol = 'KFYP'\n",
    "KFYP = yf.download(symbol, start=start_date, end=end_date)\n",
    "KFYP = KFYP.drop('Adj Close', axis=1)\n",
    "symbol = 'KGRN'\n",
    "KGRN = yf.download(symbol, start=start_date, end=end_date)\n",
    "KGRN = KGRN.drop('Adj Close', axis=1)\n",
    "symbol = 'THD'\n",
    "THD = yf.download(symbol, start=start_date, end=end_date)\n",
    "THD = THD.drop('Adj Close', axis=1)\n",
    "symbol = 'BBAX'\n",
    "BBAX = yf.download(symbol, start=start_date, end=end_date)\n",
    "BBAX = BBAX.drop('Adj Close', axis=1)\n",
    "symbol = 'FEMS'\n",
    "FEMS = yf.download(symbol, start=start_date, end=end_date)\n",
    "FEMS = FEMS.drop('Adj Close', axis=1)\n",
    "symbol = 'EZA'\n",
    "EZA = yf.download(symbol, start=start_date, end=end_date)\n",
    "EZA = EZA.drop('Adj Close', axis=1)\n",
    "symbol = 'XSD'\n",
    "XSD = yf.download(symbol, start=start_date, end=end_date)\n",
    "XSD = XSD.drop('Adj Close', axis=1)\n",
    "symbol = 'EYLD'\n",
    "EYLD = yf.download(symbol, start=start_date, end=end_date)\n",
    "EYLD = EYLD.drop('Adj Close', axis=1)\n",
    "symbol = 'FNDE'\n",
    "FNDE = yf.download(symbol, start=start_date, end=end_date)\n",
    "FNDE = FNDE.drop('Adj Close', axis=1)\n",
    "symbol = 'SPEM'\n",
    "SPEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "SPEM = SPEM.drop('Adj Close', axis=1)\n",
    "symbol = 'DXJS'\n",
    "DXJS = yf.download(symbol, start=start_date, end=end_date)\n",
    "DXJS = DXJS.drop('Adj Close', axis=1)\n",
    "symbol = 'KURE'\n",
    "KURE = yf.download(symbol, start=start_date, end=end_date)\n",
    "KURE = KURE.drop('Adj Close', axis=1)\n",
    "symbol = 'EWX'\n",
    "EWX = yf.download(symbol, start=start_date, end=end_date)\n",
    "EWX = EWX.drop('Adj Close', axis=1)\n",
    "symbol = 'FLJH'\n",
    "FLJH = yf.download(symbol, start=start_date, end=end_date)\n",
    "FLJH = FLJH.drop('Adj Close', axis=1)\n",
    "symbol = 'CQQQ'\n",
    "CQQQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "CQQQ = CQQQ.drop('Adj Close', axis=1)\n",
    "symbol = 'CHIE'\n",
    "CHIE = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIE = CHIE.drop('Adj Close', axis=1)\n",
    "symbol = 'MFEM'\n",
    "MFEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "MFEM = MFEM.drop('Adj Close', axis=1)\n",
    "symbol = 'DGS'\n",
    "DGS = yf.download(symbol, start=start_date, end=end_date)\n",
    "DGS = DGS.drop('Adj Close', axis=1)\n",
    "symbol = 'HEEM'\n",
    "HEEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "HEEM = HEEM.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# World Composite Index\n",
    "\n",
    "\n",
    "symbol = '^HSI'\n",
    "HSI = yf.download(symbol, start=start_date, end=end_date)\n",
    "HSI = HSI.drop('Adj Close', axis=1)\n",
    "symbol = '000001.SS'\n",
    "SSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "SSE = SSE.drop('Adj Close', axis=1)\n",
    "symbol = '^N225'\n",
    "N225 = yf.download(symbol, start=start_date, end=end_date)\n",
    "N225 = N225.drop('Adj Close', axis=1)\n",
    "symbol = '^KS11'\n",
    "KS11 = yf.download(symbol, start=start_date, end=end_date)\n",
    "KS11 = KS11.drop('Adj Close', axis=1)\n",
    "symbol = '^BSESN'\n",
    "BSESN = yf.download(symbol, start=start_date, end=end_date)\n",
    "BSESN = BSESN.drop('Adj Close', axis=1)\n",
    "symbol = '^MXX'\n",
    "MXX = yf.download(symbol, start=start_date, end=end_date)\n",
    "MXX = MXX.drop('Adj Close', axis=1)\n",
    "symbol = '^TNX'\n",
    "TNX = yf.download(symbol, start=start_date, end=end_date)\n",
    "TNX = TNX.drop('Adj Close', axis=1)\n",
    "symbol = '^VIX'\n",
    "VIX = yf.download(symbol, start=start_date, end=end_date)\n",
    "VIX = VIX.drop('Adj Close', axis=1)\n",
    "symbol = '^BVSP'\n",
    "BVSP = yf.download(symbol, start=start_date, end=end_date)\n",
    "BVSP = BVSP.drop('Adj Close', axis=1)\n",
    "symbol = '^IXIC'\n",
    "IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "IXIC = IXIC.drop('Adj Close', axis=1)\n",
    "symbol = '^GSPTSE'\n",
    "GSPTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "GSPTSE = GSPTSE.drop('Adj Close', axis=1)\n",
    "symbol = '^DJI'\n",
    "DJI = yf.download(symbol, start=start_date, end=end_date)\n",
    "DJI = DJI.drop('Adj Close', axis=1)\n",
    "symbol = '^FCHI'\n",
    "FCHI = yf.download(symbol, start=start_date, end=end_date)\n",
    "FCHI = FCHI.drop('Adj Close', axis=1)\n",
    "symbol = '^GDAXI'\n",
    "GDAXI = yf.download(symbol, start=start_date, end=end_date)\n",
    "GDAXI = GDAXI.drop('Adj Close', axis=1)\n",
    "symbol = '^FTSE'\n",
    "FTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "FTSE = FTSE.drop('Adj Close', axis=1)\n",
    "symbol = '^IBEX'\n",
    "IBEX = yf.download(symbol, start=start_date, end=end_date)\n",
    "IBEX = IBEX.drop('Adj Close', axis=1)\n",
    "symbol = '^N100'\n",
    "N100 = yf.download(symbol, start=start_date, end=end_date)\n",
    "N100 = N100.drop('Adj Close', axis=1)\n",
    "symbol = '^GSPC'\n",
    "GSPC = yf.download(symbol, start=start_date, end=end_date)\n",
    "GSPC = GSPC.drop('Adj Close', axis=1)\n",
    "symbol = '^RUT'\n",
    "RUT = yf.download(symbol, start=start_date, end=end_date)\n",
    "RUT = RUT.drop('Adj Close', axis=1)\n",
    "symbol = '^NYA'\n",
    "NYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "NYA = NYA.drop('Adj Close', axis=1)\n",
    "symbol = '^STI'\n",
    "STI = yf.download(symbol, start=start_date, end=end_date)\n",
    "STI = STI.drop('Adj Close', axis=1)\n",
    "symbol = '^AXJO'\n",
    "AXJO = yf.download(symbol, start=start_date, end=end_date)\n",
    "AXJO = AXJO.drop('Adj Close', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chip Maker\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "start_date  = '2023-01-01'\n",
    "end_date  = '2023-09-26'\n",
    "symbol  = 'TSM'\n",
    "TSM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "TSM1  = TSM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'INTC'\n",
    "INTC1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "INTC1  = INTC1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ASML'\n",
    "ASML1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASML1  = ASML1.drop('Adj Close', axis=1)\n",
    "symbol  = 'MU'\n",
    "MU1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MU1  = MU1.drop('Adj Close', axis=1)\n",
    "symbol  = 'NVDA'\n",
    "NVDA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NVDA1  = NVDA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'AMD'\n",
    "AMD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMD1  = AMD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'QCOM'\n",
    "QCOM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "QCOM1  = QCOM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'SNPS'\n",
    "SNPS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SNPS1  = SNPS1.drop('Adj Close', axis=1)\n",
    "symbol  = 'MRVL'\n",
    "MRVL1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MRVL1  = MRVL1.drop('Adj Close', axis=1)\n",
    "symbol  = '^IXIC'\n",
    "IXIC1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "IXIC1  = IXIC1.drop('Adj Close', axis=1)\n",
    "\n",
    "# information technology\n",
    "\n",
    "symbol  = 'AAPL'\n",
    "AAPL1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "AAPL1  = AAPL1.drop('Adj Close', axis=1)\n",
    "symbol  = 'MSFT'\n",
    "MSFT1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MSFT1  = MSFT1.drop('Adj Close', axis=1)\n",
    "symbol  = 'TSLA'\n",
    "TSLA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "TSLA1  = TSLA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'GOOGL'\n",
    "GOOGL1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GOOGL1  = GOOGL1.drop('Adj Close', axis=1)\n",
    "symbol  = 'GOOG'\n",
    "GOOG1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GOOG1  = GOOG1.drop('Adj Close', axis=1)\n",
    "symbol  = 'AMZN'\n",
    "AMZN1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMZN1  = AMZN1.drop('Adj Close', axis=1)\n",
    "symbol  = 'META'\n",
    "META1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "META1  = META1.drop('Adj Close', axis=1)\n",
    "symbol  = 'AMD'\n",
    "AMD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "AMD1  = AMD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ASML'\n",
    "ASML1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASML1  = ASML1.drop('Adj Close', axis=1)\n",
    "symbol  = 'NVDA'\n",
    "NVDA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NVDA1  = NVDA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'IBM'\n",
    "IBM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "IBM1  = IBM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'NFLX'\n",
    "NFLX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NFLX1  = NFLX1.drop('Adj Close', axis=1)\n",
    "\n",
    "# Consumer\n",
    "\n",
    "\n",
    "symbol  = 'WMT'\n",
    "WMT1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "WMT1  = WMT1.drop('Adj Close', axis=1)\n",
    "symbol  = 'TGT'\n",
    "TGT1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "TGT1  = TGT1.drop('Adj Close', axis=1)\n",
    "symbol  = 'COST'\n",
    "COST1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "COST1  = COST1.drop('Adj Close', axis=1)\n",
    "symbol  = 'HD'\n",
    "HD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HD1  = HD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'LOW'\n",
    "LOW1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "LOW1  = LOW1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'PG'\n",
    "PG1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "PG1  = PG1.drop('Adj Close', axis=1)\n",
    "symbol  = 'JNJ'\n",
    "JNJ1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "JNJ1  = JNJ1.drop('Adj Close', axis=1)\n",
    "symbol  = 'PFE'\n",
    "PFE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "PFE1  = PFE1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CVS'\n",
    "CVS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CVS1  = CVS1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'KO'\n",
    "KO1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KO1  = KO1.drop('Adj Close', axis=1)\n",
    "symbol  = 'PEP'\n",
    "PEP1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "PEP1  = PEP1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'NKE'\n",
    "NKE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NKE1  = NKE1.drop('Adj Close', axis=1)\n",
    "symbol  = 'MCD'\n",
    "MCD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MCD1  = MCD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'SBUX'\n",
    "SBUX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SBUX1  = SBUX1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'VZ'\n",
    "VZ1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "VZ1  = VZ1.drop('Adj Close', axis=1)\n",
    "symbol  = 'T'\n",
    "T1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "T1  = T1.drop('Adj Close', axis=1)\n",
    "symbol  = 'FOX'\n",
    "FOX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FOX1  = FOX1.drop('Adj Close', axis=1)\n",
    "symbol  = 'WBD'\n",
    "WBD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "WBD1  = WBD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'DIS'\n",
    "DIS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "DIS1  = DIS1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'UPS'\n",
    "UPS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "UPS1  = UPS1.drop('Adj Close', axis=1)\n",
    "symbol  = 'FDX'\n",
    "FDX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FDX1  = FDX1.drop('Adj Close', axis=1)\n",
    "symbol  = 'DAL'\n",
    "DAL1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "DAL1  = DAL1.drop('Adj Close', axis=1)\n",
    "symbol  = 'AAL'\n",
    "AAL1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "AAL1  = AAL1.drop('Adj Close', axis=1)\n",
    "symbol  = 'XOM'\n",
    "XOM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "XOM1  = XOM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CVX'\n",
    "CVX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CVX1  = CVX1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'BAC'\n",
    "BAC1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "BAC1  = BAC1.drop('Adj Close', axis=1)\n",
    "symbol  = 'JPM'\n",
    "JPM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "JPM1  = JPM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'MA'\n",
    "MA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MA1  = MA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'V'\n",
    "V1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "V1  = V1.drop('Adj Close', axis=1)\n",
    "symbol  = 'SPG'\n",
    "SPG1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SPG1  = SPG1.drop('Adj Close', axis=1)\n",
    "symbol  = 'VNO'\n",
    "VNO1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "VNO1  = VNO1.drop('Adj Close', axis=1)\n",
    "\n",
    "symbol  = 'MMM'\n",
    "MMM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MMM1  = MMM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'GE'\n",
    "GE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GE1  = GE1.drop('Adj Close', axis=1)\n",
    "symbol  = 'F'\n",
    "F1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "F1  = F1.drop('Adj Close', axis=1)\n",
    "symbol  = 'GM'\n",
    "GM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GM1  = GM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'HON'\n",
    "HON1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HON1  = HON1.drop('Adj Close', axis=1)\n",
    "symbol  = 'LMT'\n",
    "LMT1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "LMT1  = LMT1.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# Futures\n",
    "\n",
    "\n",
    "\n",
    "symbol  = 'ES=F'\n",
    "ESF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ESF1  = ESF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'YM=F'\n",
    "YMF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "YMF1  = YMF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'NQ=F'\n",
    "NQF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NQF1  = NQF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'RTY=F'\n",
    "RTYF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "RTYF1  = RTYF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZB=F'\n",
    "ZBF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZBF1  = ZBF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZN=F'\n",
    "ZNF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZNF1  = ZNF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZF=F'\n",
    "ZFF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZFF1  = ZFF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZT=F'\n",
    "ZTF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZTF1  = ZTF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'GC=F'\n",
    "GCF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GCF1  = GCF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'HG=F'\n",
    "HGF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HGF1  = HGF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'SI=F'\n",
    "SIF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SIF1  = SIF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'PL=F'\n",
    "PLF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "PLF1  = PLF1.drop('Adj Close', axis=1)\n",
    "CLF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CLF1  = CLF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'NG=F'\n",
    "NGF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NGF1  = NGF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'BZ=F'\n",
    "BZF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "BZF1  = BZF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZC=F'\n",
    "ZCF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZCF1  = ZCF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZO=F'\n",
    "ZOF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZOF1  = ZOF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'KE=F'\n",
    "KEF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KEF1  = KEF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZR=F'\n",
    "ZRF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZRF1  = ZRF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZM=F'\n",
    "ZMF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZMF1  = ZMF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZL=F'\n",
    "ZLF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZLF1  = ZLF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ZS=F'\n",
    "ZSF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ZSF1  = ZSF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'GF=F'\n",
    "GFF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GFF1  = GFF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'HE=F'\n",
    "HEF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HEF1  = HEF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'HO=F'\n",
    "HOF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HOF1  = HOF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'LE=F'\n",
    "LFF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "LFF1  = LFF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CC=F'\n",
    "CCF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CCF1  = CCF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'KC=F'\n",
    "KCF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KCF1  = KCF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CT=F'\n",
    "CTF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CTF1  = CTF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'OJ=F'\n",
    "OJF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "OJF1  = OJF1.drop('Adj Close', axis=1)\n",
    "symbol  = 'SB=F'\n",
    "SBF1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SBF1  = SBF1.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# EFTs\n",
    "\n",
    "symbol  = 'KBA'\n",
    "KBA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KBA1  = KBA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CHIQ'\n",
    "CHIQ1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIQ1  = CHIQ1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CNTX'\n",
    "CNTX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CNTX1  = CNTX1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CHIS'\n",
    "CHIS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIS1  = CHIS1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CNYA'\n",
    "CNYA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CNYA1  = CNYA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'ASHX'\n",
    "ASHX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "ASHX1  = ASHX1.drop('Adj Close', axis=1)\n",
    "symbol  = 'KFYP'\n",
    "KFYP1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KFYP1  = KFYP1.drop('Adj Close', axis=1)\n",
    "symbol  = 'KGRN'\n",
    "KGRN1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KGRN1  = KGRN1.drop('Adj Close', axis=1)\n",
    "symbol  = 'THD'\n",
    "THD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "THD1  = THD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'BBAX'\n",
    "BBAX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "BBAX1  = BBAX1.drop('Adj Close', axis=1)\n",
    "symbol  = 'FEMS'\n",
    "FEMS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FEMS1  = FEMS1.drop('Adj Close', axis=1)\n",
    "symbol  = 'EZA'\n",
    "EZA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "EZA1  = EZA1.drop('Adj Close', axis=1)\n",
    "symbol  = 'XSD'\n",
    "XSD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "XSD1  = XSD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'EYLD'\n",
    "EYLD1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "EYLD1  = EYLD1.drop('Adj Close', axis=1)\n",
    "symbol  = 'FNDE'\n",
    "FNDE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FNDE1  = FNDE1.drop('Adj Close', axis=1)\n",
    "symbol  = 'SPEM'\n",
    "SPEM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SPEM1  = SPEM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'DXJS'\n",
    "DXJS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "DXJS1  = DXJS1.drop('Adj Close', axis=1)\n",
    "symbol  = 'KURE'\n",
    "KURE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KURE1  = KURE1.drop('Adj Close', axis=1)\n",
    "symbol  = 'EWX'\n",
    "EWX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "EWX1  = EWX1.drop('Adj Close', axis=1)\n",
    "symbol  = 'FLJH'\n",
    "FLJH1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FLJH1  = FLJH1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CQQQ'\n",
    "CQQQ1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CQQQ1  = CQQQ1.drop('Adj Close', axis=1)\n",
    "symbol  = 'CHIE'\n",
    "CHIE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "CHIE1  = CHIE1.drop('Adj Close', axis=1)\n",
    "symbol  = 'MFEM'\n",
    "MFEM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MFEM1  = MFEM1.drop('Adj Close', axis=1)\n",
    "symbol  = 'DGS'\n",
    "DGS1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "DGS1  = DGS1.drop('Adj Close', axis=1)\n",
    "symbol  = 'HEEM'\n",
    "HEEM1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HEEM1  = HEEM1.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# World Composite Index\n",
    "\n",
    "\n",
    "symbol  = '^HSI'\n",
    "HSI1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "HSI1  = HSI1.drop('Adj Close', axis=1)\n",
    "symbol  = '000001.SS'\n",
    "SSE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "SSE1  = SSE1.drop('Adj Close', axis=1)\n",
    "symbol  = '^N225'\n",
    "N2251  = yf.download(symbol, start=start_date, end=end_date)\n",
    "N2251  = N2251.drop('Adj Close', axis=1)\n",
    "symbol  = '^KS11'\n",
    "KS111  = yf.download(symbol, start=start_date, end=end_date)\n",
    "KS111  = KS111.drop('Adj Close', axis=1)\n",
    "symbol  = '^BSESN'\n",
    "BSESN1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "BSESN1  = BSESN1.drop('Adj Close', axis=1)\n",
    "symbol  = '^MXX'\n",
    "MXX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "MXX1  = MXX1.drop('Adj Close', axis=1)\n",
    "symbol  = '^TNX'\n",
    "TNX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "TNX1  = TNX1.drop('Adj Close', axis=1)\n",
    "symbol  = '^VIX'\n",
    "VIX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "VIX1  = VIX1.drop('Adj Close', axis=1)\n",
    "symbol  = '^BVSP'\n",
    "BVSP1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "BVSP1  = BVSP1.drop('Adj Close', axis=1)\n",
    "symbol  = '^IXIC'\n",
    "IXIC1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "IXIC1  = IXIC1.drop('Adj Close', axis=1)\n",
    "symbol  = '^GSPTSE'\n",
    "GSPTSE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GSPTSE1  = GSPTSE1.drop('Adj Close', axis=1)\n",
    "symbol  = '^DJI'\n",
    "DJI1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "DJI1  = DJI1.drop('Adj Close', axis=1)\n",
    "symbol  = '^FCHI'\n",
    "FCHI1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FCHI1  = FCHI1.drop('Adj Close', axis=1)\n",
    "symbol  = '^GDAXI'\n",
    "GDAXI1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GDAXI1  = GDAXI1.drop('Adj Close', axis=1)\n",
    "symbol  = '^FTSE'\n",
    "FTSE1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "FTSE1  = FTSE1.drop('Adj Close', axis=1)\n",
    "symbol  = '^IBEX'\n",
    "IBEX1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "IBEX1  = IBEX1.drop('Adj Close', axis=1)\n",
    "symbol  = '^N100'\n",
    "N1001  = yf.download(symbol, start=start_date, end=end_date)\n",
    "N1001  = N1001.drop('Adj Close', axis=1)\n",
    "symbol  = '^GSPC'\n",
    "GSPC1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "GSPC1  = GSPC1.drop('Adj Close', axis=1)\n",
    "symbol  = '^RUT'\n",
    "RUT1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "RUT1  = RUT1.drop('Adj Close', axis=1)\n",
    "symbol  = '^NYA'\n",
    "NYA1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "NYA1  = NYA1.drop('Adj Close', axis=1)\n",
    "symbol  = '^STI'\n",
    "STI1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "STI1  = STI1.drop('Adj Close', axis=1)\n",
    "symbol  = '^AXJO'\n",
    "AXJO1  = yf.download(symbol, start=start_date, end=end_date)\n",
    "AXJO1  = AXJO1.drop('Adj Close', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chip Maker\n",
    "\n",
    "# import yfinance as yf\n",
    "\n",
    "# start_date = '2018-01-01'\n",
    "# end_date = '2023-09-26'\n",
    "# symbol = 'TSM'\n",
    "# TSM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TSM = TSM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'INTC'\n",
    "# INTC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# INTC = INTC.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ASML'\n",
    "# ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ASML = ASML.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'MU'\n",
    "# MU = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MU = MU.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'NVDA'\n",
    "# NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NVDA = NVDA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'AMD'\n",
    "# AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AMD = AMD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'QCOM'\n",
    "# QCOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# QCOM = QCOM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'SNPS'\n",
    "# SNPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SNPS = SNPS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'MRVL'\n",
    "# MRVL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MRVL = MRVL.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^IXIC'\n",
    "# IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IXIC = IXIC.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# # information technology\n",
    "\n",
    "# symbol = 'AAPL'\n",
    "# AAPL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AAPL = AAPL.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'MSFT'\n",
    "# MSFT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MSFT = MSFT.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'TSLA'\n",
    "# TSLA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TSLA = TSLA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'GOOGL'\n",
    "# GOOGL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GOOGL = GOOGL.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'GOOG'\n",
    "# GOOG = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GOOG = GOOG.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'AMZN'\n",
    "# AMZN = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AMZN = AMZN.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'META'\n",
    "# META = yf.download(symbol, start=start_date, end=end_date)\n",
    "# META = META.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'AMD'\n",
    "# AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AMD = AMD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ASML'\n",
    "# ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ASML = ASML.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'NVDA'\n",
    "# NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NVDA = NVDA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'IBM'\n",
    "# IBM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IBM = IBM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'NFLX'\n",
    "# NFLX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NFLX = NFLX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# # Consumer\n",
    "\n",
    "\n",
    "# symbol = 'WMT'\n",
    "# WMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# WMT = WMT.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'TGT'\n",
    "# TGT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TGT = TGT.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'COST'\n",
    "# COST = yf.download(symbol, start=start_date, end=end_date)\n",
    "# COST = COST.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'HD'\n",
    "# HD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HD = HD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'LOW'\n",
    "# LOW = yf.download(symbol, start=start_date, end=end_date)\n",
    "# LOW = LOW.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'PG'\n",
    "# PG = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PG = PG.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'JNJ'\n",
    "# JNJ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# JNJ = JNJ.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'PFE'\n",
    "# PFE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PFE = PFE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CVS'\n",
    "# CVS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CVS = CVS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'KO'\n",
    "# KO = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KO = KO.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'PEP'\n",
    "# PEP = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PEP = PEP.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'NKE'\n",
    "# NKE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NKE = NKE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'MCD'\n",
    "# MCD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MCD = MCD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'SBUX'\n",
    "# SBUX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SBUX = SBUX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'VZ'\n",
    "# VZ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# VZ = VZ.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'T'\n",
    "# T = yf.download(symbol, start=start_date, end=end_date)\n",
    "# T = T.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'FOX'\n",
    "# FOX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FOX = FOX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'WBD'\n",
    "# WBD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# WBD = WBD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'DIS'\n",
    "# DIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DIS = DIS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'UPS'\n",
    "# UPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# UPS = UPS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'FDX'\n",
    "# FDX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FDX = FDX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'DAL'\n",
    "# DAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DAL = DAL.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'AAL'\n",
    "# AAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AAL = AAL.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'XOM'\n",
    "# XOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# XOM = XOM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CVX'\n",
    "# CVX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CVX = CVX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'BAC'\n",
    "# BAC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BAC = BAC.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'JPM'\n",
    "# JPM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# JPM = JPM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'MA'\n",
    "# MA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MA = MA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'V'\n",
    "# V = yf.download(symbol, start=start_date, end=end_date)\n",
    "# V = V.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'SPG'\n",
    "# SPG = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SPG = SPG.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'VNO'\n",
    "# VNO = yf.download(symbol, start=start_date, end=end_date)\n",
    "# VNO = VNO.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "# symbol = 'MMM'\n",
    "# MMM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MMM = MMM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'GE'\n",
    "# GE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GE = GE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'F'\n",
    "# F = yf.download(symbol, start=start_date, end=end_date)\n",
    "# F = F.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'GM'\n",
    "# GM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GM = GM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'HON'\n",
    "# HON = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HON = HON.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'LMT'\n",
    "# LMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# LMT = LMT.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "\n",
    "# # Futures\n",
    "\n",
    "\n",
    "\n",
    "# symbol = 'ES=F'\n",
    "# ESF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ESF = ESF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'YM=F'\n",
    "# YMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# YMF = YMF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'NQ=F'\n",
    "# NQF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NQF = NQF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'RTY=F'\n",
    "# RTYF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# RTYF = RTYF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZB=F'\n",
    "# ZBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZBF = ZBF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZN=F'\n",
    "# ZNF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZNF = ZNF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZF=F'\n",
    "# ZFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZFF = ZFF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZT=F'\n",
    "# ZTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZTF = ZTF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'GC=F'\n",
    "# GCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GCF = GCF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'HG=F'\n",
    "# HGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HGF = HGF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'SI=F'\n",
    "# SIF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SIF = SIF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'PL=F'\n",
    "# PLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PLF = PLF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# CLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CLF = CLF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'NG=F'\n",
    "# NGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NGF = NGF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'BZ=F'\n",
    "# BZF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BZF = BZF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZC=F'\n",
    "# ZCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZCF = ZCF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZO=F'\n",
    "# ZOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZOF = ZOF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'KE=F'\n",
    "# KEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KEF = KEF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZR=F'\n",
    "# ZRF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZRF = ZRF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZM=F'\n",
    "# ZMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZMF = ZMF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZL=F'\n",
    "# ZLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZLF = ZLF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ZS=F'\n",
    "# ZSF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZSF = ZSF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'GF=F'\n",
    "# GFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GFF = GFF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'HE=F'\n",
    "# HEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HEF = HEF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'HO=F'\n",
    "# HOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HOF = HOF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'LE=F'\n",
    "# LFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# LFF = LFF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CC=F'\n",
    "# CCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CCF = CCF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'KC=F'\n",
    "# KCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KCF = KCF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CT=F'\n",
    "# CTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CTF = CTF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'OJ=F'\n",
    "# OJF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# OJF = OJF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'SB=F'\n",
    "# SBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SBF = SBF.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "\n",
    "# # EFTs\n",
    "\n",
    "# symbol = 'KBA'\n",
    "# KBA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KBA = KBA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CHIQ'\n",
    "# CHIQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CHIQ = CHIQ.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CNTX'\n",
    "# CNTX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CNTX = CNTX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CHIS'\n",
    "# CHIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CHIS = CHIS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CNYA'\n",
    "# CNYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CNYA = CNYA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'ASHX'\n",
    "# ASHX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ASHX = ASHX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'KFYP'\n",
    "# KFYP = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KFYP = KFYP.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'KGRN'\n",
    "# KGRN = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KGRN = KGRN.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'THD'\n",
    "# THD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# THD = THD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'BBAX'\n",
    "# BBAX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BBAX = BBAX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'FEMS'\n",
    "# FEMS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FEMS = FEMS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'EZA'\n",
    "# EZA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# EZA = EZA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'XSD'\n",
    "# XSD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# XSD = XSD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'EYLD'\n",
    "# EYLD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# EYLD = EYLD.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'FNDE'\n",
    "# FNDE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FNDE = FNDE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'SPEM'\n",
    "# SPEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SPEM = SPEM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'DXJS'\n",
    "# DXJS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DXJS = DXJS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'KURE'\n",
    "# KURE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KURE = KURE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'EWX'\n",
    "# EWX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# EWX = EWX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'FLJH'\n",
    "# FLJH = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FLJH = FLJH.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CQQQ'\n",
    "# CQQQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CQQQ = CQQQ.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'CHIE'\n",
    "# CHIE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CHIE = CHIE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'MFEM'\n",
    "# MFEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MFEM = MFEM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'DGS'\n",
    "# DGS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DGS = DGS.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = 'HEEM'\n",
    "# HEEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HEEM = HEEM.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n",
    "\n",
    "# # World Composite Index\n",
    "\n",
    "\n",
    "# symbol = '^HSI'\n",
    "# HSI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HSI = HSI.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '000001.SS'\n",
    "# SSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SSE = SSE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^N225'\n",
    "# N225 = yf.download(symbol, start=start_date, end=end_date)\n",
    "# N225 = N225.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^KS11'\n",
    "# KS11 = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KS11 = KS11.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^BSESN'\n",
    "# BSESN = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BSESN = BSESN.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^MXX'\n",
    "# MXX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MXX = MXX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^TNX'\n",
    "# TNX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TNX = TNX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^VIX'\n",
    "# VIX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# VIX = VIX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^BVSP'\n",
    "# BVSP = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BVSP = BVSP.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^IXIC'\n",
    "# IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IXIC = IXIC.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^GSPTSE'\n",
    "# GSPTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GSPTSE = GSPTSE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^DJI'\n",
    "# DJI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DJI = DJI.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^FCHI'\n",
    "# FCHI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FCHI = FCHI.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^GDAXI'\n",
    "# GDAXI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GDAXI = GDAXI.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^FTSE'\n",
    "# FTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FTSE = FTSE.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^IBEX'\n",
    "# IBEX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IBEX = IBEX.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^N100'\n",
    "# N100 = yf.download(symbol, start=start_date, end=end_date)\n",
    "# N100 = N100.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^GSPC'\n",
    "# GSPC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GSPC = GSPC.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^RUT'\n",
    "# RUT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# RUT = RUT.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^NYA'\n",
    "# NYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NYA = NYA.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^STI'\n",
    "# STI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# STI = STI.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "# symbol = '^AXJO'\n",
    "# AXJO = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AXJO = AXJO.drop(['Adj Close', 'Volume', 'Low', 'High'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Chip Maker\n",
    "\n",
    "# import yfinance as yf\n",
    "\n",
    "# start_date = '2013-01-01'\n",
    "# end_date = '2023-09-26'\n",
    "# symbol = 'TSM'\n",
    "# TSM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TSM = TSM.drop('Adj Close', axis=1)\n",
    "# symbol = 'INTC'\n",
    "# INTC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# INTC = INTC.drop('Adj Close', axis=1)\n",
    "# symbol = 'ASML'\n",
    "# ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ASML = ASML.drop('Adj Close', axis=1)\n",
    "# symbol = 'MU'\n",
    "# MU = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MU = MU.drop('Adj Close', axis=1)\n",
    "# symbol = 'NVDA'\n",
    "# NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NVDA = NVDA.drop('Adj Close', axis=1)\n",
    "# symbol = 'AMD'\n",
    "# AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AMD = AMD.drop('Adj Close', axis=1)\n",
    "# symbol = 'QCOM'\n",
    "# QCOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# QCOM = QCOM.drop('Adj Close', axis=1)\n",
    "# symbol = 'SNPS'\n",
    "# SNPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SNPS = SNPS.drop('Adj Close', axis=1)\n",
    "# symbol = 'MRVL'\n",
    "# MRVL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MRVL = MRVL.drop('Adj Close', axis=1)\n",
    "# symbol = '^IXIC'\n",
    "# IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IXIC = IXIC.drop('Adj Close', axis=1)\n",
    "\n",
    "# # information technology\n",
    "\n",
    "# symbol = 'AAPL'\n",
    "# AAPL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AAPL = AAPL.drop('Adj Close', axis=1)\n",
    "# symbol = 'MSFT'\n",
    "# MSFT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MSFT = MSFT.drop('Adj Close', axis=1)\n",
    "# symbol = 'TSLA'\n",
    "# TSLA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TSLA = TSLA.drop('Adj Close', axis=1)\n",
    "# symbol = 'GOOGL'\n",
    "# GOOGL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GOOGL = GOOGL.drop('Adj Close', axis=1)\n",
    "# symbol = 'GOOG'\n",
    "# GOOG = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GOOG = GOOG.drop('Adj Close', axis=1)\n",
    "# symbol = 'AMZN'\n",
    "# AMZN = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AMZN = AMZN.drop('Adj Close', axis=1)\n",
    "# symbol = 'META'\n",
    "# META = yf.download(symbol, start=start_date, end=end_date)\n",
    "# META = META.drop('Adj Close', axis=1)\n",
    "# symbol = 'AMD'\n",
    "# AMD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AMD = AMD.drop('Adj Close', axis=1)\n",
    "# symbol = 'ASML'\n",
    "# ASML = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ASML = ASML.drop('Adj Close', axis=1)\n",
    "# symbol = 'NVDA'\n",
    "# NVDA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NVDA = NVDA.drop('Adj Close', axis=1)\n",
    "# symbol = 'IBM'\n",
    "# IBM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IBM = IBM.drop('Adj Close', axis=1)\n",
    "# symbol = 'NFLX'\n",
    "# NFLX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NFLX = NFLX.drop('Adj Close', axis=1)\n",
    "\n",
    "# # Consumer\n",
    "\n",
    "\n",
    "# symbol = 'WMT'\n",
    "# WMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# WMT = WMT.drop('Adj Close', axis=1)\n",
    "# symbol = 'TGT'\n",
    "# TGT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TGT = TGT.drop('Adj Close', axis=1)\n",
    "# symbol = 'COST'\n",
    "# COST = yf.download(symbol, start=start_date, end=end_date)\n",
    "# COST = COST.drop('Adj Close', axis=1)\n",
    "# symbol = 'HD'\n",
    "# HD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HD = HD.drop('Adj Close', axis=1)\n",
    "# symbol = 'LOW'\n",
    "# LOW = yf.download(symbol, start=start_date, end=end_date)\n",
    "# LOW = LOW.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'PG'\n",
    "# PG = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PG = PG.drop('Adj Close', axis=1)\n",
    "# symbol = 'JNJ'\n",
    "# JNJ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# JNJ = JNJ.drop('Adj Close', axis=1)\n",
    "# symbol = 'PFE'\n",
    "# PFE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PFE = PFE.drop('Adj Close', axis=1)\n",
    "# symbol = 'CVS'\n",
    "# CVS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CVS = CVS.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'KO'\n",
    "# KO = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KO = KO.drop('Adj Close', axis=1)\n",
    "# symbol = 'PEP'\n",
    "# PEP = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PEP = PEP.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'NKE'\n",
    "# NKE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NKE = NKE.drop('Adj Close', axis=1)\n",
    "# symbol = 'MCD'\n",
    "# MCD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MCD = MCD.drop('Adj Close', axis=1)\n",
    "# symbol = 'SBUX'\n",
    "# SBUX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SBUX = SBUX.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'VZ'\n",
    "# VZ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# VZ = VZ.drop('Adj Close', axis=1)\n",
    "# symbol = 'T'\n",
    "# T = yf.download(symbol, start=start_date, end=end_date)\n",
    "# T = T.drop('Adj Close', axis=1)\n",
    "# symbol = 'FOX'\n",
    "# FOX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FOX = FOX.drop('Adj Close', axis=1)\n",
    "# symbol = 'WBD'\n",
    "# WBD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# WBD = WBD.drop('Adj Close', axis=1)\n",
    "# symbol = 'DIS'\n",
    "# DIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DIS = DIS.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'UPS'\n",
    "# UPS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# UPS = UPS.drop('Adj Close', axis=1)\n",
    "# symbol = 'FDX'\n",
    "# FDX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FDX = FDX.drop('Adj Close', axis=1)\n",
    "# symbol = 'DAL'\n",
    "# DAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DAL = DAL.drop('Adj Close', axis=1)\n",
    "# symbol = 'AAL'\n",
    "# AAL = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AAL = AAL.drop('Adj Close', axis=1)\n",
    "# symbol = 'XOM'\n",
    "# XOM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# XOM = XOM.drop('Adj Close', axis=1)\n",
    "# symbol = 'CVX'\n",
    "# CVX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CVX = CVX.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'BAC'\n",
    "# BAC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BAC = BAC.drop('Adj Close', axis=1)\n",
    "# symbol = 'JPM'\n",
    "# JPM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# JPM = JPM.drop('Adj Close', axis=1)\n",
    "# symbol = 'MA'\n",
    "# MA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MA = MA.drop('Adj Close', axis=1)\n",
    "# symbol = 'V'\n",
    "# V = yf.download(symbol, start=start_date, end=end_date)\n",
    "# V = V.drop('Adj Close', axis=1)\n",
    "# symbol = 'SPG'\n",
    "# SPG = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SPG = SPG.drop('Adj Close', axis=1)\n",
    "# symbol = 'VNO'\n",
    "# VNO = yf.download(symbol, start=start_date, end=end_date)\n",
    "# VNO = VNO.drop('Adj Close', axis=1)\n",
    "\n",
    "# symbol = 'MMM'\n",
    "# MMM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MMM = MMM.drop('Adj Close', axis=1)\n",
    "# symbol = 'GE'\n",
    "# GE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GE = GE.drop('Adj Close', axis=1)\n",
    "# symbol = 'F'\n",
    "# F = yf.download(symbol, start=start_date, end=end_date)\n",
    "# F = F.drop('Adj Close', axis=1)\n",
    "# symbol = 'GM'\n",
    "# GM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GM = GM.drop('Adj Close', axis=1)\n",
    "# symbol = 'HON'\n",
    "# HON = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HON = HON.drop('Adj Close', axis=1)\n",
    "# symbol = 'LMT'\n",
    "# LMT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# LMT = LMT.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# # Futures\n",
    "\n",
    "\n",
    "\n",
    "# symbol = 'ES=F'\n",
    "# ESF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ESF = ESF.drop('Adj Close', axis=1)\n",
    "# symbol = 'YM=F'\n",
    "# YMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# YMF = YMF.drop('Adj Close', axis=1)\n",
    "# symbol = 'NQ=F'\n",
    "# NQF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NQF = NQF.drop('Adj Close', axis=1)\n",
    "# symbol = 'RTY=F'\n",
    "# RTYF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# RTYF = RTYF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZB=F'\n",
    "# ZBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZBF = ZBF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZN=F'\n",
    "# ZNF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZNF = ZNF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZF=F'\n",
    "# ZFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZFF = ZFF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZT=F'\n",
    "# ZTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZTF = ZTF.drop('Adj Close', axis=1)\n",
    "# symbol = 'GC=F'\n",
    "# GCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GCF = GCF.drop('Adj Close', axis=1)\n",
    "# symbol = 'HG=F'\n",
    "# HGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HGF = HGF.drop('Adj Close', axis=1)\n",
    "# symbol = 'SI=F'\n",
    "# SIF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SIF = SIF.drop('Adj Close', axis=1)\n",
    "# symbol = 'PL=F'\n",
    "# PLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# PLF = PLF.drop('Adj Close', axis=1)\n",
    "# CLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CLF = CLF.drop('Adj Close', axis=1)\n",
    "# symbol = 'NG=F'\n",
    "# NGF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NGF = NGF.drop('Adj Close', axis=1)\n",
    "# symbol = 'BZ=F'\n",
    "# BZF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BZF = BZF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZC=F'\n",
    "# ZCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZCF = ZCF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZO=F'\n",
    "# ZOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZOF = ZOF.drop('Adj Close', axis=1)\n",
    "# symbol = 'KE=F'\n",
    "# KEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KEF = KEF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZR=F'\n",
    "# ZRF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZRF = ZRF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZM=F'\n",
    "# ZMF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZMF = ZMF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZL=F'\n",
    "# ZLF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZLF = ZLF.drop('Adj Close', axis=1)\n",
    "# symbol = 'ZS=F'\n",
    "# ZSF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ZSF = ZSF.drop('Adj Close', axis=1)\n",
    "# symbol = 'GF=F'\n",
    "# GFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GFF = GFF.drop('Adj Close', axis=1)\n",
    "# symbol = 'HE=F'\n",
    "# HEF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HEF = HEF.drop('Adj Close', axis=1)\n",
    "# symbol = 'HO=F'\n",
    "# HOF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HOF = HOF.drop('Adj Close', axis=1)\n",
    "# symbol = 'LE=F'\n",
    "# LFF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# LFF = LFF.drop('Adj Close', axis=1)\n",
    "# symbol = 'CC=F'\n",
    "# CCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CCF = CCF.drop('Adj Close', axis=1)\n",
    "# symbol = 'KC=F'\n",
    "# KCF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KCF = KCF.drop('Adj Close', axis=1)\n",
    "# symbol = 'CT=F'\n",
    "# CTF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CTF = CTF.drop('Adj Close', axis=1)\n",
    "# symbol = 'OJ=F'\n",
    "# OJF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# OJF = OJF.drop('Adj Close', axis=1)\n",
    "# symbol = 'SB=F'\n",
    "# SBF = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SBF = SBF.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# # EFTs\n",
    "\n",
    "# symbol = 'KBA'\n",
    "# KBA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KBA = KBA.drop('Adj Close', axis=1)\n",
    "# symbol = 'CHIQ'\n",
    "# CHIQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CHIQ = CHIQ.drop('Adj Close', axis=1)\n",
    "# symbol = 'CNTX'\n",
    "# CNTX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CNTX = CNTX.drop('Adj Close', axis=1)\n",
    "# symbol = 'CHIS'\n",
    "# CHIS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CHIS = CHIS.drop('Adj Close', axis=1)\n",
    "# symbol = 'CNYA'\n",
    "# CNYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CNYA = CNYA.drop('Adj Close', axis=1)\n",
    "# symbol = 'ASHX'\n",
    "# ASHX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# ASHX = ASHX.drop('Adj Close', axis=1)\n",
    "# symbol = 'KFYP'\n",
    "# KFYP = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KFYP = KFYP.drop('Adj Close', axis=1)\n",
    "# symbol = 'KGRN'\n",
    "# KGRN = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KGRN = KGRN.drop('Adj Close', axis=1)\n",
    "# symbol = 'THD'\n",
    "# THD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# THD = THD.drop('Adj Close', axis=1)\n",
    "# symbol = 'BBAX'\n",
    "# BBAX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BBAX = BBAX.drop('Adj Close', axis=1)\n",
    "# symbol = 'FEMS'\n",
    "# FEMS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FEMS = FEMS.drop('Adj Close', axis=1)\n",
    "# symbol = 'EZA'\n",
    "# EZA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# EZA = EZA.drop('Adj Close', axis=1)\n",
    "# symbol = 'XSD'\n",
    "# XSD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# XSD = XSD.drop('Adj Close', axis=1)\n",
    "# symbol = 'EYLD'\n",
    "# EYLD = yf.download(symbol, start=start_date, end=end_date)\n",
    "# EYLD = EYLD.drop('Adj Close', axis=1)\n",
    "# symbol = 'FNDE'\n",
    "# FNDE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FNDE = FNDE.drop('Adj Close', axis=1)\n",
    "# symbol = 'SPEM'\n",
    "# SPEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SPEM = SPEM.drop('Adj Close', axis=1)\n",
    "# symbol = 'DXJS'\n",
    "# DXJS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DXJS = DXJS.drop('Adj Close', axis=1)\n",
    "# symbol = 'KURE'\n",
    "# KURE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KURE = KURE.drop('Adj Close', axis=1)\n",
    "# symbol = 'EWX'\n",
    "# EWX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# EWX = EWX.drop('Adj Close', axis=1)\n",
    "# symbol = 'FLJH'\n",
    "# FLJH = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FLJH = FLJH.drop('Adj Close', axis=1)\n",
    "# symbol = 'CQQQ'\n",
    "# CQQQ = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CQQQ = CQQQ.drop('Adj Close', axis=1)\n",
    "# symbol = 'CHIE'\n",
    "# CHIE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# CHIE = CHIE.drop('Adj Close', axis=1)\n",
    "# symbol = 'MFEM'\n",
    "# MFEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MFEM = MFEM.drop('Adj Close', axis=1)\n",
    "# symbol = 'DGS'\n",
    "# DGS = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DGS = DGS.drop('Adj Close', axis=1)\n",
    "# symbol = 'HEEM'\n",
    "# HEEM = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HEEM = HEEM.drop('Adj Close', axis=1)\n",
    "\n",
    "\n",
    "# # World Composite Index\n",
    "\n",
    "\n",
    "# symbol = '^HSI'\n",
    "# HSI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# HSI = HSI.drop('Adj Close', axis=1)\n",
    "# symbol = '000001.SS'\n",
    "# SSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# SSE = SSE.drop('Adj Close', axis=1)\n",
    "# symbol = '^N225'\n",
    "# N225 = yf.download(symbol, start=start_date, end=end_date)\n",
    "# N225 = N225.drop('Adj Close', axis=1)\n",
    "# symbol = '^KS11'\n",
    "# KS11 = yf.download(symbol, start=start_date, end=end_date)\n",
    "# KS11 = KS11.drop('Adj Close', axis=1)\n",
    "# symbol = '^BSESN'\n",
    "# BSESN = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BSESN = BSESN.drop('Adj Close', axis=1)\n",
    "# symbol = '^MXX'\n",
    "# MXX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# MXX = MXX.drop('Adj Close', axis=1)\n",
    "# symbol = '^TNX'\n",
    "# TNX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# TNX = TNX.drop('Adj Close', axis=1)\n",
    "# symbol = '^VIX'\n",
    "# VIX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# VIX = VIX.drop('Adj Close', axis=1)\n",
    "# symbol = '^BVSP'\n",
    "# BVSP = yf.download(symbol, start=start_date, end=end_date)\n",
    "# BVSP = BVSP.drop('Adj Close', axis=1)\n",
    "# symbol = '^IXIC'\n",
    "# IXIC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IXIC = IXIC.drop('Adj Close', axis=1)\n",
    "# symbol = '^GSPTSE'\n",
    "# GSPTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GSPTSE = GSPTSE.drop('Adj Close', axis=1)\n",
    "# symbol = '^DJI'\n",
    "# DJI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# DJI = DJI.drop('Adj Close', axis=1)\n",
    "# symbol = '^FCHI'\n",
    "# FCHI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FCHI = FCHI.drop('Adj Close', axis=1)\n",
    "# symbol = '^GDAXI'\n",
    "# GDAXI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GDAXI = GDAXI.drop('Adj Close', axis=1)\n",
    "# symbol = '^FTSE'\n",
    "# FTSE = yf.download(symbol, start=start_date, end=end_date)\n",
    "# FTSE = FTSE.drop('Adj Close', axis=1)\n",
    "# symbol = '^IBEX'\n",
    "# IBEX = yf.download(symbol, start=start_date, end=end_date)\n",
    "# IBEX = IBEX.drop('Adj Close', axis=1)\n",
    "# symbol = '^N100'\n",
    "# N100 = yf.download(symbol, start=start_date, end=end_date)\n",
    "# N100 = N100.drop('Adj Close', axis=1)\n",
    "# symbol = '^GSPC'\n",
    "# GSPC = yf.download(symbol, start=start_date, end=end_date)\n",
    "# GSPC = GSPC.drop('Adj Close', axis=1)\n",
    "# symbol = '^RUT'\n",
    "# RUT = yf.download(symbol, start=start_date, end=end_date)\n",
    "# RUT = RUT.drop('Adj Close', axis=1)\n",
    "# symbol = '^NYA'\n",
    "# NYA = yf.download(symbol, start=start_date, end=end_date)\n",
    "# NYA = NYA.drop('Adj Close', axis=1)\n",
    "# symbol = '^STI'\n",
    "# STI = yf.download(symbol, start=start_date, end=end_date)\n",
    "# STI = STI.drop('Adj Close', axis=1)\n",
    "# symbol = '^AXJO'\n",
    "# AXJO = yf.download(symbol, start=start_date, end=end_date)\n",
    "# AXJO = AXJO.drop('Adj Close', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_inverse transformation\n",
    "import csv\n",
    "# train_data_list = [data3, data4, data5, data6, data7, data8, data9, data10\n",
    "#                   , data11, data12, data13, data14, data15, data16, data17, data18, data19, data20]\n",
    "# data21, data22, data23, data24, data25, data26, data27, data28, data29, data30, data31, data32, data33, data34, data35, data36, data37, data38, data39, data40\n",
    "data = None\n",
    "# train_data_list=[HSI,SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO]\n",
    "train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "                WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,\n",
    "                 MMM,GE,F,GM,HON,LMT,AAPL,GOOGL,GOOG,NVDA,AMZN,META,AMD,ASML,TSM,INTC,MU,QCOM,SNPS,MRVL,IBM,NFLX,MSFT,\n",
    "                 SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO, \n",
    "                 ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,CLF,GCF,HGF,SIF,NGF,BZF,ZOF,KEF,ZRF,ZMF,ZLF,ZCF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                 WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,\n",
    "#                  ]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                 WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,\n",
    "#                  MMM,GE,F,GM,HON,LMT,AAPL,GOOGL,GOOG,NVDA,AMZN,META,AMD,ASML,TSLA,TSM,INTC,MU,QCOM,SNPS,MRVL,IBM,NFLX,MSFT,\n",
    "#                  SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                  ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,GCF,HGF,SIF,NGF,BZF,ZCF,ZOF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF,\n",
    "#                  ]\n",
    "# train_data_list=[ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,CLF,GCF,HGF,SIF,NGF,BZF,ZCF,ZOF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF]\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,\n",
    "#                  FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,GSPC,DJI,RUT,\n",
    "#                  HSI,SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,ESF,YMF,NQF,\n",
    "#                  RTYF,ZBF,ZNF,CLF,GCF,HGF,SIF,CLF,NGF,ZCF,KEF,MSFT,AAPL,GOOGL,GOOG,NVDA,AMZN,META,TSLA,AMD,ASML]\n",
    "\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FOX,\n",
    "#                  FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,GSPC,DJI,PFE,CVS,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS]\n",
    "# valid_data_list=[MSFT,AAPL,GOOGL,AMZN,META,TSLA,AMD,ASML,NVDA,TSM]\n",
    "# valid_data_list=[ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,CLF,GCF,HGF,SIF,NGF,BZF,ZOF,KEF,ZRF,ZMF,ZLF,ZCF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF]\n",
    "\n",
    "# train_data_list=[ESF,YMF,NQF,RTYF,ZBF,ZNF,CLF,GCF,HGF,SIF,CLF,NGF,ZCF,ZFF,ZTF,PLF,PAF,BZF,ZOF,KCF,CTF]\n",
    "# train_data_list=[ESF,YMF,NQF,ZBF,ZNF,GCF,HGF,SIF,CLF,ZCF,NGF,WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,CVS,KO,PEP]\n",
    "\n",
    "# valid_data_list = [SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO, \n",
    "#                  ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,CLF,GCF,HGF,SIF,NGF,BZF,ZOF,KEF,ZRF,ZMF,ZLF,ZCF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF]\n",
    "\n",
    "valid_data_list=[TSLA]\n",
    "\n",
    "# valid_data_list = [ESF]\n",
    "\n",
    "symbol = 'TSLA'\n",
    "start_date = '2022-01-01'\n",
    "end_date = '2023-09-26'\n",
    "start_date_short = '2023-01-01'\n",
    "end_date_short = '2023-09-26'\n",
    "target_col = 'Close'\n",
    "\n",
    "n_trials = 1\n",
    "n_top_models = 3\n",
    "n_predict = 5\n",
    "n_last_sequence = 100\n",
    "forward = 0\n",
    "# valid_size = 0.1\n",
    "\n",
    "# save_directory = \"/home/young78703/Data_Science_Project/model_save/LSTM_TimeSeries_Regression/random_search/tsla_new/tsla_1\"\n",
    "# results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df = random_search(\n",
    "#     data=data, train_data_list=train_data_list, valid_data_list=valid_data_list, symbol=symbol, start_date=start_date, end_date=end_date, \n",
    "#     start_date_short=start_date_short, end_date_short=end_date_short, target_col=target_col, n_trials=n_trials, n_top_models=n_top_models, \n",
    "#     model_save=True, save_directory=save_directory, plot_loss=False, predict_plot=True, future_plot=False, overall_future_plot=True, \n",
    "#     use_target_col=True, future_predictions=None, n_predict=n_predict, n_last_sequence=n_last_sequence, forward=forward)\n",
    "save_directory = \"/home/young78703/Data_Science_Project/model_save/LSTM_TimeSeries_Regression/random_search/tsla_new/tsla_test\"\n",
    "results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df = random_search(\n",
    "    data=data, train_data_list=train_data_list, valid_data_list=valid_data_list, symbol=symbol, start_date=start_date, end_date=end_date, \n",
    "    start_date_short=start_date_short, end_date_short=end_date_short, target_col=target_col, n_trials=n_trials, n_top_models=n_top_models, \n",
    "    model_save=True, save_directory=save_directory, plot_loss=False, predict_plot=True, future_plot=False, overall_future_plot=True, \n",
    "    use_target_col=True, future_predictions=None, n_last_sequence=n_last_sequence, forward=forward)\n",
    "\n",
    "output_file_path = \"/home/young78703/Data_Science_Project/model_save/LSTM_TimeSeries_Regression/random_search/tsla_new/tsla_test\"\n",
    "\n",
    "# Save results_df\n",
    "results_df.to_csv(f'{output_file_path}_results.csv', index=True)\n",
    "\n",
    "#Save top_models\n",
    "top_models_df = pd.DataFrame(top_models, columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "top_models_df.to_csv(f'{output_file_path}_top_models.csv', index=True)\n",
    "\n",
    "# Save the future_metrics DataFrame to a CSV file\n",
    "all_future_metric_finals.to_csv(f'{output_file_path}_all_future_metrics.csv', index=True)\n",
    "# Save future_predictions\n",
    "all_future_predictions_df.to_csv(f'{output_file_path}_all_future_predictions.csv', index=True)\n",
    "# Save the overall_future_metrics DataFrame to a CSV file\n",
    "all_overall_future_metrics_df.to_csv(f'{output_file_path}_all_overall_future_metrics.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_inverse transformation\n",
    "import csv\n",
    "# train_data_list = [data3, data4, data5, data6, data7, data8, data9, data10\n",
    "#                   , data11, data12, data13, data14, data15, data16, data17, data18, data19, data20]\n",
    "# data21, data22, data23, data24, data25, data26, data27, data28, data29, data30, data31, data32, data33, data34, data35, data36, data37, data38, data39, data40\n",
    "data = None\n",
    "# train_data_list=[HSI,SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                 WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,\n",
    "#                  MMM,GE,F,GM,HON,LMT,MSFT,AAPL,GOOGL,GOOG,NVDA,AMZN,META,TSLA,AMD,ASML,\n",
    "#                  SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,VIX,IXIC,GSPTSE,N100,STI,AXJO]\n",
    "# train_data_list=[KBA,CHIQ,CNTX,CHIS,CNYA,ASHX,KFYP,KGRN,THD,BBAX,FEMS,EZA,XSD,EYLD,FNDE,SPEM,DXJS,KURE,EWX,FLJH,CQQQ,CHIE,MFEM,DGS,HEEM,\n",
    "#                  ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,GCF,HGF,SIF,NGF,BZF,ZCF,ZOF,ZRF,ZMF,ZLF,ZSF,GFF,HEF,HOF,LFF,CCF,KCF,CTF,OJF,SBF,\n",
    "#                  ]\n",
    "train_data_list=[ESF,NQF,RTYF,ZBF,ZNF,ZFF,ZTF,CLF,GCF]\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,\n",
    "#                  FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,GSPC,DJI,RUT,\n",
    "#                  HSI,SSE,N225,KS11,BSESN,FCHI,GDAXI,FTSE,IBEX,GSPC,DJI,RUT,NYA,ESF,YMF,NQF,\n",
    "#                  RTYF,ZBF,ZNF,CLF,GCF,HGF,SIF,CLF,NGF,ZCF,KEF,MSFT,AAPL,GOOGL,GOOG,NVDA,AMZN,META,TSLA,AMD,ASML]\n",
    "\n",
    "# train_data_list=[WMT,TGT,COST,HD,LOW,PG,JNJ,PFE,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS,FOX,\n",
    "#                  FDX,DAL,AAL,XOM,CVX,BAC,JPM,MA,V,SPG,VNO,MMM,GE,F,GM,HON,LMT,GSPC,DJI,PFE,CVS,KO,PEP,NKE,MCD,SBUX,VZ,T,FOX,WBD,DIS,UPS]\n",
    "# valid_data_list=[MSFT,AAPL,GOOGL,AMZN,META,TSLA,AMD,ASML,NVDA,TSM]\n",
    "# valid_data_list=[GOOG]\n",
    "\n",
    "# train_data_list=[ESF,YMF,NQF,CLF,GCF]\n",
    "\n",
    "valid_data_list = [ZCF]\n",
    "\n",
    "symbol='ZC=F'\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2023-09-26'\n",
    "start_date_short = '2023-01-01'\n",
    "end_date_short = '2023-09-26'\n",
    "target_col = 'Close'\n",
    "\n",
    "\n",
    "n_trials = 50\n",
    "n_top_models = 1\n",
    "# n_predict = 5\n",
    "n_last_sequence = 100\n",
    "forward = 0\n",
    "# valid_size = 0.1\n",
    "# save_directory = \"/home/young78703/Data_Science_Project/model_save/Future_Stock_Price/rut_10_30\"\n",
    "# save_directory = \"/home/young78703/Data_Science_Project/model_save/Fixed_Model/model1/shopify_50\"\n",
    "# save_directory = \"/home/young78703/Data_Science_Project/model_save/Fixed_Model/shopify_50\"\n",
    "save_directory = \"/home/young78703/Data_Science_Project/model_save/LSTM_TimeSeries_Regression/random_search/indicators/zc=f_test\"\n",
    "results_df, top_models, all_future_predictions_df, all_future_metric_finals, all_overall_future_metrics_df = random_search(\n",
    "    data=data, train_data_list=train_data_list, valid_data_list=valid_data_list, symbol=symbol, start_date=start_date, end_date=end_date, \n",
    "    start_date_short=start_date_short, end_date_short=end_date_short, target_col=target_col, n_trials=n_trials, n_top_models=n_top_models, \n",
    "    model_save=True, save_directory=save_directory, plot_loss=False, predict_plot=True, future_plot=False, overall_future_plot=True, \n",
    "    use_target_col=False, future_predictions=None, n_last_sequence=n_last_sequence, forward=forward)\n",
    "\n",
    "# output_file_path = \"/home/young78703/Data_Science_Project/output/Fixed_Model/model1/shopify_50.csv\"\n",
    "# output_file_path = \"/home/young78703/Data_Science_Project/output/Future_Stock_Price/rut/rut_50.csv\"\n",
    "output_file_path = \"/home/young78703/Data_Science_Project/model_save/LSTM_TimeSeries_Regression/random_search/indicators/zc=f_test\"\n",
    "\n",
    "# Save results_df\n",
    "results_df.to_csv(f'{output_file_path}_results.csv', index=True)\n",
    "\n",
    "#Save top_models\n",
    "top_models_df = pd.DataFrame(top_models, columns=[\"Trial\", \"Parameters\", \"Train MSE\", \"Train MAE\", \"Train R2\", \"Test MSE\", \"Test MAE\", \"Test R2\"])\n",
    "top_models_df.to_csv(f'{output_file_path}_top_models.csv', index=True)\n",
    "\n",
    "# Save the future_metrics DataFrame to a CSV file\n",
    "all_future_metric_finals.to_csv(f'{output_file_path}_all_future_metrics.csv', index=True)\n",
    "# Save future_predictions\n",
    "all_future_predictions_df.to_csv(f'{output_file_path}_all_future_predictions.csv', index=True)\n",
    "# Save the overall_future_metrics DataFrame to a CSV file\n",
    "all_overall_future_metrics_df.to_csv(f'{output_file_path}_all_overall_future_metrics.csv', index=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
