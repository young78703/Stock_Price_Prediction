{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_seq_len + merged_future_prediction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import os\n",
    "import yfinance as yf\n",
    "import timeit\n",
    "import datetime\n",
    "import calendar\n",
    "import warnings\n",
    "from torch.nn.modules.transformer import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.modules.transformer import TransformerDecoder, TransformerDecoderLayer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "def create_sequences(data, seq_len):\n",
    "    X = []\n",
    "    y = []\n",
    "    data = data.values  # This line is added\n",
    "    for i in range(seq_len, data.shape[0]):\n",
    "        X.append(data[i-seq_len:i, :])\n",
    "        y.append(data[i:i+1, :])  # Change target shape to (1, n_features)\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    return X, y\n",
    "\n",
    "# def calculate_indicators(data, interval=None, seq_len=None):\n",
    "#     # Adjust parameters based on frequency\n",
    "#     if interval == '1wk':  # Weekly data\n",
    "#         period = 6\n",
    "#         rsi_period = 9\n",
    "#         short_ema_period = 8\n",
    "#         long_ema_period = 16\n",
    "#         signal_period = 5\n",
    "#         vol_period = 14\n",
    "#         bband_window_size = 12\n",
    "#     elif interval == '1mo':  # Monthly data\n",
    "#         period = 4\n",
    "#         rsi_period = 7\n",
    "#         short_ema_period = 6\n",
    "#         long_ema_period = 12\n",
    "#         signal_period = 4\n",
    "#         vol_period = 10\n",
    "#         bband_window_size = 8\n",
    "#     elif interval == '1d':  # Monthly data\n",
    "#         period = 10\n",
    "#         rsi_period = 14\n",
    "#         short_ema_period = 12\n",
    "#         long_ema_period = 26\n",
    "#         signal_period = 9\n",
    "#         vol_period = 26\n",
    "#         bband_window_size = 20\n",
    "#     elif interval == None:  # not specified\n",
    "#         period = 5\n",
    "#         rsi_period = 7\n",
    "#         short_ema_period = 6\n",
    "#         long_ema_period = 13\n",
    "#         signal_period = 4\n",
    "#         vol_period = 13\n",
    "#         bband_window_size = 10\n",
    "\n",
    "#     if data.shape[0] < long_ema_period + seq_len:\n",
    "#         return None\n",
    "\n",
    "#     # Check if 'Close' column exists in the data\n",
    "#     if 'Close' in data.columns:\n",
    "#         # Calculate daily returns\n",
    "#         data['Return'] = data['Close'].pct_change()\n",
    "#         # Calculate VPT (Volume-Price Trend)\n",
    "#         data['VPT'] = (data['Volume'] * data['Return']).cumsum() \n",
    "\n",
    "#         # Calculate the percentage change for the price range\n",
    "#         data['Price Range %'] = ((data['High'] - data['Low']) / data['Close']) * 100\n",
    "#         # Calculate the percentage change for the candle body size\n",
    "#         data['Body Size %'] = ((data['Open'] - data['Close']) / data['Close']) * 100\n",
    "#         # Calculate the percentage change for the upper shadow size\n",
    "#         data['Upper Shadow %'] = data.apply(lambda row: (row.High - max(row.Open, row.Close)) / row.Close * 100, axis=1)\n",
    "#         # Calculate the percentage change for the lower shadow size\n",
    "#         data['Lower Shadow %'] = data.apply(lambda row: (min(row.Open, row.Close) - row.Low) / row.Close * 100, axis=1)\n",
    "\n",
    "#         lag = (period - 1) // 2\n",
    "#         # Calculate adjusted price\n",
    "#         adjusted_price = 2 * data['Close'] - data['Close'].shift(lag)\n",
    "\n",
    "#         # Calculate ZLEMA directly using adjusted price\n",
    "#         zlema = adjusted_price.ewm(span=period).mean()\n",
    "#         data['ZLEMA'] = zlema\n",
    "\n",
    "#         data['ZLEMA %'] = zlema.pct_change()\n",
    "\n",
    "#         # Define tolerance as a percentage of the close price\n",
    "#         tolerance = 0.015\n",
    "\n",
    "#         # Initialize column with 0s\n",
    "#         data['ZLEMA_Signal'] = 2\n",
    "\n",
    "#         # Buy signal when Close is above (1 + tolerance) * ZLEMA\n",
    "#         data.loc[data['Close'] > data['ZLEMA'] * (1 + tolerance), 'ZLEMA_Signal'] = 3\n",
    "\n",
    "#         # Sell signal when Close is below (1 - tolerance) * ZLEMA\n",
    "#         data.loc[data['Close'] < data['ZLEMA'] * (1 - tolerance), 'ZLEMA_Signal'] = 1\n",
    "\n",
    "#         # Calculate the first exponential moving average (EMA1).\n",
    "#         ema1 = data['Close'].ewm(span=period, min_periods=period).mean()\n",
    "#         # Calculate the second exponential moving average (EMA2).\n",
    "#         ema2 = ema1.ewm(span=period, min_periods=period).mean()\n",
    "#         # Calculate the third exponential moving average (EMA3).\n",
    "#         ema3 = ema2.ewm(span=period, min_periods=period).mean()\n",
    "#         # Calculate the TEMA.\n",
    "#         tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "\n",
    "#         # # Convert the TEMA to a Pandas Series.\n",
    "#         data['TEMA'] = tema\n",
    "\n",
    "#         data['TEMA %'] = tema.pct_change()\n",
    "\n",
    "#         # Initialize column with 0s\n",
    "#         data['TEMA_Signal'] = 2\n",
    "\n",
    "#         # Buy signal when Close is above (1 + tolerance) * TEMA\n",
    "#         data.loc[data ['Close']> data ['TEMA']* (1 + tolerance), 'TEMA_Signal']= 3\n",
    "\n",
    "#         # Sell signal when Close is below (1 - tolerance) * TEMA\n",
    "#         data.loc[data ['Close']< data ['TEMA']*(1-tolerance), 'TEMA_Signal']= 1\n",
    "\n",
    "#         # Calculate RSI and its signal\n",
    "#         delta = data['Close'].diff()\n",
    "#         up, down = delta.copy(), delta.copy()\n",
    "\n",
    "#         up[up < 0] = 0\n",
    "#         down[down > 0] = 0\n",
    "\n",
    "#         average_gain = up.rolling(window=rsi_period).mean()\n",
    "#         average_loss = abs(down.rolling(window=rsi_period).mean())\n",
    "\n",
    "#         rs = average_gain / average_loss\n",
    "\n",
    "#         data['RSI'] =100 - (100 / (1 + rs))\n",
    "#         data['RSI_Diff'] = data['RSI'].diff()\n",
    "\n",
    "#         data[\"RSI_Signal\"] =2\n",
    "#         data.loc[data.RSI < 30,\"RSI_Signal\"] =3\n",
    "#         data.loc[data.RSI > 70,\"RSI_Signal\"] =1\n",
    "\n",
    "#         # Calculate MACD Line: (12-day EMA - 26-day EMA)\n",
    "#         EMA_short = data['Close'].ewm(span=short_ema_period).mean()\n",
    "#         EMA_long = data['Close'].ewm(span=long_ema_period).mean()\n",
    "#         data['MACD_Line'] = EMA_short - EMA_long\n",
    "\n",
    "#         # Calculate Signal Line: a n-day MA of MACD Line\n",
    "#         data['Signal_Line'] = data[\"MACD_Line\"].ewm(span=signal_period).mean()\n",
    "\n",
    "#         # Generate MACD signals based on crossovers\n",
    "#         data[\"MACD_Signal\"] = 2 # Neutral\n",
    "#         data.loc[(data.MACD_Line > data.Signal_Line) & (data.MACD_Line.shift() < data.Signal_Line.shift()),\"MACD_Signal\"] = 3 # Buy\n",
    "#         data.loc[(data.MACD_Line < data.Signal_Line) & (data.MACD_Line.shift() > data.Signal_Line.shift()),\"MACD_Signal\"] = 1 # Sell\n",
    "\n",
    "#         # Generate EMA signals based on crossovers\n",
    "#         data[\"EMA_Short\"]=EMA_short\n",
    "#         data[\"EMA_Long\"]=EMA_long\n",
    "\n",
    "#         # Define a small percentage as a threshold\n",
    "#         ema_diff_threshold = 0.01 # 1% difference\n",
    "\n",
    "#         # Calculate absolute percent difference between short and long EMA\n",
    "#         data['EMA_Diff'] = (EMA_short - EMA_long)/ EMA_long\n",
    "#         data['EMA_Diff %'] = data['EMA_Diff'].pct_change()\n",
    "\n",
    "#         data[\"EMA_Signal\"] = np.where((data[\"EMA_Short\"] > data[\"EMA_Long\"]) & (abs(data[\"EMA_Diff\"]) > ema_diff_threshold), 3, \n",
    "#                              np.where((data[\"EMA_Short\"] < data[\"EMA_Long\"]) & (abs(data[\"EMA_Diff\"]) > ema_diff_threshold), 1, 2))\n",
    "\n",
    "#         # Calculate Volatility as rolling standard deviation of log returns\n",
    "#         data[\"Log_Return\"] =(np.log(data.Close).diff())\n",
    "#         data[\"Volatility\"] =(data.Log_Return.rolling(window=vol_period, center=False).std())\n",
    "#         data['Volatility %'] = data['Volatility'].pct_change()\n",
    "\n",
    "#         # Generate Volatility signals based on high or low volatility\n",
    "#         high_vol_threshold = data[\"Volatility\"].quantile(0.85)\n",
    "#         low_vol_threshold = data[\"Volatility\"].quantile(0.15)\n",
    "        \n",
    "#         data['Volatility_Signal'] = 2\n",
    "#         data.loc[data.Volatility > high_vol_threshold, 'Volatility_Signal'] = 3\n",
    "#         data.loc[data.Volatility < low_vol_threshold, 'Volatility_Signal'] = 1\n",
    "\n",
    "#         # Calculate Bollinger Bands\n",
    "#         data[\"Middle_Band\"] = data[\"Close\"].rolling(window=bband_window_size, center=False).mean()\n",
    "#         data[\"Std_Dev\"] = data[\"Close\"].rolling(window=bband_window_size, center=False).std()\n",
    "#         data[\"Upper_Band\"] = data[\"Middle_Band\"] + data[\"Std_Dev\"] * 2\n",
    "#         data[\"Lower_Band\"] = data[\"Middle_Band\"] - data[\"Std_Dev\"] * 2\n",
    "\n",
    "#         # Identify shifting points\n",
    "#         data[\"BBand_Signal\"] = 2  # neutral\n",
    "\n",
    "#         # Create a copy of the \"Close\" column\n",
    "#         close_shifted = data[\"Close\"].copy()\n",
    "\n",
    "#         # Check for crossing Upper Band\n",
    "#         cross_upper_band = (close_shifted.shift(1) <= data[\"Upper_Band\"].shift(1)) & (data[\"Close\"] > data[\"Upper_Band\"])\n",
    "#         data.loc[cross_upper_band, \"BBand_Signal\"] = 3  # buy\n",
    "\n",
    "#         # Check for crossing Lower Band\n",
    "#         cross_lower_band = (close_shifted.shift(1) >= data[\"Lower_Band\"].shift(1)) & (data[\"Close\"] < data[\"Lower_Band\"])\n",
    "#         data.loc[cross_lower_band, \"BBand_Signal\"] = 1  # sell\n",
    "\n",
    "#         # Drop unnecessary columns\n",
    "#         data.drop([\"Middle_Band\", \"Std_Dev\", \"Open\", 'Low', 'High', 'Volume'], axis=1, inplace=True)\n",
    "#         data.dropna(inplace=True)\n",
    "\n",
    "#     else:\n",
    "#         print(\"'Close' column is not present in the input dataframe.\")\n",
    "\n",
    "#     return data\n",
    "\n",
    "def calculate_indicators(data, interval=None, seq_len=None):\n",
    "    # Adjust parameters based on frequency\n",
    "    if interval == '1wk':  # Weekly data\n",
    "        period = 6\n",
    "        rsi_period = 9\n",
    "        short_ema_period = 8\n",
    "        long_ema_period = 16\n",
    "        signal_period = 5\n",
    "        vol_period = 14\n",
    "        bband_window_size = 12\n",
    "    elif interval == '1mo':  # Monthly data\n",
    "        period = 4\n",
    "        rsi_period = 7\n",
    "        short_ema_period = 6\n",
    "        long_ema_period = 12\n",
    "        signal_period = 4\n",
    "        vol_period = 10\n",
    "        bband_window_size = 8\n",
    "    elif interval == '1d':  # Monthly data\n",
    "        period = 10\n",
    "        rsi_period = 14\n",
    "        short_ema_period = 12\n",
    "        long_ema_period = 26\n",
    "        signal_period = 9\n",
    "        vol_period = 26\n",
    "        bband_window_size = 20\n",
    "    elif interval == None:  # not specified\n",
    "        period = 5\n",
    "        rsi_period = 7\n",
    "        short_ema_period = 6\n",
    "        long_ema_period = 13\n",
    "        signal_period = 4\n",
    "        vol_period = 13\n",
    "        bband_window_size = 10\n",
    "    if data.shape[0] < long_ema_period + seq_len:\n",
    "        return None\n",
    "\n",
    "    # Check if 'Close' column exists in the data\n",
    "    if 'Close' in data.columns:\n",
    "        # Calculate daily returns\n",
    "        data['Return'] = data['Close'].pct_change()\n",
    "        # Calculate VPT (Volume-Price Trend)\n",
    "        data['VPT'] = (data['Volume'] * data['Return']).cumsum()\n",
    "        # Price Range\n",
    "        data['Price Range'] = data['High'] - data['Low']\n",
    "        # Candle Body Size\n",
    "        data['Body Size'] = data['Open'] - data['Close']\n",
    "        # Upper Shadow Size\n",
    "        data['Upper Shadow'] = data.apply(lambda row: row.High - max(row.Open, row.Close), axis=1)\n",
    "        # Lower Shadow Size\n",
    "        data['Lower Shadow'] = data.apply(lambda row: min(row.Open, row.Close) - row.Low, axis=1)\n",
    "\n",
    "        lag = (period - 1) // 2\n",
    "        # Calculate adjusted price\n",
    "        adjusted_price = 2 * data['Close'] - data['Close'].shift(lag)\n",
    "\n",
    "        # Calculate ZLEMA directly using adjusted price\n",
    "        zlema = adjusted_price.ewm(span=period).mean()\n",
    "        data['ZLEMA'] = zlema\n",
    "\n",
    "        # Define tolerance as a percentage of the close price\n",
    "        tolerance = 0.02\n",
    "\n",
    "        # Initialize column with 0s\n",
    "        data['ZLEMA_Signal'] = 2\n",
    "\n",
    "        # Buy signal when Close is above (1 + tolerance) * ZLEMA\n",
    "        data.loc[data['Close'] > data['ZLEMA'] * (1 + tolerance), 'ZLEMA_Signal'] = 3\n",
    "\n",
    "        # Sell signal when Close is below (1 - tolerance) * ZLEMA\n",
    "        data.loc[data['Close'] < data['ZLEMA'] * (1 - tolerance), 'ZLEMA_Signal'] = 1\n",
    "\n",
    "        # Calculate the first exponential moving average (EMA1).\n",
    "        ema1 = data['Close'].ewm(span=period, min_periods=period).mean()\n",
    "        # Calculate the second exponential moving average (EMA2).\n",
    "        ema2 = ema1.ewm(span=period, min_periods=period).mean()\n",
    "        # Calculate the third exponential moving average (EMA3).\n",
    "        ema3 = ema2.ewm(span=period, min_periods=period).mean()\n",
    "        # Calculate the TEMA.\n",
    "        tema = 3 * ema1 - 3 * ema2 + ema3\n",
    "\n",
    "        # Convert the TEMA to a Pandas Series.\n",
    "        data['TEMA'] = tema\n",
    "\n",
    "        # Initialize column with 0s\n",
    "        data['TEMA_Signal'] = 2\n",
    "\n",
    "        # Buy signal when Close is above (1 + tolerance) * TEMA\n",
    "        data.loc[data ['Close']> data ['TEMA']* (1 + tolerance), 'TEMA_Signal']= 3\n",
    "\n",
    "        # Sell signal when Close is below (1 - tolerance) * TEMA\n",
    "        data.loc[data ['Close']< data ['TEMA']*(1-tolerance), 'TEMA_Signal']= 1\n",
    "\n",
    "        # Calculate RSI and its signal\n",
    "        delta = data['Close'].diff()\n",
    "        up, down = delta.copy(), delta.copy()\n",
    "\n",
    "        up[up < 0] = 0\n",
    "        down[down > 0] = 0\n",
    "\n",
    "        average_gain = up.rolling(window=rsi_period).mean()\n",
    "        average_loss = abs(down.rolling(window=rsi_period).mean())\n",
    "\n",
    "        rs = average_gain / average_loss\n",
    "\n",
    "        data['RSI'] =100 - (100 / (1 + rs))\n",
    "        data[\"RSI_Signal\"] =2\n",
    "        data.loc[data.RSI < 30,\"RSI_Signal\"] =3\n",
    "        data.loc[data.RSI > 70,\"RSI_Signal\"] =1\n",
    "\n",
    "        # # Calculate MACD Line: (12-day EMA - 26-day EMA)\n",
    "        EMA_short = data['Close'].ewm(span=short_ema_period).mean()\n",
    "        EMA_long = data['Close'].ewm(span=long_ema_period).mean()\n",
    "        data['MACD_Line'] = EMA_short - EMA_long\n",
    "\n",
    "        # Calculate Signal Line: a n-day MA of MACD Line\n",
    "        data['Signal_Line'] = data[\"MACD_Line\"].ewm(span=signal_period).mean()\n",
    "\n",
    "        # Generate MACD signals based on crossovers\n",
    "        data[\"MACD_Signal\"] = 2 # Neutral\n",
    "        data.loc[(data.MACD_Line > data.Signal_Line) & (data.MACD_Line.shift() < data.Signal_Line.shift()),\"MACD_Signal\"] = 3 # Buy\n",
    "        data.loc[(data.MACD_Line < data.Signal_Line) & (data.MACD_Line.shift() > data.Signal_Line.shift()),\"MACD_Signal\"] = 1 # Sell\n",
    "\n",
    "        # Generate EMA signals based on crossovers\n",
    "        data[\"EMA_Short\"]=EMA_short\n",
    "        data[\"EMA_Long\"]=EMA_long\n",
    "\n",
    "        # Define a small percentage as a threshold\n",
    "        ema_diff_threshold = 0.01 # 1% difference\n",
    "\n",
    "        # Calculate absolute percent difference between short and long EMA\n",
    "        data['EMA_Diff'] = (EMA_short - EMA_long) / EMA_long\n",
    "\n",
    "        data[\"EMA_Signal\"] = np.where((data[\"EMA_Short\"] > data[\"EMA_Long\"]) & (abs(data[\"EMA_Diff\"]) > ema_diff_threshold), 3, \n",
    "                             np.where((data[\"EMA_Short\"] < data[\"EMA_Long\"]) & (abs(data[\"EMA_Diff\"]) > ema_diff_threshold), 1, 2))\n",
    "\n",
    "        # Calculate Volatility as rolling standard deviation of log returns\n",
    "        data[\"Log_Return\"] =(np.log(data.Close).diff())\n",
    "        data[\"Volatility\"] =(data.Log_Return.rolling(window=vol_period, center=False).std())\n",
    "\n",
    "        # Generate Volatility signals based on high or low volatility\n",
    "        high_vol_threshold = data[\"Volatility\"].quantile(0.85)\n",
    "        low_vol_threshold = data[\"Volatility\"].quantile(0.15)\n",
    "        \n",
    "        data['Volatility_Signal'] = 2\n",
    "        data.loc[data.Volatility > high_vol_threshold, 'Volatility_Signal'] = 3\n",
    "        data.loc[data.Volatility < low_vol_threshold, 'Volatility_Signal'] = 1\n",
    "\n",
    "        # Calculate Bollinger Bands\n",
    "        data[\"Middle_Band\"] = data[\"Close\"].rolling(window=bband_window_size, center=False).mean()\n",
    "        data[\"Std_Dev\"] = data[\"Close\"].rolling(window=bband_window_size, center=False).std()\n",
    "        data[\"Upper_Band\"] = data[\"Middle_Band\"] + data[\"Std_Dev\"] * 2\n",
    "        data[\"Lower_Band\"] = data[\"Middle_Band\"] - data[\"Std_Dev\"] * 2\n",
    "\n",
    "        # Identify shifting points\n",
    "        data[\"BBand_Signal\"] = 2  # neutral\n",
    "\n",
    "        # Create a copy of the \"Close\" column\n",
    "        close_shifted = data[\"Close\"].copy()\n",
    "\n",
    "        # Check for crossing Upper Band\n",
    "        cross_upper_band = (close_shifted.shift(1) <= data[\"Upper_Band\"].shift(1)) & (data[\"Close\"] > data[\"Upper_Band\"])\n",
    "        data.loc[cross_upper_band, \"BBand_Signal\"] = 3  # buy\n",
    "\n",
    "        # Check for crossing Lower Band\n",
    "        cross_lower_band = (close_shifted.shift(1) >= data[\"Lower_Band\"].shift(1)) & (data[\"Close\"] < data[\"Lower_Band\"])\n",
    "        data.loc[cross_lower_band, \"BBand_Signal\"] = 1  # sell\n",
    "\n",
    "        # Drop unnecessary columns\n",
    "        data = data.drop(columns=[\"Middle_Band\", \"Std_Dev\", 'Open', 'Low', 'High', 'Volume'])\n",
    "\n",
    "    else:\n",
    "        print(\"'Close' column is not present in the input dataframe.\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def fetch_valid_data(symbol_list_valid, start_date_valid, interval):    \n",
    "   \n",
    "    # Initialize an empty DataFrame\n",
    "    all_data_valid = []\n",
    "\n",
    "    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    for symbol in symbol_list_valid:\n",
    "        # Download the stock price data\n",
    "        data = yf.download(symbol, start=start_date_valid, end=today, interval=interval)\n",
    "        data = data.copy()\n",
    "\n",
    "        # Convert the index to datetime\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "        # # print(data.index[-1])\n",
    "\n",
    "        if interval.endswith('mo'):\n",
    "            # Check if the last data point is from the current month and year\n",
    "            if data.index[-1].month == today.month and data.index[-1].year == today.year:\n",
    "                # Count the number of days in the current month up to today\n",
    "                days_this_month_so_far = today.day\n",
    "\n",
    "            # If the number of days in the last month is less than 20, drop the last month's data\n",
    "            if days_this_month_so_far < 20:\n",
    "                data = data.iloc[:-1].copy()\n",
    "\n",
    "        elif interval.endswith('wk'):\n",
    "            weekdays_till_today = (today.weekday() + 1) % 7\n",
    "            # If the number of days in the last week is less than 3, drop the last week's data\n",
    "            if weekdays_till_today < 3:  # 0: Monday, ..., 4: Friday\n",
    "                data = data.iloc[:-1].copy()\n",
    "\n",
    "        data.dropna(inplace=True)\n",
    "        data = data[data['Volume'] != 0].copy()\n",
    "        \n",
    "        # Check if 'Adj Close' is in columns and drop it\n",
    "        if 'Adj Close' in data.columns:\n",
    "            data = data.drop('Adj Close', axis=1).copy()            \n",
    "                    # Add a column to identify the symbol\n",
    "        data['Symbol'] = symbol\n",
    "        \n",
    "                # Append the data to the all_data DataFrame\n",
    "        all_data_valid.append(data)\n",
    "        \n",
    "    # Concatenate all_data into a single DataFrame \n",
    "    all_data_valid_df = pd.concat(all_data_valid)\n",
    "    \n",
    "    # # Reset the index of the final DataFrame\n",
    "    # all_data_df.reset_index(inplace=True)\n",
    "            \n",
    "    return all_data_valid_df\n",
    "\n",
    "def fetch_train_data(symbol_list_train, start_date, end_date, interval):    \n",
    "   \n",
    "    # Initialize an empty DataFrame\n",
    "    all_data_train = []\n",
    "    \n",
    "    for symbol in symbol_list_train:\n",
    "    \n",
    "        data = yf.download(symbol, start=start_date, end=end_date, interval=interval)\n",
    "        data = data.copy()\n",
    "\n",
    "        data.dropna(inplace=True)\n",
    "        data = data[data['Volume'] != 0].copy()\n",
    "        \n",
    "        # Check if 'Adj Close' is in columns and drop it\n",
    "        if 'Adj Close' in data.columns:\n",
    "            data = data.drop('Adj Close', axis=1).copy()            \n",
    "        # Add a column to identify the symbol\n",
    "        data['Symbol'] = symbol\n",
    "        \n",
    "        # Append the data to the all_data DataFrame\n",
    "        all_data_train.append(data)\n",
    "        \n",
    "    # Concatenate all_data into a single DataFrame \n",
    "    all_data_train_df = pd.concat(all_data_train)\n",
    "               \n",
    "    return all_data_train_df\n",
    "\n",
    "def prepare_data_separate(all_data_train_df, all_data_valid_df, symbol_list_train=None, symbol_list_valid=None, seq_len=None, target_col=None, symbol=None, \n",
    "                          interval=None, scaler=StandardScaler(), forward=-1):\n",
    "\n",
    "    combined_train_data = None\n",
    "    for symbol in symbol_list_train:\n",
    "\n",
    "        train_data = all_data_train_df[all_data_train_df['Symbol'] == symbol].copy()    \n",
    "        train_data = train_data.drop('Symbol', axis=1)\n",
    "\n",
    "        train_data = train_data.copy()\n",
    "        train_data = calculate_indicators(train_data, interval=interval, seq_len=seq_len)\n",
    "\n",
    "        # If train_data is None, it means that the symbol is not available, so skip it\n",
    "        if train_data is None:\n",
    "            print(f\"Not enough data for {symbol}. Skipping...\") \n",
    "            continue\n",
    "\n",
    "        train_data['Target'] = train_data[target_col].shift(forward)\n",
    "        train_data.dropna(inplace=True)\n",
    "        train_data = train_data.drop(target_col, axis=1)\n",
    "\n",
    "        train_data[train_data.columns] = scaler.fit_transform(train_data)\n",
    "        print(f'train_data.shape: {train_data.shape}')\n",
    "\n",
    "        if combined_train_data is None:\n",
    "            combined_train_data = train_data\n",
    "        else:\n",
    "            combined_train_data = pd.concat([combined_train_data, train_data], ignore_index=True)\n",
    "    \n",
    "    combined_valid_data = None\n",
    "    for symbol in symbol_list_valid:\n",
    "\n",
    "        valid_data = all_data_valid_df[all_data_valid_df['Symbol'] == symbol].copy()   \n",
    "        valid_data = valid_data.drop('Symbol', axis=1)\n",
    "        valid_data = valid_data.copy()\n",
    "        valid_data = calculate_indicators(valid_data, interval=interval, seq_len=seq_len)\n",
    "\n",
    "        # If valid_data is None, it means that the symbol is not available, so skip it\n",
    "        if valid_data is None:\n",
    "            print(f\"Not enough data for {symbol}. Skipping...\") \n",
    "            continue\n",
    "        \n",
    "        valid_data['Target'] = valid_data[target_col].shift(forward)\n",
    "        valid_data.dropna(inplace=True)\n",
    "        valid_data = valid_data.drop(target_col, axis=1)\n",
    "        valid_data[valid_data.columns] = scaler.fit_transform(valid_data)\n",
    "        print(f'valid_data.shape: {valid_data.shape}')\n",
    "\n",
    "        if combined_valid_data is None:\n",
    "            combined_valid_data = valid_data\n",
    "        else:\n",
    "            combined_valid_data = pd.concat([combined_valid_data, valid_data], ignore_index=True)\n",
    "\n",
    "    return combined_train_data, combined_valid_data, seq_len\n",
    "\n",
    "def prepare_data_common(train_data, valid_data, seq_len):\n",
    "    # Create sequences\n",
    "    X_train, y_train = create_sequences(train_data, seq_len)\n",
    "    X_valid, y_valid = create_sequences(valid_data, seq_len)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.Tensor(y_train)\n",
    "    X_valid = torch.Tensor(X_valid)\n",
    "    y_valid = torch.Tensor(y_valid)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "# class DecoderOnlyTransformerModel(nn.Module):\n",
    "#     def __init__(self, n_features, nhead=8, nhid=64, nlayers=6, dropout=0.1,\n",
    "#                  l1_regularization=0, l2_regularization=0,\n",
    "#                  activation_function=torch.nn.ReLU()):\n",
    "#         super(DecoderOnlyTransformerModel, self).__init__()\n",
    "\n",
    "#         self.pos_encoder = nn.Sequential(\n",
    "#             nn.Linear(n_features, nhid),\n",
    "#             activation_function,\n",
    "#             nn.Linear(nhid, nhid),\n",
    "#             activation_function\n",
    "#         )\n",
    "\n",
    "#         decoder_layers = TransformerDecoderLayer(nhid, nhead)\n",
    "#         self.transformer_decoder = TransformerDecoder(decoder_layers,nlayers)\n",
    "\n",
    "#         self.decoder = nn.Linear(nhid,n_features)\n",
    "\n",
    "#         self.l1_regularization = l1_regularization\n",
    "#         self.l2_regularization = l2_regularization\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # initrange = 0.1\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[0].weight)\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[2].weight)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "#     def regularization_loss(self):\n",
    "#         l1_loss = 0\n",
    "#         l2_loss = 0\n",
    "#         for param in self.parameters():\n",
    "#             l1_loss += torch.norm(param, 1)\n",
    "#             l2_loss += torch.norm(param, 2) ** 2\n",
    "#         return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "#     def generate_square_subsequent_mask(self, sz):\n",
    "#         mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "#         mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "#         return mask\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.pos_encoder(x)\n",
    "#         output = torch.zeros_like(x)\n",
    "\n",
    "#         # Process in chunks\n",
    "#         chunk_size = 500  # Set based on your available memory\n",
    "#         for i in range(0, x.size(0), chunk_size):\n",
    "#             x_chunk = x[i:i+chunk_size]\n",
    "#             tgt_mask = self.generate_square_subsequent_mask(x_chunk.size(0)).to(x.device)\n",
    "#             output_chunk = self.transformer_decoder(x_chunk, x_chunk, tgt_mask=tgt_mask)\n",
    "#             output[i:i+chunk_size] = output_chunk\n",
    "\n",
    "#         output = self.decoder(output)\n",
    "#         return output[:, -1:, :] \n",
    "    \n",
    "# class TransformerModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, n_features, nhead=8, nhid=64, nlayers=1, dropout=0.1, l1_regularization=0, l2_regularization=0, activation_function = torch.nn.LeakyReLU()):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "#         # model architecture\n",
    "#         self.pos_encoder = nn.Sequential(\n",
    "#             nn.Linear(n_features, nhid),\n",
    "#             activation_function,\n",
    "#             nn.Linear(nhid, nhid),\n",
    "#             activation_function\n",
    "#         )\n",
    "\n",
    "#         encoder_layers = TransformerEncoderLayer(nhid, nhead, nhid, dropout,)\n",
    "#         # encoder_layers.self_attn.batch_first = True\n",
    "#         self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "#         self.decoder = nn.Linear(nhid, n_features)\n",
    "#         self.l1_regularization = l1_regularization\n",
    "#         self.l2_regularization = l2_regularization\n",
    "#         self.init_weights()\n",
    "\n",
    "#     def init_weights(self):\n",
    "#         # initrange = 0.1\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[0].weight)\n",
    "#         nn.init.xavier_uniform_(self.pos_encoder[2].weight)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         nn.init.xavier_uniform_(self.decoder.weight)\n",
    "\n",
    "#     def regularization_loss(self):\n",
    "#         l1_loss = 0\n",
    "#         l2_loss = 0\n",
    "#         for param in self.parameters():\n",
    "#             l1_loss += torch.norm(param, 1)\n",
    "#             l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "#         return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "#     def forward(self, src):\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_encoder(src)\n",
    "#         output = self.decoder(output)\n",
    "#         output = output[:, -1:, :]\n",
    "\n",
    "#         return output\n",
    "\n",
    "class LSTMStockPredictor(nn.Module):\n",
    "    def __init__(self, n_features, nhid=64, nlayers=1, dropout=0.1, l1_regularization=0, l2_regularization=0):\n",
    "        super(LSTMStockPredictor, self).__init__()\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.l1_regularization = l1_regularization\n",
    "        self.l2_regularization = l2_regularization\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(n_features, nhid, nlayers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Output Layer\n",
    "        self.fc = nn.Linear(nhid, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.lstm.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "\n",
    "        self.fc.bias.data.zero_()\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def regularization_loss(self):\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "        for param in self.parameters():\n",
    "            l1_loss += torch.norm(param, 1)\n",
    "            l2_loss += torch.norm(param, 2) ** 2\n",
    "\n",
    "        return self.l1_regularization * l1_loss + self.l2_regularization * l2_loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.nlayers, x.size(0), self.nhid).to(x.device)\n",
    "        c0 = torch.zeros(self.nlayers, x.size(0), self.nhid).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  \n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=torch.optim.Adam, criterion=nn.MSELoss,\n",
    "                l1_regularization=0, l2_regularization=0, activation_function=None,\n",
    "                batch_size=32, patience=10, min_delta=0.0001, learning_rate=1e-3, max_norm=1.0, nan_patience=1):\n",
    "\n",
    "    # Enable cuDNN\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.cuda.empty_cache()\n",
    "    optimizer = optimizer(model.parameters(), lr=learning_rate)\n",
    "    criterion = criterion()\n",
    "\n",
    "    # Setup GPU device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Put model on GPU\n",
    "    model.to(device)\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    X_valid = X_valid.to(device)\n",
    "    y_valid = y_valid.to(device)\n",
    "    valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "    # print(next(model.parameters()).device)\n",
    "    # print(X_train.device)\n",
    "\n",
    "    # Early stopping parameters\n",
    "    patience = patience  # number of epochs with no improvement\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    # NaN stopping parameters\n",
    "    nan_counter = 0\n",
    "    stopped_early = False\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(n_epochs):\n",
    "        # print(next(model.parameters()).device)\n",
    "        # print(X_train.device)\n",
    "        epoch_train_losses = []\n",
    "        for batch_X_train, batch_y_train in train_loader:\n",
    "            batch_X_train = batch_X_train.to(device)\n",
    "            batch_y_train = batch_y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X_train)\n",
    "            loss = criterion(output, batch_y_train)\n",
    "            reg_loss = model.regularization_loss()\n",
    "            total_loss = loss + reg_loss\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                nan_counter += 1\n",
    "            else:\n",
    "                nan_counter = 0\n",
    "\n",
    "            if nan_counter >= nan_patience:\n",
    "                print(f\"Training stopped early at epoch {epoch} due to NaNs in loss\")\n",
    "                stopped_early = True\n",
    "                break\n",
    "\n",
    "            total_loss.backward()\n",
    "            # Add the gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_losses.append(total_loss.item())\n",
    "\n",
    "        # Break the outer loop if NaN stopping was triggered\n",
    "        if nan_counter >= nan_patience:\n",
    "            break\n",
    "\n",
    "        train_losses.append(np.mean(epoch_train_losses))\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X_valid, batch_y_valid in valid_loader:\n",
    "                batch_X_valid = batch_X_valid.to(device)\n",
    "                batch_y_valid = batch_y_valid.to(device)\n",
    "\n",
    "                valid_output = model(batch_X_valid)\n",
    "                val_loss = criterion(valid_output, batch_y_valid)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "        val_losses.append(np.mean(epoch_val_losses))\n",
    "\n",
    "        # Print the running output\n",
    "        print(f\"Epoch {epoch}: Train Loss = {train_losses[-1]:.5f}, Val Loss = {val_losses[-1]:.5f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_losses[-1] < best_val_loss - min_delta:\n",
    "            best_val_loss = val_losses[-1]\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered due to no improvement in validation loss.\")\n",
    "            break\n",
    "\n",
    "    return train_losses, val_losses, stopped_early\n",
    "\n",
    "def plot_results(train_losses, val_losses, trial, save_directory=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.plot(val_losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train Loss', 'Valid Loss'])\n",
    "    plt.title(f'Train and Valid Losses (Trial {trial+1})')\n",
    "\n",
    "    if save_directory:\n",
    "        save_path = os.path.join(save_directory, f\"loss_plot_trial_{trial}.png\")\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def model_save(target_col=None, n_trials=1, model_save=True, save_directory=None, plot_loss=True, symbol_list_train=None, symbol_list_valid=None, start_date=None, end_date=None, start_date_valid=None, \n",
    "               interval=None, freq=None, n_predict=5, seq_len=5, n_last_sequence=1, forward=-1):\n",
    "\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    all_results_params = []\n",
    "\n",
    "    for trial in range(n_trials):\n",
    "        print(f\"Trial {trial + 1} of {n_trials}\")\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "\n",
    "        # seq_len = random.choice(range(3, 6))\n",
    "        # nhead = random.choice(range(15, 21))\n",
    "        # nhid = nhead * random.choice(range(21, 31))\n",
    "        # nlayers = random.choice(range(1, 2))\n",
    "        # # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # # activation_function = random.choice([torch.nn.GELU(), torch.nn.SiLU(), torch.nn.Mish()])\n",
    "        # activation_function = random.choice([torch.nn.Mish()])\n",
    "        # dropout = 0.01 * random.choice(range(9, 10))\n",
    "        # optimizer = random.choice([torch.optim.Adam, torch.optim.AdamW])\n",
    "        # criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        # # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.SmoothL1Loss, torch.nn.KLDivLoss])\n",
    "        # n_epochs = random.choice(range(300, 301))\n",
    "        # batch_size = random.choice(range(256, 257))\n",
    "        # learning_rate = 0.00001 * random.choice(range(10, 11))\n",
    "        # patience = random.choice(range(30, 31))\n",
    "        # min_delta = random.choice([0.0001])\n",
    "        # l1_regularization = 0.0000001 * random.choice(range(1, 2))\n",
    "        # l2_regularization = l1_regularization\n",
    "\n",
    "        seq_len = random.choice(range(3, 6))\n",
    "        # nhead = random.choice(range(20, 21))\n",
    "        nhid = random.choice(range(1000, 2000))\n",
    "        nlayers = random.choice(range(1, 2))\n",
    "        # activation_function = random.choice([torch.nn.Tanh(), torch.nn.Sigmoid(), torch.nn.ELU(), torch.nn.ReLU(), torch.nn.LeakyReLU(negative_slope=0.01)])\n",
    "        # activation_function = random.choice([torch.nn.GELU(), torch.nn.SiLU()])\n",
    "        # activation_function = random.choice([torch.nn.Tanh()])\n",
    "        # activation_function = random.choice([torch.nn.Mish()])\n",
    "        dropout = 0 * random.choice(range(9, 10))\n",
    "        optimizer = random.choice([torch.optim.AdamW])\n",
    "        criterion = random.choice([torch.nn.SmoothL1Loss])\n",
    "        # criterion = random.choice([torch.nn.MSELoss, torch.nn.L1Loss, torch.nn.KLDivLoss])\n",
    "        n_epochs = random.choice(range(300, 501))\n",
    "        batch_size = random.choice(range(256, 257))\n",
    "        learning_rate = 0.00001 * random.choice(range(10, 11))\n",
    "        patience = random.choice(range(30, 31))\n",
    "        min_delta = 0.00001 * random.choice(range(1, 2))\n",
    "        l1_regularization = 0.0000001 * random.choice(range(4, 5))\n",
    "        l2_regularization = l1_regularization\n",
    "\n",
    "        all_data_train_df = fetch_train_data(symbol_list_train=symbol_list_train, start_date=start_date, end_date=end_date, interval=interval)\n",
    "\n",
    "        all_data_valid_df = fetch_valid_data(symbol_list_valid=symbol_list_valid, start_date_valid=start_date_valid, interval=interval)\n",
    "\n",
    "        train_data, valid_data, seq_len = prepare_data_separate(all_data_train_df=all_data_train_df, all_data_valid_df=all_data_valid_df, symbol_list_train=symbol_list_train, \n",
    "                                                                symbol_list_valid=symbol_list_valid, interval=interval, seq_len=seq_len, target_col=target_col, forward=forward)\n",
    "\n",
    "        # Call prepare_data_common() with test_data_unnormalized\n",
    "        X_train, y_train, X_valid, y_valid = prepare_data_common(train_data=train_data, valid_data=valid_data, seq_len=seq_len)\n",
    "\n",
    "        # input_shape = (X_train.shape[0], seq_len, X_train.shape[2])  \n",
    "\n",
    "        # Initialize the model\n",
    "        model = LSTMStockPredictor(n_features=X_train.shape[2], nhid=nhid, nlayers=nlayers, dropout=dropout, l1_regularization=l1_regularization, l2_regularization=l2_regularization)\n",
    "\n",
    "        # Train the model\n",
    "        train_losses, val_losses, stopped_early = train_model(model, X_train, y_train, X_valid, y_valid, n_epochs, optimizer=optimizer, criterion=criterion,\n",
    "                                                              batch_size=batch_size, patience=patience, min_delta=min_delta, learning_rate=learning_rate)\n",
    "        # Check if training stopped early due to NaNs or not\n",
    "        if stopped_early:\n",
    "            print(f\"Random search iteration {trial+1} stopped early due to NaNs in loss\")\n",
    "            # Using 'continue' here will skip the remaining statements of the current iteration and proceed to the next iteration\n",
    "            continue\n",
    "\n",
    "        if plot_loss:\n",
    "            plot_results(train_losses, val_losses, trial, save_directory=save_directory)\n",
    "\n",
    "        # Save the model\n",
    "        if model_save:\n",
    "            if save_directory:\n",
    "                save_path = os.path.join(save_directory, f\"model_trial_{trial}.pt\")\n",
    "            else:\n",
    "                save_path = f\"model_trial_{trial}.pt\"\n",
    "            torch.save(model, save_path)\n",
    "\n",
    "        # Add the results to the results dataframe\n",
    "        params = {\"seq_len\": seq_len, \"nhid\": nhid, \"nlayers\": nlayers, \n",
    "                \"optimizer\": optimizer, \"criterion\": criterion, \"dropout\": dropout, \"n_epochs\": n_epochs,\n",
    "                \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"patience\": patience, \"min_delta\": min_delta,\n",
    "                \"l1_regularization\": l1_regularization, \"l2_regularization\": l2_regularization,\n",
    "                \"n_predict\": n_predict, \"n_last_sequence\": n_last_sequence, \"forward\": forward, 'interval': interval, 'frequency': freq\n",
    "                }\n",
    "\n",
    "        results_params = {**params, \"trial\": trial+1, \"train loss\": train_losses[-1], \"valid loss\": val_losses[-1]}\n",
    "\n",
    "        all_results_params.append(results_params)\n",
    "        all_results_params_df = pd.DataFrame(all_results_params)\n",
    "\n",
    "        if save_directory:\n",
    "            all_results_params_df.to_csv(os.path.join(save_directory, f\"all_results_params_{trial}.csv\"))\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Trial {trial + 1} of {n_trials} is: {duration} seconds\")\n",
    "\n",
    "    return all_results_params_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
