{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LogisticRegression,\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    ElasticNet,\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostClassifier,\n",
    "    AdaBoostRegressor,\n",
    ")\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking Performance of Linear Machine Learning Models: including prediction of new data\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "import os\n",
    "import timeit\n",
    "import joblib\n",
    "\n",
    "def predict_future_prices(model, data, scale_params_df):\n",
    "    # Create a list of symbols in the original order\n",
    "    original_order = scale_params_df['Symbol'].tolist()\n",
    "    \n",
    "    # Convert the scale_params DataFrame into a dictionary for easy lookup\n",
    "    scale_params = scale_params_df.set_index('Symbol').to_dict(orient='index')\n",
    "\n",
    "    # Initialize a list to hold predictions with the symbol as the key\n",
    "    predictions = []\n",
    "\n",
    "    if 'Symbol' in data.columns:\n",
    "        # Group by 'Symbol' and predict for each group\n",
    "        grouped = data.groupby('Symbol')\n",
    "        for symbol, group_data in grouped:\n",
    "            X = group_data.drop(columns=['Symbol'])\n",
    "            if 'Target' in X.columns:\n",
    "                X = X.drop(columns=['Target'])\n",
    "\n",
    "            # Make predictions for the current group\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            # Scale the predictions if scale parameters are provided\n",
    "            if symbol in scale_params:\n",
    "                params = scale_params[symbol]\n",
    "                y_pred = (y_pred * params['Target_Std'] + params['Target_Mean']).item()\n",
    "                predictions.append((symbol, y_pred))\n",
    "\n",
    "    else:\n",
    "        print(\"The 'Symbol' column is missing from the data.\")\n",
    "        return None\n",
    "\n",
    "    # Order predictions to match the original order of symbols\n",
    "    ordered_predictions = [{ 'Symbol': symbol, 'Prediction': next((pred for sym, pred in predictions if sym == symbol), None) } for symbol in original_order]\n",
    "\n",
    "    return ordered_predictions\n",
    "\n",
    "def split_data(X, y, train_frac=0.7, valid_frac=0.15, random_state=False):\n",
    "    \"\"\" Split the data into train, validation, and test sets. \"\"\"\n",
    "    total_count = X.shape[0]\n",
    "    train_size = int(total_count * train_frac)\n",
    "    valid_size = int(total_count * valid_frac)\n",
    "    # test_size = total_count - train_size - valid_size\n",
    "\n",
    "    if random_state:\n",
    "\n",
    "        indices = np.random.permutation(total_count)\n",
    "        train_indices = indices[:train_size]\n",
    "        valid_indices = indices[train_size:train_size + valid_size]\n",
    "        test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_valid, y_valid = X[valid_indices], y[valid_indices]\n",
    "        X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "    else:\n",
    "        X_train, y_train = X[:train_size], y[:train_size]\n",
    "        X_valid, y_valid = X[train_size:train_size + valid_size], y[train_size:train_size + valid_size]\n",
    "        X_test, y_test = X[train_size + valid_size:], y[train_size + valid_size:]\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "def check_model_performance_linear(ModelClasses, data, dependent_var, drop_columns=[], train_frac=0.7, valid_frac=0.15, scaler=MinMaxScaler(), \n",
    "                                   pca=5, random_state=True, output_file_path=None, save_directory=None):\n",
    "    \"\"\"\n",
    "    Check the performance of multiple models on a dataset, and predict values for new data.\n",
    "\n",
    "    Args:\n",
    "        ModelClasses (list): A list of model classes to evaluate.\n",
    "        data (pandas.DataFrame): The dataset to use for training and evaluation.\n",
    "        dependent_var (str): The name of the dependent variable column.\n",
    "        drop_columns (list, optional): A list of column names to drop from the independent variables. Defaults to [].\n",
    "        train_frac (float, optional): The proportion of the data to use for training. Defaults to 0.7.\n",
    "        valid_frac (float, optional): The proportion of the data to use for validation. Defaults to 0.15.\n",
    "        scaler (object, optional): The scaler to use for scaling the independent variables. Defaults to MinMaxScaler().\n",
    "        new_data (pandas.DataFrame, optional): New data for which to predict values. Defaults to None.\n",
    "        random_state (int, optional): The random seed to use for splitting the data into training and testing sets. Defaults to None.\n",
    "        output_file_path (str, optional): Path to the directory where output files are stored. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with the performance metrics for each model, and predicted values for new data.\n",
    "    \"\"\"\n",
    "    if save_directory and not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "\n",
    "    if data is None:\n",
    "        # # Load tensors\n",
    "        # X_train = torch.load(os.path.join(output_file_path, 'X_train.pt'))\n",
    "        # y_train = torch.load(os.path.join(output_file_path, 'y_train.pt'))\n",
    "        # X_valid = torch.load(os.path.join(output_file_path, 'X_valid.pt'))\n",
    "        # y_valid = torch.load(os.path.join(output_file_path, 'y_valid.pt'))\n",
    "        # X_test = torch.load(os.path.join(output_file_path, 'X_test.pt'))\n",
    "        # y_test = torch.load(os.path.join(output_file_path, 'y_test.pt'))\n",
    "\n",
    "        X_train = pd.read_csv(os.path.join(output_file_path, 'X_train.csv'))\n",
    "        y_train = pd.read_csv(os.path.join(output_file_path, 'y_train.csv'))\n",
    "        X_valid = pd.read_csv(os.path.join(output_file_path, 'X_valid.csv'))\n",
    "        y_valid = pd.read_csv(os.path.join(output_file_path, 'y_valid.csv'))\n",
    "        X_test = pd.read_csv(os.path.join(output_file_path, 'X_test.csv'))\n",
    "        y_test = pd.read_csv(os.path.join(output_file_path, 'y_test.csv'))\n",
    "\n",
    "        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "        if pca is not None:\n",
    "            pca_transformer = PCA(n_components=pca)\n",
    "            X_train = pca_transformer.fit_transform(X_train)\n",
    "            X_valid = pca_transformer.transform(X_valid)\n",
    "            X_test = pca_transformer.transform(X_test)\n",
    "            \n",
    "    else:\n",
    "        # Define the independent and dependent variables\n",
    "        X = data.drop([dependent_var] + drop_columns, axis=1)\n",
    "        y = data[dependent_var]\n",
    "\n",
    "        # Split the data into training, validation, and testing sets\n",
    "        X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(X, y, train_frac=train_frac, valid_frac=valid_frac, random_state=random_state)\n",
    "\n",
    "        # Demension Reduction using PCA (Prinscipal Component Analysis)\n",
    "        if pca is not None:\n",
    "            pca_transformer = PCA(n_components=pca)\n",
    "            X_train = pca_transformer.fit_transform(X_train)\n",
    "            X_valid = pca_transformer.transform(X_valid)\n",
    "            X_test = pca_transformer.transform(X_test)\n",
    "\n",
    "        # Scale the independent variables\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_valid = scaler.transform(X_valid)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    results_future = []\n",
    "\n",
    "    # Iterate over each model class\n",
    "    for ModelClass in ModelClasses:\n",
    "\n",
    "        start = timeit.default_timer()\n",
    "        \n",
    "        model_name = ModelClass.__name__\n",
    "        \n",
    "        # Initialize and fit the model\n",
    "        model = ModelClass().fit(X_train, y_train)\n",
    "\n",
    "        # Save the model\n",
    "        if save_directory:\n",
    "            model_filename = os.path.join(save_directory, f\"{model_name}.joblib\")\n",
    "            joblib.dump(model, model_filename)\n",
    "            print(f\"Saved {model_name} model to {model_filename}\")\n",
    "\n",
    "        # Predictions on training set\n",
    "        y_pred_train = model.predict(X_train)\n",
    "        r2_train = r2_score(y_train, y_pred_train)\n",
    "        mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "        mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "        \n",
    "        # Predictions on validation set\n",
    "        y_pred_valid = model.predict(X_valid)\n",
    "        r2_valid = r2_score(y_valid, y_pred_valid)\n",
    "        mae_valid = mean_absolute_error(y_valid, y_pred_valid)\n",
    "        mse_valid = mean_squared_error(y_valid, y_pred_valid)\n",
    "        \n",
    "        # Predictions on test set\n",
    "        y_pred_test = model.predict(X_test)\n",
    "        r2_test = r2_score(y_test, y_pred_test)\n",
    "        mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "        mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "        # Metrics calculations...        \n",
    "        # Store results\n",
    "        result=({\n",
    "            \"Model\": model_name,\n",
    "            \"Train R^2\": r2_train,\n",
    "            \"Train MAE\": mae_train,\n",
    "            \"Train MSE\": mse_train,\n",
    "            \"Validation R^2\": r2_valid,\n",
    "            \"Validation MAE\": mae_valid,\n",
    "            \"Validation MSE\": mse_valid,\n",
    "            \"Test R^2\": r2_test,\n",
    "            \"Test MAE\": mae_test,\n",
    "            \"Test MSE\": mse_test,\n",
    "        })\n",
    "        results.append(result)\n",
    "\n",
    "        # Predict future prices\n",
    "        # Load future data\n",
    "        future_data = pd.read_csv(os.path.join(output_file_path, 'future_data.csv'))\n",
    "        scale_params = pd.read_csv(os.path.join(output_file_path, 'scale_params.csv'))\n",
    "\n",
    "        # Assume 'Symbol' is a non-numeric column that we want to exclude from transformations\n",
    "        future_data_numeric = future_data.copy().drop(columns=['Symbol'])\n",
    "\n",
    "        # Apply PCA transformations used on the training data\n",
    "        # future_data_scaled = scaler.transform(future_data_numeric)\n",
    "        if pca is not None:\n",
    "            future_data_pca = pca_transformer.transform(future_data_numeric)\n",
    "            future_data_pca = pd.DataFrame(future_data_pca, columns=future_data.columns)\n",
    "\n",
    "            rescaled_predictions = predict_future_prices(model, future_data_pca, scale_params)\n",
    "\n",
    "        else:\n",
    "            rescaled_predictions = predict_future_prices(model, future_data, scale_params)\n",
    "            \n",
    "        rescaled_predictions_df = pd.DataFrame(rescaled_predictions)\n",
    "        rescaled_predictions_df['Model'] = model_name\n",
    "        results_future.append(rescaled_predictions_df)\n",
    "\n",
    "        end = timeit.default_timer()\n",
    "        # Calculate and print duration\n",
    "        duration = end - start\n",
    "        print(f\"Execution Time of Symbol_{model_name} is: {duration} seconds\")\n",
    "\n",
    "    results = pd.DataFrame(results)\n",
    "\n",
    "    # Combine all prediction dataframes into a single dataframe\n",
    "    results_future_df = pd.concat(results_future, ignore_index=True)\n",
    "\n",
    "    # Sort the dataframe by 'Symbol'\n",
    "    original_order = future_data['Symbol'].unique()\n",
    "    # Now apply the pivot to rearrange the predicted prices side by side for each model\n",
    "    pivoted_results_future_df = results_future_df.pivot(index='Symbol', columns='Model', values='Prediction')\n",
    "    # Reindex the pivot table to maintain the original order\n",
    "    pivoted_results_future_df = pivoted_results_future_df.reindex(original_order)\n",
    "\n",
    "    return results, pivoted_results_future_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelClasses = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    DecisionTreeRegressor,\n",
    "    # AdaBoostRegressor,\n",
    "    KNeighborsRegressor,\n",
    "    XGBRegressor,\n",
    "    # RandomForestRegressor,\n",
    "    # GradientBoostingRegressor,\n",
    "    # SVR\n",
    "]\n",
    "train_frac=0.7\n",
    "valid_frac=0.20\n",
    "scaler=StandardScaler()\n",
    "pca=None\n",
    "random_state=True\n",
    "time_series=False\n",
    "new_data=None\n",
    "\n",
    "output_file_path=r\"C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\Data_save\\ML_Regression\\Time_Series_Lag\\stock_SP_100_indicator_daily_05072024\"\n",
    "\n",
    "# check_model_performance_linear(ModelClasses, df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "#    'Longitude', 'Regionname'])\n",
    "save_directory = r'C:\\Users\\young78703\\Documents\\GitHub\\Machine-Learning-Projects\\output\\ML_Regression\\Time_Series_Lag\\stock_SP_100_indicator_daily_05072024'\n",
    "results, results_future = check_model_performance_linear(ModelClasses=ModelClasses, data=None, dependent_var=None, drop_columns=[], scaler=scaler, output_file_path=output_file_path, \n",
    "                                         pca=pca, random_state=random_state, save_directory=save_directory)\n",
    "print(results)\n",
    "print(results_future)\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "results.to_csv(os.path.join(save_directory, 'Results.csv'))\n",
    "results_future.to_csv(os.path.join(save_directory, 'Results_Futures.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ModelClasses = [\n",
    "    LinearRegression,\n",
    "    Ridge,\n",
    "    Lasso,\n",
    "    DecisionTreeRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    SVR,\n",
    "    KNeighborsRegressor,\n",
    "    XGBRegressor]\n",
    "    \n",
    "    hyperparameters = {\n",
    "     'LinearRegression': {},\n",
    "    'Ridge': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.1, 1, 10]\n",
    "    },\n",
    "    'DecisionTreeRegressor': {\n",
    "        'max_depth': [2, 4, 8],\n",
    "        'min_samples_leaf': [1, 2, 5]\n",
    "    },\n",
    "    'RandomForestRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [2, 4, 8]\n",
    "    },\n",
    "    'GradientBoostingRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4]\n",
    "    },\n",
    "    'AdaBoostRegressor': {\n",
    "        'n_estimators': [25, 50, 100],\n",
    "        'learning_rate': [0.5, 1, 2]\n",
    "    },\n",
    "    'SVR': {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'KNeighborsRegressor': {\n",
    "        'n_neighbors': [3, 5, 7],\n",
    "        'weights': ['uniform', 'distance']\n",
    "    },\n",
    "    'XGBRegressor': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'learning_rate': [0.01, 0.1, 1],\n",
    "        'max_depth': [2, 4, 6],\n",
    "        'subsample': [0.5, 0.75, 1],\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns= ML_Regression_models_with_GridSearch(ModelClasses, hyperparameters, df, 'Price', drop_columns=['Suburb', 'Address','Type','Method', 'Bedroom2', 'SellerG','Date','Postcode', 'CouncilArea', 'Lattitude',\n",
    "   'Longitude', 'Regionname'],new_data=None, test_size=0.2, random_state=42, scaler=StandardScaler(), scoring = 'neg_mean_squared_error', cv=3, pca=None, save_model=False, save_dir=None)\n",
    "print(returns)\n",
    "returns.to_csv('ML_Linear_Model_Results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
